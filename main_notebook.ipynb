{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25057,
     "status": "ok",
     "timestamp": 1766218102949,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "yjOQ-WYSbo3W",
    "outputId": "d10446ed-2b1a-4b41-fab9-85bcb8385a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/Painter_Assignment\"\n",
    "SRC_PATH = os.path.join(PROJECT_PATH, \"src\")\n",
    "CHECKPOINT_PATH = os.path.join(PROJECT_PATH, \"checkpoints\")\n",
    "KAGGLE_JSON_PATH = os.path.join(PROJECT_PATH, \"kaggle.json\")\n",
    "\n",
    "os.makedirs(SRC_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16141,
     "status": "ok",
     "timestamp": 1766218135998,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "RiLjPihgEngk",
    "outputId": "89da5f3a-a2ce-492c-dfd7-090cc4bb9cf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from Kaggle...\n",
      "Downloading gan-getting-started.zip to /content\n",
      " 83% 305M/367M [00:06<00:01, 41.5MB/s]\n",
      "100% 367M/367M [00:06<00:00, 58.0MB/s]\n",
      "Dataset downloaded and extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(KAGGLE_JSON_PATH):\n",
    "    shutil.copy(KAGGLE_JSON_PATH, \"/content/kaggle.json\")\n",
    "    os.chmod(\"/content/kaggle.json\", 0o600)\n",
    "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n",
    "else:\n",
    "    print(f\"ERROR: kaggle.json not found in {PROJECT_PATH}\")\n",
    "\n",
    "if not os.path.exists(\"/content/dataset\"):\n",
    "    print(\"Downloading dataset from Kaggle...\")\n",
    "    !kaggle competitions download -c gan-getting-started\n",
    "    !unzip -q gan-getting-started.zip -d /content/dataset\n",
    "    print(\"Dataset downloaded and extracted successfully!\")\n",
    "else:\n",
    "    print(\"Dataset already exists on local disk.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12339,
     "status": "ok",
     "timestamp": 1766218196897,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "FJHjfeWGE9dO",
    "outputId": "32516fbc-75d3-44ea-813d-616253bb9cab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melene-gabeskiria2004\u001b[0m (\u001b[33melene-gabeskiria2004-free-univiersity-of-tbilisi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb -q\n",
    "import wandb\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    wandb.login(key=userdata.get('wandb_api_key'))\n",
    "except:\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6199,
     "status": "ok",
     "timestamp": 1766218207974,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "4WBpXpGnL11v"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7334,
     "status": "ok",
     "timestamp": 1766218328247,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "qcjpc56UL4Bu"
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/content/drive/MyDrive/Painter_Assignment/src\")\n",
    "\n",
    "from config import Config\n",
    "from dataset import UnpairedDataset\n",
    "from models import GeneratorResNet, Discriminator\n",
    "from utils import weights_init_normal, ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 2440,
     "status": "ok",
     "timestamp": 1766218336122,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "JwE9J8p9L7YC",
    "outputId": "6c4036bd-37ac-40e8-f56e-152098a9ce5e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251220_081214-m4wfymx2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter/runs/m4wfymx2' target=\"_blank\">stellar-dawn-10</a></strong> to <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter/runs/m4wfymx2' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter/runs/m4wfymx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter/runs/m4wfymx2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fee256674a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=Config.PROJECT_NAME,\n",
    "    config={k:v for k,v in Config.__dict__.items() if not k.startswith('__')},\n",
    "    resume=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRlzjOIXMEb7"
   },
   "outputs": [],
   "source": [
    "G_AB = GeneratorResNet().to(Config.DEVICE) # Photo -> Monet\n",
    "G_BA = GeneratorResNet().to(Config.DEVICE) # Monet -> Photo\n",
    "D_A = Discriminator().to(Config.DEVICE)    # Discriminator for Photos\n",
    "D_B = Discriminator().to(Config.DEVICE)    # Discriminator for Monets\n",
    "\n",
    "# Initialize Optimizers\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    list(G_AB.parameters()) + list(G_BA.parameters()),\n",
    "    lr=Config.LR_G, betas=(Config.B1, Config.B2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=Config.LR_D, betas=(Config.B1, Config.B2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=Config.LR_D, betas=(Config.B1, Config.B2))\n",
    "\n",
    "# Define Loss Functions\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1766143795816,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "1v8dXDMZ77ea",
    "outputId": "4583ee1e-21f7-4dbb-908a-d93dc6c93971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(Config.LOAD_MODEL)\n",
    "print(Config.SAVE_EPOCH_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "error",
     "timestamp": 1766218312014,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "1f03a40a",
    "outputId": "7203b0d4-df75-4e0a-b039-71b97f3eeda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config module was not previously imported.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-112920769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Re-import Config to ensure we're using the reloaded module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "if 'config' in sys.modules:\n",
    "    importlib.reload(sys.modules['config'])\n",
    "    importlib.reload(sys.modules['models'])\n",
    "\n",
    "    print(\"Config module reloaded successfully.\")\n",
    "else:\n",
    "    print(\"Config module was not previously imported.\")\n",
    "\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1837,
     "status": "ok",
     "timestamp": 1766143802137,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "c-01WjtWMYvf",
    "outputId": "9ca6d3f8-af0e-4831-8543-d385eb73f8a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Resuming training from: /content/drive/MyDrive/Painter_Assignment/checkpoints/epoch_5.pth\n",
      "üì∏ Data Loaded. Monet: 300, Photo: 7038\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "if Config.LOAD_MODEL:\n",
    "    load_path = f\"{Config.CHECKPOINT_DIR}/epoch_{Config.START_EPOCH}.pth\"\n",
    "    print(f\"üîÑ Resuming training from: {load_path}\")\n",
    "\n",
    "    checkpoint = torch.load(load_path, map_location=Config.DEVICE)\n",
    "    G_AB.load_state_dict(checkpoint['G_AB'])\n",
    "    G_BA.load_state_dict(checkpoint['G_BA'])\n",
    "    D_A.load_state_dict(checkpoint['D_A'])\n",
    "    D_B.load_state_dict(checkpoint['D_B'])\n",
    "    optimizer_G.load_state_dict(checkpoint['optimizer_G'])\n",
    "    optimizer_D_A.load_state_dict(checkpoint['optimizer_D_A'])\n",
    "    optimizer_D_B.load_state_dict(checkpoint['optimizer_D_B'])\n",
    "\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "else:\n",
    "    print(\"Starting Fresh Training...\")\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "\n",
    "dataset = UnpairedDataset(Config.TRAIN_MONET, Config.TRAIN_PHOTO)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Image buffers to stabilize the discriminator\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "print(f\"üì∏ Data Loaded. Monet: {len(dataset.monet_files)}, Photo: {len(dataset.photo_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2872731,
     "status": "ok",
     "timestamp": 1766150586974,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "pmaoFy-cFsNX",
    "outputId": "d7bc61d3-94ae-4e10-881d-e43f809a8802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Training Started with Mixed Precision (AMP), Soft Labels, and Instance Noise...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2721993729.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipython-input-2721993729.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/tmp/ipython-input-2721993729.py:56: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/tmp/ipython-input-2721993729.py:82: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] [Batch 0] [G loss: 1.6232] [D loss: 0.3251]\n",
      "[Epoch 6] [Batch 100] [G loss: 2.4958] [D loss: 0.0797]\n",
      "[Epoch 6] [Batch 200] [G loss: 2.1762] [D loss: 0.1746]\n",
      "[Epoch 6] [Batch 300] [G loss: 1.8289] [D loss: 0.2608]\n",
      "[Epoch 6] [Batch 400] [G loss: 2.2896] [D loss: 0.4124]\n",
      "[Epoch 6] [Batch 500] [G loss: 2.5710] [D loss: 0.1300]\n",
      "[Epoch 6] [Batch 600] [G loss: 2.8669] [D loss: 0.0977]\n",
      "[Epoch 6] [Batch 700] [G loss: 2.7652] [D loss: 0.1246]\n",
      "[Epoch 6] [Batch 800] [G loss: 2.3175] [D loss: 0.1347]\n",
      "[Epoch 6] [Batch 900] [G loss: 2.5670] [D loss: 0.0603]\n",
      "[Epoch 6] [Batch 1000] [G loss: 2.1580] [D loss: 0.1303]\n",
      "[Epoch 6] [Batch 1100] [G loss: 1.4449] [D loss: 0.0772]\n",
      "[Epoch 6] [Batch 1200] [G loss: 2.6945] [D loss: 0.1572]\n",
      "[Epoch 6] [Batch 1300] [G loss: 2.6889] [D loss: 0.2389]\n",
      "[Epoch 6] [Batch 1400] [G loss: 1.4748] [D loss: 0.0861]\n",
      "[Epoch 6] [Batch 1500] [G loss: 2.0488] [D loss: 0.2625]\n",
      "[Epoch 6] [Batch 1600] [G loss: 3.0297] [D loss: 0.1093]\n",
      "[Epoch 6] [Batch 1700] [G loss: 2.2209] [D loss: 0.0635]\n",
      "[Epoch 6] [Batch 1800] [G loss: 1.8195] [D loss: 0.2465]\n",
      "[Epoch 6] [Batch 1900] [G loss: 1.5797] [D loss: 0.0908]\n",
      "[Epoch 6] [Batch 2000] [G loss: 1.9176] [D loss: 0.3299]\n",
      "[Epoch 6] [Batch 2100] [G loss: 1.7599] [D loss: 0.1379]\n",
      "[Epoch 6] [Batch 2200] [G loss: 1.8612] [D loss: 0.1244]\n",
      "[Epoch 6] [Batch 2300] [G loss: 2.7579] [D loss: 0.0687]\n",
      "[Epoch 6] [Batch 2400] [G loss: 1.8963] [D loss: 0.3212]\n",
      "[Epoch 6] [Batch 2500] [G loss: 2.1063] [D loss: 0.2317]\n",
      "[Epoch 6] [Batch 2600] [G loss: 2.2953] [D loss: 0.1280]\n",
      "[Epoch 6] [Batch 2700] [G loss: 1.5492] [D loss: 0.2078]\n",
      "[Epoch 6] [Batch 2800] [G loss: 2.0144] [D loss: 0.0931]\n",
      "[Epoch 6] [Batch 2900] [G loss: 1.8206] [D loss: 0.2423]\n",
      "[Epoch 6] [Batch 3000] [G loss: 1.8474] [D loss: 0.1143]\n",
      "[Epoch 6] [Batch 3100] [G loss: 1.9591] [D loss: 0.2228]\n",
      "[Epoch 6] [Batch 3200] [G loss: 3.8791] [D loss: 0.0526]\n",
      "[Epoch 6] [Batch 3300] [G loss: 2.0441] [D loss: 0.0874]\n",
      "[Epoch 6] [Batch 3400] [G loss: 1.5527] [D loss: 0.1637]\n",
      "[Epoch 6] [Batch 3500] [G loss: 1.4540] [D loss: 0.2284]\n",
      "[Epoch 6] [Batch 3600] [G loss: 1.8086] [D loss: 0.1153]\n",
      "[Epoch 6] [Batch 3700] [G loss: 2.2309] [D loss: 0.1287]\n",
      "[Epoch 6] [Batch 3800] [G loss: 2.4227] [D loss: 0.1433]\n",
      "[Epoch 6] [Batch 3900] [G loss: 2.1177] [D loss: 0.1912]\n",
      "[Epoch 6] [Batch 4000] [G loss: 1.5306] [D loss: 0.1261]\n",
      "[Epoch 6] [Batch 4100] [G loss: 2.3443] [D loss: 0.1725]\n",
      "[Epoch 6] [Batch 4200] [G loss: 2.5033] [D loss: 0.1384]\n",
      "[Epoch 6] [Batch 4300] [G loss: 2.1482] [D loss: 0.1241]\n",
      "[Epoch 6] [Batch 4400] [G loss: 2.0904] [D loss: 0.1729]\n",
      "[Epoch 6] [Batch 4500] [G loss: 1.9675] [D loss: 0.2306]\n",
      "[Epoch 6] [Batch 4600] [G loss: 3.1125] [D loss: 0.1465]\n",
      "[Epoch 6] [Batch 4700] [G loss: 2.2386] [D loss: 0.1109]\n",
      "[Epoch 6] [Batch 4800] [G loss: 1.7780] [D loss: 0.0936]\n",
      "[Epoch 6] [Batch 4900] [G loss: 1.9894] [D loss: 0.2299]\n",
      "[Epoch 6] [Batch 5000] [G loss: 2.0153] [D loss: 0.0767]\n",
      "[Epoch 6] [Batch 5100] [G loss: 3.5731] [D loss: 0.0401]\n",
      "[Epoch 6] [Batch 5200] [G loss: 1.8588] [D loss: 0.1143]\n",
      "[Epoch 6] [Batch 5300] [G loss: 2.5557] [D loss: 0.1150]\n",
      "[Epoch 6] [Batch 5400] [G loss: 2.4563] [D loss: 0.1229]\n",
      "[Epoch 6] [Batch 5500] [G loss: 3.2412] [D loss: 0.1488]\n",
      "[Epoch 6] [Batch 5600] [G loss: 1.9788] [D loss: 0.1162]\n",
      "[Epoch 6] [Batch 5700] [G loss: 2.1695] [D loss: 0.0399]\n",
      "[Epoch 6] [Batch 5800] [G loss: 2.2674] [D loss: 0.1656]\n",
      "[Epoch 6] [Batch 5900] [G loss: 2.1702] [D loss: 0.1588]\n",
      "[Epoch 6] [Batch 6000] [G loss: 2.4192] [D loss: 0.1943]\n",
      "[Epoch 6] [Batch 6100] [G loss: 1.8876] [D loss: 0.1713]\n",
      "[Epoch 6] [Batch 6200] [G loss: 2.0353] [D loss: 0.0877]\n",
      "[Epoch 6] [Batch 6300] [G loss: 2.1291] [D loss: 0.3553]\n",
      "[Epoch 6] [Batch 6400] [G loss: 2.5046] [D loss: 0.1996]\n",
      "[Epoch 6] [Batch 6500] [G loss: 1.9072] [D loss: 0.1289]\n",
      "[Epoch 6] [Batch 6600] [G loss: 2.6926] [D loss: 0.0564]\n",
      "[Epoch 6] [Batch 6700] [G loss: 1.9355] [D loss: 0.2014]\n",
      "[Epoch 6] [Batch 6800] [G loss: 2.2000] [D loss: 0.1666]\n",
      "[Epoch 6] [Batch 6900] [G loss: 1.8604] [D loss: 0.0545]\n",
      "[Epoch 6] [Batch 7000] [G loss: 2.8619] [D loss: 0.0445]\n",
      "Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints/epoch_6.pth\n",
      "[Epoch 7] [Batch 0] [G loss: 1.4919] [D loss: 0.1839]\n",
      "[Epoch 7] [Batch 100] [G loss: 2.1296] [D loss: 0.1716]\n",
      "[Epoch 7] [Batch 200] [G loss: 2.3412] [D loss: 0.2072]\n",
      "[Epoch 7] [Batch 300] [G loss: 1.7639] [D loss: 0.2107]\n",
      "[Epoch 7] [Batch 400] [G loss: 1.4620] [D loss: 0.0781]\n",
      "[Epoch 7] [Batch 500] [G loss: 2.7845] [D loss: 0.0918]\n",
      "[Epoch 7] [Batch 600] [G loss: 1.7437] [D loss: 0.2152]\n",
      "[Epoch 7] [Batch 700] [G loss: 1.9677] [D loss: 0.1864]\n",
      "[Epoch 7] [Batch 800] [G loss: 2.2055] [D loss: 0.1848]\n",
      "[Epoch 7] [Batch 900] [G loss: 2.0581] [D loss: 0.2782]\n",
      "[Epoch 7] [Batch 1000] [G loss: 1.9869] [D loss: 0.3632]\n",
      "[Epoch 7] [Batch 1100] [G loss: 2.1876] [D loss: 0.0505]\n",
      "[Epoch 7] [Batch 1200] [G loss: 1.6144] [D loss: 0.2599]\n",
      "[Epoch 7] [Batch 1300] [G loss: 2.2827] [D loss: 0.1806]\n",
      "[Epoch 7] [Batch 1400] [G loss: 2.6048] [D loss: 0.0785]\n",
      "[Epoch 7] [Batch 1500] [G loss: 1.6254] [D loss: 0.3274]\n",
      "[Epoch 7] [Batch 1600] [G loss: 2.1829] [D loss: 0.0671]\n",
      "[Epoch 7] [Batch 1700] [G loss: 1.5966] [D loss: 0.1745]\n",
      "[Epoch 7] [Batch 1800] [G loss: 2.1369] [D loss: 0.1988]\n",
      "[Epoch 7] [Batch 1900] [G loss: 2.1713] [D loss: 0.1928]\n",
      "[Epoch 7] [Batch 2000] [G loss: 1.9923] [D loss: 0.2682]\n",
      "[Epoch 7] [Batch 2100] [G loss: 2.0541] [D loss: 0.1161]\n",
      "[Epoch 7] [Batch 2200] [G loss: 1.9565] [D loss: 0.0565]\n",
      "[Epoch 7] [Batch 2300] [G loss: 1.9064] [D loss: 0.0981]\n",
      "[Epoch 7] [Batch 2400] [G loss: 1.5661] [D loss: 0.3218]\n",
      "[Epoch 7] [Batch 2500] [G loss: 1.5388] [D loss: 0.1287]\n",
      "[Epoch 7] [Batch 2600] [G loss: 1.9396] [D loss: 0.0572]\n",
      "[Epoch 7] [Batch 2700] [G loss: 1.5624] [D loss: 0.1553]\n",
      "[Epoch 7] [Batch 2800] [G loss: 2.1149] [D loss: 0.3341]\n",
      "[Epoch 7] [Batch 2900] [G loss: 1.7183] [D loss: 0.2083]\n",
      "[Epoch 7] [Batch 3000] [G loss: 1.8323] [D loss: 0.1306]\n",
      "[Epoch 7] [Batch 3100] [G loss: 2.8000] [D loss: 0.1246]\n",
      "[Epoch 7] [Batch 3200] [G loss: 2.1401] [D loss: 0.1158]\n",
      "[Epoch 7] [Batch 3300] [G loss: 2.2880] [D loss: 0.0776]\n",
      "[Epoch 7] [Batch 3400] [G loss: 2.3555] [D loss: 0.1359]\n",
      "[Epoch 7] [Batch 3500] [G loss: 2.0264] [D loss: 0.2317]\n",
      "[Epoch 7] [Batch 3600] [G loss: 1.8031] [D loss: 0.0525]\n",
      "[Epoch 7] [Batch 3700] [G loss: 1.9536] [D loss: 0.0432]\n",
      "[Epoch 7] [Batch 3800] [G loss: 1.8725] [D loss: 0.1577]\n",
      "[Epoch 7] [Batch 3900] [G loss: 1.6882] [D loss: 0.0787]\n",
      "[Epoch 7] [Batch 4000] [G loss: 2.0015] [D loss: 0.1375]\n",
      "[Epoch 7] [Batch 4100] [G loss: 1.7811] [D loss: 0.1374]\n",
      "[Epoch 7] [Batch 4200] [G loss: 1.8620] [D loss: 0.0817]\n",
      "[Epoch 7] [Batch 4300] [G loss: 2.3941] [D loss: 0.1229]\n",
      "[Epoch 7] [Batch 4400] [G loss: 2.0371] [D loss: 0.1323]\n",
      "[Epoch 7] [Batch 4500] [G loss: 1.9256] [D loss: 0.1123]\n",
      "[Epoch 7] [Batch 4600] [G loss: 2.6384] [D loss: 0.2383]\n",
      "[Epoch 7] [Batch 4700] [G loss: 2.1193] [D loss: 0.1123]\n",
      "[Epoch 7] [Batch 4800] [G loss: 1.9078] [D loss: 0.0880]\n",
      "[Epoch 7] [Batch 4900] [G loss: 2.5732] [D loss: 0.1189]\n",
      "[Epoch 7] [Batch 5000] [G loss: 1.9834] [D loss: 0.1540]\n",
      "[Epoch 7] [Batch 5100] [G loss: 2.5155] [D loss: 0.0498]\n",
      "[Epoch 7] [Batch 5200] [G loss: 1.9113] [D loss: 0.1514]\n",
      "[Epoch 7] [Batch 5300] [G loss: 1.9281] [D loss: 0.2873]\n",
      "[Epoch 7] [Batch 5400] [G loss: 2.4045] [D loss: 0.1821]\n",
      "[Epoch 7] [Batch 5500] [G loss: 2.3442] [D loss: 0.1025]\n",
      "[Epoch 7] [Batch 5600] [G loss: 1.6739] [D loss: 0.1543]\n",
      "[Epoch 7] [Batch 5700] [G loss: 2.1266] [D loss: 0.1374]\n",
      "[Epoch 7] [Batch 5800] [G loss: 2.0576] [D loss: 0.1587]\n",
      "[Epoch 7] [Batch 5900] [G loss: 1.9791] [D loss: 0.0467]\n",
      "[Epoch 7] [Batch 6000] [G loss: 1.5483] [D loss: 0.1033]\n",
      "[Epoch 7] [Batch 6100] [G loss: 1.4933] [D loss: 0.1026]\n",
      "[Epoch 7] [Batch 6200] [G loss: 2.0058] [D loss: 0.1835]\n",
      "[Epoch 7] [Batch 6300] [G loss: 3.3957] [D loss: 0.2125]\n",
      "[Epoch 7] [Batch 6400] [G loss: 1.4255] [D loss: 0.3140]\n",
      "[Epoch 7] [Batch 6500] [G loss: 2.4029] [D loss: 0.0583]\n",
      "[Epoch 7] [Batch 6600] [G loss: 1.9010] [D loss: 0.1562]\n",
      "[Epoch 7] [Batch 6700] [G loss: 1.8542] [D loss: 0.1294]\n",
      "[Epoch 7] [Batch 6800] [G loss: 2.0285] [D loss: 0.1505]\n",
      "[Epoch 7] [Batch 6900] [G loss: 1.5873] [D loss: 0.1151]\n",
      "[Epoch 7] [Batch 7000] [G loss: 1.8653] [D loss: 0.1020]\n",
      "Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints/epoch_7.pth\n",
      "[Epoch 8] [Batch 0] [G loss: 1.7998] [D loss: 0.1346]\n",
      "[Epoch 8] [Batch 100] [G loss: 2.2957] [D loss: 0.1724]\n",
      "[Epoch 8] [Batch 200] [G loss: 2.0475] [D loss: 0.0842]\n",
      "[Epoch 8] [Batch 300] [G loss: 1.6245] [D loss: 0.1150]\n",
      "[Epoch 8] [Batch 400] [G loss: 1.7485] [D loss: 0.1877]\n",
      "[Epoch 8] [Batch 500] [G loss: 1.8354] [D loss: 0.1019]\n",
      "[Epoch 8] [Batch 600] [G loss: 1.9052] [D loss: 0.0855]\n",
      "[Epoch 8] [Batch 700] [G loss: 1.7115] [D loss: 0.1337]\n",
      "[Epoch 8] [Batch 800] [G loss: 2.0109] [D loss: 0.2078]\n",
      "[Epoch 8] [Batch 900] [G loss: 1.8558] [D loss: 0.1100]\n",
      "[Epoch 8] [Batch 1000] [G loss: 1.2891] [D loss: 0.0938]\n",
      "[Epoch 8] [Batch 1100] [G loss: 2.2157] [D loss: 0.1572]\n",
      "[Epoch 8] [Batch 1200] [G loss: 2.1432] [D loss: 0.1600]\n",
      "[Epoch 8] [Batch 1300] [G loss: 2.0005] [D loss: 0.1309]\n",
      "[Epoch 8] [Batch 1400] [G loss: 1.5624] [D loss: 0.1893]\n",
      "[Epoch 8] [Batch 1500] [G loss: 1.5124] [D loss: 0.0561]\n",
      "[Epoch 8] [Batch 1600] [G loss: 2.4042] [D loss: 0.1674]\n",
      "[Epoch 8] [Batch 1700] [G loss: 2.9905] [D loss: 0.1293]\n",
      "[Epoch 8] [Batch 1800] [G loss: 2.5626] [D loss: 0.0897]\n",
      "[Epoch 8] [Batch 1900] [G loss: 1.5959] [D loss: 0.1359]\n",
      "[Epoch 8] [Batch 2000] [G loss: 1.6185] [D loss: 0.0919]\n",
      "[Epoch 8] [Batch 2100] [G loss: 1.6640] [D loss: 0.0745]\n",
      "[Epoch 8] [Batch 2200] [G loss: 1.9128] [D loss: 0.0745]\n",
      "[Epoch 8] [Batch 2300] [G loss: 1.4778] [D loss: 0.3227]\n",
      "[Epoch 8] [Batch 2400] [G loss: 2.0272] [D loss: 0.1499]\n",
      "[Epoch 8] [Batch 2500] [G loss: 1.7555] [D loss: 0.1201]\n",
      "[Epoch 8] [Batch 2600] [G loss: 2.2542] [D loss: 0.1903]\n",
      "[Epoch 8] [Batch 2700] [G loss: 1.9519] [D loss: 0.1779]\n",
      "[Epoch 8] [Batch 2800] [G loss: 2.0125] [D loss: 0.2003]\n",
      "[Epoch 8] [Batch 2900] [G loss: 2.2209] [D loss: 0.0506]\n",
      "[Epoch 8] [Batch 3000] [G loss: 2.5040] [D loss: 0.1691]\n",
      "[Epoch 8] [Batch 3100] [G loss: 2.1043] [D loss: 0.1690]\n",
      "[Epoch 8] [Batch 3200] [G loss: 2.0346] [D loss: 0.1584]\n",
      "[Epoch 8] [Batch 3300] [G loss: 2.2742] [D loss: 0.1438]\n",
      "[Epoch 8] [Batch 3400] [G loss: 1.8287] [D loss: 0.0830]\n",
      "[Epoch 8] [Batch 3500] [G loss: 1.8613] [D loss: 0.1334]\n",
      "[Epoch 8] [Batch 3600] [G loss: 2.0185] [D loss: 0.0457]\n",
      "[Epoch 8] [Batch 3700] [G loss: 2.0674] [D loss: 0.0610]\n",
      "[Epoch 8] [Batch 3800] [G loss: 1.4721] [D loss: 0.0926]\n",
      "[Epoch 8] [Batch 3900] [G loss: 1.9499] [D loss: 0.1016]\n",
      "[Epoch 8] [Batch 4000] [G loss: 2.0372] [D loss: 0.2968]\n",
      "[Epoch 8] [Batch 4100] [G loss: 2.8793] [D loss: 0.1496]\n",
      "[Epoch 8] [Batch 4200] [G loss: 2.3593] [D loss: 0.1180]\n",
      "[Epoch 8] [Batch 4300] [G loss: 1.3792] [D loss: 0.0904]\n",
      "[Epoch 8] [Batch 4400] [G loss: 1.8855] [D loss: 0.0415]\n",
      "[Epoch 8] [Batch 4500] [G loss: 2.0246] [D loss: 0.1117]\n",
      "[Epoch 8] [Batch 4600] [G loss: 2.8121] [D loss: 0.1209]\n",
      "[Epoch 8] [Batch 4700] [G loss: 1.6733] [D loss: 0.0822]\n",
      "[Epoch 8] [Batch 4800] [G loss: 1.8268] [D loss: 0.0438]\n",
      "[Epoch 8] [Batch 4900] [G loss: 1.6606] [D loss: 0.0840]\n",
      "[Epoch 8] [Batch 5000] [G loss: 2.0057] [D loss: 0.1142]\n",
      "[Epoch 8] [Batch 5100] [G loss: 1.8348] [D loss: 0.0352]\n",
      "[Epoch 8] [Batch 5200] [G loss: 2.0495] [D loss: 0.1452]\n",
      "[Epoch 8] [Batch 5300] [G loss: 1.8068] [D loss: 0.1922]\n",
      "[Epoch 8] [Batch 5400] [G loss: 2.1883] [D loss: 0.1518]\n",
      "[Epoch 8] [Batch 5500] [G loss: 1.8268] [D loss: 0.1117]\n",
      "[Epoch 8] [Batch 5600] [G loss: 2.3233] [D loss: 0.0900]\n",
      "[Epoch 8] [Batch 5700] [G loss: 2.1080] [D loss: 0.2810]\n",
      "[Epoch 8] [Batch 5800] [G loss: 3.2452] [D loss: 0.2584]\n",
      "[Epoch 8] [Batch 5900] [G loss: 1.8748] [D loss: 0.0763]\n",
      "[Epoch 8] [Batch 6000] [G loss: 2.2808] [D loss: 0.1090]\n",
      "[Epoch 8] [Batch 6100] [G loss: 2.6091] [D loss: 0.1784]\n",
      "[Epoch 8] [Batch 6200] [G loss: 2.0511] [D loss: 0.2297]\n",
      "[Epoch 8] [Batch 6300] [G loss: 2.9725] [D loss: 0.2910]\n",
      "[Epoch 8] [Batch 6400] [G loss: 1.1895] [D loss: 0.1601]\n",
      "[Epoch 8] [Batch 6500] [G loss: 2.3585] [D loss: 0.0672]\n",
      "[Epoch 8] [Batch 6600] [G loss: 2.1161] [D loss: 0.1494]\n",
      "[Epoch 8] [Batch 6700] [G loss: 1.8815] [D loss: 0.0864]\n",
      "[Epoch 8] [Batch 6800] [G loss: 2.0008] [D loss: 0.1460]\n",
      "[Epoch 8] [Batch 6900] [G loss: 2.0806] [D loss: 0.2346]\n",
      "[Epoch 8] [Batch 7000] [G loss: 2.1653] [D loss: 0.2641]\n",
      "Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints/epoch_8.pth\n",
      "[Epoch 9] [Batch 0] [G loss: 1.6628] [D loss: 0.1475]\n",
      "[Epoch 9] [Batch 100] [G loss: 2.5531] [D loss: 0.0728]\n",
      "[Epoch 9] [Batch 200] [G loss: 1.7597] [D loss: 0.2162]\n",
      "[Epoch 9] [Batch 300] [G loss: 1.5111] [D loss: 0.2039]\n",
      "[Epoch 9] [Batch 400] [G loss: 1.3487] [D loss: 0.1686]\n",
      "[Epoch 9] [Batch 500] [G loss: 1.1977] [D loss: 0.0915]\n",
      "[Epoch 9] [Batch 600] [G loss: 2.2920] [D loss: 0.0687]\n",
      "[Epoch 9] [Batch 700] [G loss: 1.6836] [D loss: 0.2805]\n",
      "[Epoch 9] [Batch 800] [G loss: 2.0202] [D loss: 0.2058]\n",
      "[Epoch 9] [Batch 900] [G loss: 2.1847] [D loss: 0.1404]\n",
      "[Epoch 9] [Batch 1000] [G loss: 1.9392] [D loss: 0.1465]\n",
      "[Epoch 9] [Batch 1100] [G loss: 1.9843] [D loss: 0.0832]\n",
      "[Epoch 9] [Batch 1200] [G loss: 1.9112] [D loss: 0.1440]\n",
      "[Epoch 9] [Batch 1300] [G loss: 2.1358] [D loss: 0.0791]\n",
      "[Epoch 9] [Batch 1400] [G loss: 2.2511] [D loss: 0.0929]\n",
      "[Epoch 9] [Batch 1500] [G loss: 1.8825] [D loss: 0.0969]\n",
      "[Epoch 9] [Batch 1600] [G loss: 2.0461] [D loss: 0.0735]\n",
      "[Epoch 9] [Batch 1700] [G loss: 2.1032] [D loss: 0.1922]\n",
      "[Epoch 9] [Batch 1800] [G loss: 2.2113] [D loss: 0.0838]\n",
      "[Epoch 9] [Batch 1900] [G loss: 1.5189] [D loss: 0.2035]\n",
      "[Epoch 9] [Batch 2000] [G loss: 2.0500] [D loss: 0.0735]\n",
      "[Epoch 9] [Batch 2100] [G loss: 2.4259] [D loss: 0.1104]\n",
      "[Epoch 9] [Batch 2200] [G loss: 1.7911] [D loss: 0.0808]\n",
      "[Epoch 9] [Batch 2300] [G loss: 2.3373] [D loss: 0.2457]\n",
      "[Epoch 9] [Batch 2400] [G loss: 2.0882] [D loss: 0.0838]\n",
      "[Epoch 9] [Batch 2500] [G loss: 1.5905] [D loss: 0.0738]\n",
      "[Epoch 9] [Batch 2600] [G loss: 1.8856] [D loss: 0.0828]\n",
      "[Epoch 9] [Batch 2700] [G loss: 1.3912] [D loss: 0.1675]\n",
      "[Epoch 9] [Batch 2800] [G loss: 1.0410] [D loss: 0.1306]\n",
      "[Epoch 9] [Batch 2900] [G loss: 1.5574] [D loss: 0.3104]\n",
      "[Epoch 9] [Batch 3000] [G loss: 1.4914] [D loss: 0.1077]\n",
      "[Epoch 9] [Batch 3100] [G loss: 1.3526] [D loss: 0.1018]\n",
      "[Epoch 9] [Batch 3200] [G loss: 1.9365] [D loss: 0.0816]\n",
      "[Epoch 9] [Batch 3300] [G loss: 1.7465] [D loss: 0.0327]\n",
      "[Epoch 9] [Batch 3400] [G loss: 1.0865] [D loss: 0.0900]\n",
      "[Epoch 9] [Batch 3500] [G loss: 1.5165] [D loss: 0.2086]\n",
      "[Epoch 9] [Batch 3600] [G loss: 1.6756] [D loss: 0.0510]\n",
      "[Epoch 9] [Batch 3700] [G loss: 1.5668] [D loss: 0.1318]\n",
      "[Epoch 9] [Batch 3800] [G loss: 1.5641] [D loss: 0.0810]\n",
      "[Epoch 9] [Batch 3900] [G loss: 2.2564] [D loss: 0.2542]\n",
      "[Epoch 9] [Batch 4000] [G loss: 1.2654] [D loss: 0.1950]\n",
      "[Epoch 9] [Batch 4100] [G loss: 1.8211] [D loss: 0.0380]\n",
      "[Epoch 9] [Batch 4200] [G loss: 1.9141] [D loss: 0.1676]\n",
      "[Epoch 9] [Batch 4300] [G loss: 2.1136] [D loss: 0.1477]\n",
      "[Epoch 9] [Batch 4400] [G loss: 1.4658] [D loss: 0.1629]\n",
      "[Epoch 9] [Batch 4500] [G loss: 1.9445] [D loss: 0.1907]\n",
      "[Epoch 9] [Batch 4600] [G loss: 1.4971] [D loss: 0.1446]\n",
      "[Epoch 9] [Batch 4700] [G loss: 1.5615] [D loss: 0.0592]\n",
      "[Epoch 9] [Batch 4800] [G loss: 2.4004] [D loss: 0.0750]\n",
      "[Epoch 9] [Batch 4900] [G loss: 2.1834] [D loss: 0.0991]\n",
      "[Epoch 9] [Batch 5000] [G loss: 1.6319] [D loss: 0.1011]\n",
      "[Epoch 9] [Batch 5100] [G loss: 1.5625] [D loss: 0.0759]\n",
      "[Epoch 9] [Batch 5200] [G loss: 1.8524] [D loss: 0.2756]\n",
      "[Epoch 9] [Batch 5300] [G loss: 1.5435] [D loss: 0.2266]\n",
      "[Epoch 9] [Batch 5400] [G loss: 1.7537] [D loss: 0.1433]\n",
      "[Epoch 9] [Batch 5500] [G loss: 1.8495] [D loss: 0.1065]\n",
      "[Epoch 9] [Batch 5600] [G loss: 1.9253] [D loss: 0.0929]\n",
      "[Epoch 9] [Batch 5700] [G loss: 2.1934] [D loss: 0.0474]\n",
      "[Epoch 9] [Batch 5800] [G loss: 2.1198] [D loss: 0.0421]\n",
      "[Epoch 9] [Batch 5900] [G loss: 1.4695] [D loss: 0.1466]\n",
      "[Epoch 9] [Batch 6000] [G loss: 2.4707] [D loss: 0.1167]\n",
      "[Epoch 9] [Batch 6100] [G loss: 1.4954] [D loss: 0.0650]\n",
      "[Epoch 9] [Batch 6200] [G loss: 1.4702] [D loss: 0.0941]\n",
      "[Epoch 9] [Batch 6300] [G loss: 1.7436] [D loss: 0.1339]\n",
      "[Epoch 9] [Batch 6400] [G loss: 1.4597] [D loss: 0.0758]\n",
      "[Epoch 9] [Batch 6500] [G loss: 2.1104] [D loss: 0.0774]\n",
      "[Epoch 9] [Batch 6600] [G loss: 2.1896] [D loss: 0.1276]\n",
      "[Epoch 9] [Batch 6700] [G loss: 1.6748] [D loss: 0.0506]\n",
      "[Epoch 9] [Batch 6800] [G loss: 2.2857] [D loss: 0.0955]\n",
      "[Epoch 9] [Batch 6900] [G loss: 2.3923] [D loss: 0.1107]\n",
      "[Epoch 9] [Batch 7000] [G loss: 1.5506] [D loss: 0.0870]\n",
      "Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints/epoch_9.pth\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(\"Training Started with Mixed Precision (AMP), Soft Labels, and Instance Noise...\")\n",
    "\n",
    "for epoch in range(start_epoch, Config.EPOCHS):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        real_A = batch[\"photo\"].to(Config.DEVICE)\n",
    "        real_B = batch[\"monet\"].to(Config.DEVICE)\n",
    "\n",
    "        valid = torch.ones((real_A.size(0), 1, 16, 16), requires_grad=False).to(Config.DEVICE)\n",
    "        fake = torch.zeros((real_A.size(0), 1, 16, 16), requires_grad=False).to(Config.DEVICE)\n",
    "        valid_smooth = torch.full((real_A.size(0), 1, 16, 16), Config.REAL_LABEL_SMOOTH, requires_grad=False).to(Config.DEVICE)\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2 * Config.LAMBDA_ID\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_BA(real_B)\n",
    "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "            # Cycle loss\n",
    "            recov_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "            recov_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2 * Config.LAMBDA_CYCLE\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + loss_cycle + loss_identity\n",
    "\n",
    "        scaler.scale(loss_G).backward()\n",
    "        scaler.step(optimizer_G)\n",
    "        scaler.update()\n",
    "\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            # get the buffered image FIRST\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "\n",
    "            # generate noise\n",
    "            noise_real = torch.randn_like(real_A) * 0.05\n",
    "            noise_fake = torch.randn_like(fake_A_) * 0.05\n",
    "\n",
    "            # add noise\n",
    "            real_A_noisy = real_A + noise_real\n",
    "            fake_A_noisy = fake_A_.detach() + noise_fake\n",
    "\n",
    "            # calculate Loss\n",
    "            loss_real = criterion_GAN(D_A(real_A_noisy), valid_smooth)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A_noisy), fake)\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "        scaler.scale(loss_D_A).backward()\n",
    "        scaler.step(optimizer_D_A)\n",
    "        scaler.update()\n",
    "\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            # get the buffered image FIRST\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "\n",
    "            # generate noise\n",
    "            noise_real = torch.randn_like(real_B) * 0.05\n",
    "            noise_fake = torch.randn_like(fake_B_) * 0.05\n",
    "\n",
    "            # add noise\n",
    "            real_B_noisy = real_B + noise_real\n",
    "            fake_B_noisy = fake_B_.detach() + noise_fake\n",
    "\n",
    "            # calculate Loss\n",
    "            loss_real = criterion_GAN(D_B(real_B_noisy), valid_smooth)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B_noisy), fake)\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "        scaler.scale(loss_D_B).backward()\n",
    "        scaler.step(optimizer_D_B)\n",
    "        scaler.update()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            wandb.log({\n",
    "                \"Loss/Generator\": loss_G.item(),\n",
    "                \"Loss/Discriminator\": (loss_D_A.item() + loss_D_B.item()),\n",
    "                \"Epoch\": epoch\n",
    "            })\n",
    "            print(f\"[Epoch {epoch}] [Batch {i}] [G loss: {loss_G.item():.4f}] [D loss: {(loss_D_A.item() + loss_D_B.item()):.4f}]\")\n",
    "\n",
    "    img_real_A = real_A[0].detach().cpu() * 0.5 + 0.5\n",
    "    img_fake_B = fake_B[0].detach().cpu() * 0.5 + 0.5\n",
    "    img_real_B = real_B[0].detach().cpu() * 0.5 + 0.5\n",
    "    img_fake_A = fake_A[0].detach().cpu() * 0.5 + 0.5\n",
    "\n",
    "    wandb.log({\n",
    "        \"Visual/Real Photo\": wandb.Image(img_real_A, caption=f\"Real Photo (Epoch {epoch})\"),\n",
    "        \"Visual/Generated Monet\": wandb.Image(img_fake_B, caption=f\"Generated Monet (Epoch {epoch})\"),\n",
    "        \"Visual/Real Monet\": wandb.Image(img_real_B, caption=f\"Real Monet (Epoch {epoch})\"),\n",
    "        \"Visual/Generated Photo\": wandb.Image(img_fake_A, caption=f\"Reconstructed Photo (Epoch {epoch})\")\n",
    "    })\n",
    "\n",
    "    if epoch % Config.SAVE_EPOCH_FREQ == 0:\n",
    "        save_path = f\"{Config.CHECKPOINT_DIR}/epoch_{epoch}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'G_AB': G_AB.state_dict(),\n",
    "            'G_BA': G_BA.state_dict(),\n",
    "            'D_A': D_A.state_dict(),\n",
    "            'D_B': D_B.state_dict(),\n",
    "            'optimizer_G': optimizer_G.state_dict(),\n",
    "            'optimizer_D_A': optimizer_D_A.state_dict(),\n",
    "            'optimizer_D_B': optimizer_D_B.state_dict()\n",
    "        }, save_path)\n",
    "        print(f\"Checkpoint saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1766150812311,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "e0472210",
    "outputId": "8e343cb3-bb83-4014-db12-7bc31515a8a2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ</td></tr><tr><td>Loss/Discriminator</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Loss/Generator</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>9</td></tr><tr><td>Loss/Discriminator</td><td>0.08697</td></tr><tr><td>Loss/Generator</td><td>1.55063</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-galaxy-9</strong> at: <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter/runs/de1fj2k2' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter/runs/de1fj2k2</a><br> View project at: <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter</a><br>Synced 5 W&B file(s), 64 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251219_053506-de1fj2k2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCHnUGMf1ImM"
   },
   "source": [
    "U-net experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1766218976968,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "ekmdhzIm2L95"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast # Mixed Precision\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1766218981436,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "RtmRCjFV2sFx"
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/content/drive/MyDrive/University/Painter_Assignment/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1766218996534,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "BFEY45xD2v9C"
   },
   "outputs": [],
   "source": [
    "from config_unet import ConfigUNet\n",
    "from dataset import UnpairedDataset\n",
    "from models import GeneratorUNet, Discriminator\n",
    "from utils import weights_init_normal, ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1766220097511,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "e78J03dK21ol",
    "outputId": "9c052cfd-8692-4e58-fa16-1e83447acf76"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251220_084136-s9pgls7g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET/runs/s9pgls7g' target=\"_blank\">absurd-cloud-2</a></strong> to <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET/runs/s9pgls7g' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET/runs/s9pgls7g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET/runs/s9pgls7g?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fee248f5eb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=ConfigUNet.PROJECT_NAME,\n",
    "    config={k:v for k,v in ConfigUNet.__dict__.items() if not k.startswith('__')},\n",
    "    reinit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1084,
     "status": "ok",
     "timestamp": 1766220100925,
     "user": {
      "displayName": "Elene Gabeskiria",
      "userId": "09323722659653611241"
     },
     "user_tz": -240
    },
    "id": "Umhk6_cw133K",
    "outputId": "ea167c74-26ef-4d6d-b44c-905ffd2c0e06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting EXPERIMENT 2: U-Net Architecture on cuda...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting EXPERIMENT 2: U-Net Architecture on {ConfigUNet.DEVICE}...\")\n",
    "\n",
    "G_AB = GeneratorUNet().to(ConfigUNet.DEVICE) # Photo -> Monet\n",
    "G_BA = GeneratorUNet().to(ConfigUNet.DEVICE) # Monet -> Photo\n",
    "D_A = Discriminator().to(ConfigUNet.DEVICE)\n",
    "D_B = Discriminator().to(ConfigUNet.DEVICE)\n",
    "\n",
    "# Optimizers & Loss\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    list(G_AB.parameters()) + list(G_BA.parameters()),\n",
    "    lr=ConfigUNet.LR, betas=(ConfigUNet.B1, ConfigUNet.B2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=ConfigUNet.LR, betas=(ConfigUNet.B1, ConfigUNet.B2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=ConfigUNet.LR, betas=(ConfigUNet.B1, ConfigUNet.B2))\n",
    "\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "# initialize Weights \n",
    "G_AB.apply(weights_init_normal)\n",
    "G_BA.apply(weights_init_normal)\n",
    "D_A.apply(weights_init_normal)\n",
    "D_B.apply(weights_init_normal)\n",
    "\n",
    "dataset = UnpairedDataset(ConfigUNet.TRAIN_MONET, ConfigUNet.TRAIN_PHOTO)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=ConfigUNet.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=ConfigUNet.NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Buffers & Scaler\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "scaler = GradScaler('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "cWmG4OO_28x1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] [Batch 0] [G loss: 11.0310]\n",
      "[Epoch 0] [Batch 100] [G loss: 2.5375]\n",
      "[Epoch 0] [Batch 200] [G loss: 3.7616]\n",
      "[Epoch 0] [Batch 300] [G loss: 1.6436]\n",
      "[Epoch 0] [Batch 400] [G loss: 1.9185]\n",
      "[Epoch 0] [Batch 500] [G loss: 2.2433]\n",
      "[Epoch 0] [Batch 600] [G loss: 1.5672]\n",
      "[Epoch 0] [Batch 700] [G loss: 1.4593]\n",
      "[Epoch 0] [Batch 800] [G loss: 1.5946]\n",
      "[Epoch 0] [Batch 900] [G loss: 1.4376]\n",
      "[Epoch 0] [Batch 1000] [G loss: 1.2024]\n",
      "[Epoch 0] [Batch 1100] [G loss: 1.5474]\n",
      "[Epoch 0] [Batch 1200] [G loss: 1.2385]\n",
      "[Epoch 0] [Batch 1300] [G loss: 1.6008]\n",
      "[Epoch 0] [Batch 1400] [G loss: 1.9840]\n",
      "[Epoch 0] [Batch 1500] [G loss: 1.4590]\n",
      "[Epoch 0] [Batch 1600] [G loss: 1.5041]\n",
      "[Epoch 0] [Batch 1700] [G loss: 1.0736]\n",
      "[Epoch 0] [Batch 1800] [G loss: 1.1419]\n",
      "[Epoch 0] [Batch 1900] [G loss: 1.1036]\n",
      "[Epoch 0] [Batch 2000] [G loss: 1.1801]\n",
      "[Epoch 0] [Batch 2100] [G loss: 1.5649]\n",
      "[Epoch 0] [Batch 2200] [G loss: 1.1619]\n",
      "[Epoch 0] [Batch 2300] [G loss: 1.3058]\n",
      "[Epoch 0] [Batch 2400] [G loss: 1.1057]\n",
      "[Epoch 0] [Batch 2500] [G loss: 1.4710]\n",
      "[Epoch 0] [Batch 2600] [G loss: 1.5119]\n",
      "[Epoch 0] [Batch 2700] [G loss: 1.1775]\n",
      "[Epoch 0] [Batch 2800] [G loss: 1.4669]\n",
      "[Epoch 0] [Batch 2900] [G loss: 1.4929]\n",
      "[Epoch 0] [Batch 3000] [G loss: 1.5087]\n",
      "[Epoch 0] [Batch 3100] [G loss: 1.3398]\n",
      "[Epoch 0] [Batch 3200] [G loss: 1.2277]\n",
      "[Epoch 0] [Batch 3300] [G loss: 1.0512]\n",
      "[Epoch 0] [Batch 3400] [G loss: 1.5058]\n",
      "[Epoch 0] [Batch 3500] [G loss: 1.1340]\n",
      "[Epoch 0] [Batch 3600] [G loss: 1.4128]\n",
      "[Epoch 0] [Batch 3700] [G loss: 0.6962]\n",
      "[Epoch 0] [Batch 3800] [G loss: 1.1730]\n",
      "[Epoch 0] [Batch 3900] [G loss: 1.3582]\n",
      "[Epoch 0] [Batch 4000] [G loss: 0.9402]\n",
      "[Epoch 0] [Batch 4100] [G loss: 1.4749]\n",
      "[Epoch 0] [Batch 4200] [G loss: 0.9387]\n",
      "[Epoch 0] [Batch 4300] [G loss: 1.1703]\n",
      "[Epoch 0] [Batch 4400] [G loss: 1.2362]\n",
      "[Epoch 0] [Batch 4500] [G loss: 1.3750]\n",
      "[Epoch 0] [Batch 4600] [G loss: 1.6516]\n",
      "[Epoch 0] [Batch 4700] [G loss: 0.9485]\n",
      "[Epoch 0] [Batch 4800] [G loss: 1.3900]\n",
      "[Epoch 0] [Batch 4900] [G loss: 1.1735]\n",
      "[Epoch 0] [Batch 5000] [G loss: 1.5115]\n",
      "[Epoch 0] [Batch 5100] [G loss: 1.2935]\n",
      "[Epoch 0] [Batch 5200] [G loss: 1.5329]\n",
      "[Epoch 0] [Batch 5300] [G loss: 1.0842]\n",
      "[Epoch 0] [Batch 5400] [G loss: 1.1323]\n",
      "[Epoch 0] [Batch 5500] [G loss: 1.2079]\n",
      "[Epoch 0] [Batch 5600] [G loss: 1.0326]\n",
      "[Epoch 0] [Batch 5700] [G loss: 1.0789]\n",
      "[Epoch 0] [Batch 5800] [G loss: 1.3734]\n",
      "[Epoch 0] [Batch 5900] [G loss: 1.2342]\n",
      "[Epoch 0] [Batch 6000] [G loss: 1.0070]\n",
      "[Epoch 0] [Batch 6100] [G loss: 1.1358]\n",
      "[Epoch 0] [Batch 6200] [G loss: 0.8241]\n",
      "[Epoch 0] [Batch 6300] [G loss: 1.0267]\n",
      "[Epoch 0] [Batch 6400] [G loss: 1.0917]\n",
      "[Epoch 0] [Batch 6500] [G loss: 1.4351]\n",
      "[Epoch 0] [Batch 6600] [G loss: 1.0687]\n",
      "[Epoch 0] [Batch 6700] [G loss: 0.9199]\n",
      "[Epoch 0] [Batch 6800] [G loss: 1.1692]\n",
      "[Epoch 0] [Batch 6900] [G loss: 1.1550]\n",
      "[Epoch 0] [Batch 7000] [G loss: 1.5624]\n",
      "üíæ UNet Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints_unet/epoch_0.pth\n",
      "[Epoch 1] [Batch 0] [G loss: 1.2371]\n",
      "[Epoch 1] [Batch 100] [G loss: 1.3762]\n",
      "[Epoch 1] [Batch 200] [G loss: 1.3412]\n",
      "[Epoch 1] [Batch 300] [G loss: 1.0697]\n",
      "[Epoch 1] [Batch 400] [G loss: 1.3961]\n",
      "[Epoch 1] [Batch 500] [G loss: 1.5950]\n",
      "[Epoch 1] [Batch 600] [G loss: 1.4401]\n",
      "[Epoch 1] [Batch 700] [G loss: 1.1453]\n",
      "[Epoch 1] [Batch 800] [G loss: 2.0030]\n",
      "[Epoch 1] [Batch 900] [G loss: 1.9273]\n",
      "[Epoch 1] [Batch 1000] [G loss: 1.4044]\n",
      "[Epoch 1] [Batch 1100] [G loss: 1.2827]\n",
      "[Epoch 1] [Batch 1200] [G loss: 1.2252]\n",
      "[Epoch 1] [Batch 1300] [G loss: 0.8928]\n",
      "[Epoch 1] [Batch 1400] [G loss: 1.0974]\n",
      "[Epoch 1] [Batch 1500] [G loss: 1.1470]\n",
      "[Epoch 1] [Batch 1600] [G loss: 1.0391]\n",
      "[Epoch 1] [Batch 1700] [G loss: 1.2172]\n",
      "[Epoch 1] [Batch 1800] [G loss: 1.3111]\n",
      "[Epoch 1] [Batch 1900] [G loss: 1.3373]\n",
      "[Epoch 1] [Batch 2000] [G loss: 1.3800]\n",
      "[Epoch 1] [Batch 2100] [G loss: 1.1924]\n",
      "[Epoch 1] [Batch 2200] [G loss: 1.4957]\n",
      "[Epoch 1] [Batch 2300] [G loss: 1.0609]\n",
      "[Epoch 1] [Batch 2400] [G loss: 1.1851]\n",
      "[Epoch 1] [Batch 2500] [G loss: 1.0208]\n",
      "[Epoch 1] [Batch 2600] [G loss: 0.9855]\n",
      "[Epoch 1] [Batch 2700] [G loss: 1.2849]\n",
      "[Epoch 1] [Batch 2800] [G loss: 1.1853]\n",
      "[Epoch 1] [Batch 2900] [G loss: 1.2429]\n",
      "[Epoch 1] [Batch 3000] [G loss: 0.9602]\n",
      "[Epoch 1] [Batch 3100] [G loss: 1.3470]\n",
      "[Epoch 1] [Batch 3200] [G loss: 1.2022]\n",
      "[Epoch 1] [Batch 3300] [G loss: 1.1298]\n",
      "[Epoch 1] [Batch 3400] [G loss: 1.2107]\n",
      "[Epoch 1] [Batch 3500] [G loss: 1.1715]\n",
      "[Epoch 1] [Batch 3600] [G loss: 1.1049]\n",
      "[Epoch 1] [Batch 3700] [G loss: 1.1143]\n",
      "[Epoch 1] [Batch 3800] [G loss: 1.4026]\n",
      "[Epoch 1] [Batch 3900] [G loss: 1.3894]\n",
      "[Epoch 1] [Batch 4000] [G loss: 1.1541]\n",
      "[Epoch 1] [Batch 4100] [G loss: 1.4699]\n",
      "[Epoch 1] [Batch 4200] [G loss: 1.3883]\n",
      "[Epoch 1] [Batch 4300] [G loss: 0.8217]\n",
      "[Epoch 1] [Batch 4400] [G loss: 1.2439]\n",
      "[Epoch 1] [Batch 4500] [G loss: 0.9813]\n",
      "[Epoch 1] [Batch 4600] [G loss: 1.1454]\n",
      "[Epoch 1] [Batch 4700] [G loss: 1.4115]\n",
      "[Epoch 1] [Batch 4800] [G loss: 0.7819]\n",
      "[Epoch 1] [Batch 4900] [G loss: 1.0947]\n",
      "[Epoch 1] [Batch 5000] [G loss: 1.1524]\n",
      "[Epoch 1] [Batch 5100] [G loss: 1.4248]\n",
      "[Epoch 1] [Batch 5200] [G loss: 1.1418]\n",
      "[Epoch 1] [Batch 5300] [G loss: 1.0084]\n",
      "[Epoch 1] [Batch 5400] [G loss: 1.3065]\n",
      "[Epoch 1] [Batch 5500] [G loss: 1.1069]\n",
      "[Epoch 1] [Batch 5600] [G loss: 1.2558]\n",
      "[Epoch 1] [Batch 5700] [G loss: 1.1002]\n",
      "[Epoch 1] [Batch 5800] [G loss: 0.8735]\n",
      "[Epoch 1] [Batch 5900] [G loss: 1.2485]\n",
      "[Epoch 1] [Batch 6000] [G loss: 1.3113]\n",
      "[Epoch 1] [Batch 6100] [G loss: 1.3132]\n",
      "[Epoch 1] [Batch 6200] [G loss: 0.8770]\n",
      "[Epoch 1] [Batch 6300] [G loss: 0.9042]\n",
      "[Epoch 1] [Batch 6400] [G loss: 1.2463]\n",
      "[Epoch 1] [Batch 6500] [G loss: 1.0924]\n",
      "[Epoch 1] [Batch 6600] [G loss: 1.2114]\n",
      "[Epoch 1] [Batch 6700] [G loss: 1.5799]\n",
      "[Epoch 1] [Batch 6800] [G loss: 1.4505]\n",
      "[Epoch 1] [Batch 6900] [G loss: 0.9111]\n",
      "[Epoch 1] [Batch 7000] [G loss: 1.6463]\n",
      "[Epoch 2] [Batch 0] [G loss: 1.5428]\n",
      "[Epoch 2] [Batch 100] [G loss: 1.5727]\n",
      "[Epoch 2] [Batch 200] [G loss: 1.5676]\n",
      "[Epoch 2] [Batch 300] [G loss: 1.4794]\n",
      "[Epoch 2] [Batch 400] [G loss: 1.2531]\n",
      "[Epoch 2] [Batch 500] [G loss: 1.0990]\n",
      "[Epoch 2] [Batch 600] [G loss: 1.4070]\n",
      "[Epoch 2] [Batch 700] [G loss: 1.1545]\n",
      "[Epoch 2] [Batch 800] [G loss: 1.4855]\n",
      "[Epoch 2] [Batch 900] [G loss: 1.3202]\n",
      "[Epoch 2] [Batch 1000] [G loss: 1.4701]\n",
      "[Epoch 2] [Batch 1100] [G loss: 1.2545]\n",
      "[Epoch 2] [Batch 1200] [G loss: 1.6776]\n",
      "[Epoch 2] [Batch 1300] [G loss: 1.0330]\n",
      "[Epoch 2] [Batch 1400] [G loss: 1.2857]\n",
      "[Epoch 2] [Batch 1500] [G loss: 1.1736]\n",
      "[Epoch 2] [Batch 1600] [G loss: 1.3915]\n",
      "[Epoch 2] [Batch 1700] [G loss: 1.2745]\n",
      "[Epoch 2] [Batch 1800] [G loss: 1.3664]\n",
      "[Epoch 2] [Batch 1900] [G loss: 1.0462]\n",
      "[Epoch 2] [Batch 2000] [G loss: 1.3364]\n",
      "[Epoch 2] [Batch 2100] [G loss: 2.1767]\n",
      "[Epoch 2] [Batch 2200] [G loss: 1.1117]\n",
      "[Epoch 2] [Batch 2300] [G loss: 1.1078]\n",
      "[Epoch 2] [Batch 2400] [G loss: 0.7584]\n",
      "[Epoch 2] [Batch 2500] [G loss: 1.0778]\n",
      "[Epoch 2] [Batch 2600] [G loss: 1.3250]\n",
      "[Epoch 2] [Batch 2700] [G loss: 1.2257]\n",
      "[Epoch 2] [Batch 2800] [G loss: 1.4089]\n",
      "[Epoch 2] [Batch 2900] [G loss: 1.3816]\n",
      "[Epoch 2] [Batch 3000] [G loss: 1.1036]\n",
      "[Epoch 2] [Batch 3100] [G loss: 0.9511]\n",
      "[Epoch 2] [Batch 3200] [G loss: 1.4803]\n",
      "[Epoch 2] [Batch 3300] [G loss: 1.4260]\n",
      "[Epoch 2] [Batch 3400] [G loss: 1.3308]\n",
      "[Epoch 2] [Batch 3500] [G loss: 0.9567]\n",
      "[Epoch 2] [Batch 3600] [G loss: 1.4520]\n",
      "[Epoch 2] [Batch 3700] [G loss: 1.0090]\n",
      "[Epoch 2] [Batch 3800] [G loss: 1.6836]\n",
      "[Epoch 2] [Batch 3900] [G loss: 1.5363]\n",
      "[Epoch 2] [Batch 4000] [G loss: 1.1830]\n",
      "[Epoch 2] [Batch 4100] [G loss: 1.0025]\n",
      "[Epoch 2] [Batch 4200] [G loss: 1.4985]\n",
      "[Epoch 2] [Batch 4300] [G loss: 1.5245]\n",
      "[Epoch 2] [Batch 4400] [G loss: 0.9307]\n",
      "[Epoch 2] [Batch 4500] [G loss: 1.3394]\n",
      "[Epoch 2] [Batch 4600] [G loss: 1.7518]\n",
      "[Epoch 2] [Batch 4700] [G loss: 1.6416]\n",
      "[Epoch 2] [Batch 4800] [G loss: 1.0599]\n",
      "[Epoch 2] [Batch 4900] [G loss: 0.9317]\n",
      "[Epoch 2] [Batch 5000] [G loss: 1.3724]\n",
      "[Epoch 2] [Batch 5100] [G loss: 0.8650]\n",
      "[Epoch 2] [Batch 5200] [G loss: 1.3276]\n",
      "[Epoch 2] [Batch 5300] [G loss: 1.5178]\n",
      "[Epoch 2] [Batch 5400] [G loss: 1.0470]\n",
      "[Epoch 2] [Batch 5500] [G loss: 1.4484]\n",
      "[Epoch 2] [Batch 5600] [G loss: 1.1359]\n",
      "[Epoch 2] [Batch 5700] [G loss: 1.2925]\n",
      "[Epoch 2] [Batch 5800] [G loss: 1.5115]\n",
      "[Epoch 2] [Batch 5900] [G loss: 1.7149]\n",
      "[Epoch 2] [Batch 6000] [G loss: 0.9734]\n",
      "[Epoch 2] [Batch 6100] [G loss: 1.1289]\n",
      "[Epoch 2] [Batch 6200] [G loss: 0.9057]\n",
      "[Epoch 2] [Batch 6300] [G loss: 1.1510]\n",
      "[Epoch 2] [Batch 6400] [G loss: 1.3981]\n",
      "[Epoch 2] [Batch 6500] [G loss: 1.5797]\n",
      "[Epoch 2] [Batch 6600] [G loss: 1.5707]\n",
      "[Epoch 2] [Batch 6700] [G loss: 1.5199]\n",
      "[Epoch 2] [Batch 6800] [G loss: 1.7588]\n",
      "[Epoch 2] [Batch 6900] [G loss: 1.0139]\n",
      "[Epoch 2] [Batch 7000] [G loss: 1.0879]\n",
      "[Epoch 3] [Batch 0] [G loss: 1.2226]\n",
      "[Epoch 3] [Batch 100] [G loss: 1.6558]\n",
      "[Epoch 3] [Batch 200] [G loss: 1.5710]\n",
      "[Epoch 3] [Batch 300] [G loss: 1.3150]\n",
      "[Epoch 3] [Batch 400] [G loss: 1.7428]\n",
      "[Epoch 3] [Batch 500] [G loss: 2.1671]\n",
      "[Epoch 3] [Batch 600] [G loss: 1.6207]\n",
      "[Epoch 3] [Batch 700] [G loss: 1.3952]\n",
      "[Epoch 3] [Batch 800] [G loss: 1.5785]\n",
      "[Epoch 3] [Batch 900] [G loss: 1.3103]\n",
      "[Epoch 3] [Batch 1000] [G loss: 1.8252]\n",
      "[Epoch 3] [Batch 1100] [G loss: 1.3238]\n",
      "[Epoch 3] [Batch 1200] [G loss: 1.4143]\n",
      "[Epoch 3] [Batch 1300] [G loss: 1.4990]\n",
      "[Epoch 3] [Batch 1400] [G loss: 1.0500]\n",
      "[Epoch 3] [Batch 1500] [G loss: 1.3187]\n",
      "[Epoch 3] [Batch 1600] [G loss: 0.8754]\n",
      "[Epoch 3] [Batch 1700] [G loss: 1.3128]\n",
      "[Epoch 3] [Batch 1800] [G loss: 1.7076]\n",
      "[Epoch 3] [Batch 1900] [G loss: 1.3684]\n",
      "[Epoch 3] [Batch 2000] [G loss: 1.0477]\n",
      "[Epoch 3] [Batch 2100] [G loss: 1.3405]\n",
      "[Epoch 3] [Batch 2200] [G loss: 1.4560]\n",
      "[Epoch 3] [Batch 2300] [G loss: 1.0820]\n",
      "[Epoch 3] [Batch 2400] [G loss: 1.5864]\n",
      "[Epoch 3] [Batch 2500] [G loss: 1.4413]\n",
      "[Epoch 3] [Batch 2600] [G loss: 1.2867]\n",
      "[Epoch 3] [Batch 2700] [G loss: 1.5423]\n",
      "[Epoch 3] [Batch 2800] [G loss: 1.3491]\n",
      "[Epoch 3] [Batch 2900] [G loss: 1.2216]\n",
      "[Epoch 3] [Batch 3000] [G loss: 1.0299]\n",
      "[Epoch 3] [Batch 3100] [G loss: 1.3826]\n",
      "[Epoch 3] [Batch 3200] [G loss: 1.5708]\n",
      "[Epoch 3] [Batch 3300] [G loss: 1.4490]\n",
      "[Epoch 3] [Batch 3400] [G loss: 1.6585]\n",
      "[Epoch 3] [Batch 3500] [G loss: 0.9817]\n",
      "[Epoch 3] [Batch 3600] [G loss: 1.4114]\n",
      "[Epoch 3] [Batch 3700] [G loss: 1.0379]\n",
      "[Epoch 3] [Batch 3800] [G loss: 1.4535]\n",
      "[Epoch 3] [Batch 3900] [G loss: 1.0376]\n",
      "[Epoch 3] [Batch 4000] [G loss: 1.3889]\n",
      "[Epoch 3] [Batch 4100] [G loss: 1.4904]\n",
      "[Epoch 3] [Batch 4200] [G loss: 1.3359]\n",
      "[Epoch 3] [Batch 4300] [G loss: 1.7404]\n",
      "[Epoch 3] [Batch 4400] [G loss: 1.4189]\n",
      "[Epoch 3] [Batch 4500] [G loss: 1.1961]\n",
      "[Epoch 3] [Batch 4600] [G loss: 1.1298]\n",
      "[Epoch 3] [Batch 4700] [G loss: 1.3413]\n",
      "[Epoch 3] [Batch 4800] [G loss: 1.5789]\n",
      "[Epoch 3] [Batch 4900] [G loss: 1.0594]\n",
      "[Epoch 3] [Batch 5000] [G loss: 1.6880]\n",
      "[Epoch 3] [Batch 5100] [G loss: 1.4212]\n",
      "[Epoch 3] [Batch 5200] [G loss: 1.2981]\n",
      "[Epoch 3] [Batch 5300] [G loss: 1.9802]\n",
      "[Epoch 3] [Batch 5400] [G loss: 1.3688]\n",
      "[Epoch 3] [Batch 5500] [G loss: 1.4229]\n",
      "[Epoch 3] [Batch 5600] [G loss: 1.1505]\n",
      "[Epoch 3] [Batch 5700] [G loss: 1.3468]\n",
      "[Epoch 3] [Batch 5800] [G loss: 1.1224]\n",
      "[Epoch 3] [Batch 5900] [G loss: 1.5276]\n",
      "[Epoch 3] [Batch 6000] [G loss: 1.6541]\n",
      "[Epoch 3] [Batch 6100] [G loss: 1.6463]\n",
      "[Epoch 3] [Batch 6200] [G loss: 1.0369]\n",
      "[Epoch 3] [Batch 6300] [G loss: 1.2061]\n",
      "[Epoch 3] [Batch 6400] [G loss: 1.7488]\n",
      "[Epoch 3] [Batch 6500] [G loss: 1.5191]\n",
      "[Epoch 3] [Batch 6600] [G loss: 1.4423]\n",
      "[Epoch 3] [Batch 6700] [G loss: 1.4945]\n",
      "[Epoch 3] [Batch 6800] [G loss: 1.4292]\n",
      "[Epoch 3] [Batch 6900] [G loss: 1.2810]\n",
      "[Epoch 3] [Batch 7000] [G loss: 1.3721]\n",
      "[Epoch 4] [Batch 0] [G loss: 1.5154]\n",
      "[Epoch 4] [Batch 100] [G loss: 1.7034]\n",
      "[Epoch 4] [Batch 200] [G loss: 1.6070]\n",
      "[Epoch 4] [Batch 300] [G loss: 1.5267]\n",
      "[Epoch 4] [Batch 400] [G loss: 1.3714]\n",
      "[Epoch 4] [Batch 500] [G loss: 1.2787]\n",
      "[Epoch 4] [Batch 600] [G loss: 1.1691]\n",
      "[Epoch 4] [Batch 700] [G loss: 1.3259]\n",
      "[Epoch 4] [Batch 800] [G loss: 1.2257]\n",
      "[Epoch 4] [Batch 900] [G loss: 1.4633]\n",
      "[Epoch 4] [Batch 1000] [G loss: 1.5992]\n",
      "[Epoch 4] [Batch 1100] [G loss: 1.1866]\n",
      "[Epoch 4] [Batch 1200] [G loss: 1.5633]\n",
      "[Epoch 4] [Batch 1300] [G loss: 1.1004]\n",
      "[Epoch 4] [Batch 1400] [G loss: 1.1471]\n",
      "[Epoch 4] [Batch 1500] [G loss: 1.1738]\n",
      "[Epoch 4] [Batch 1600] [G loss: 1.8417]\n",
      "[Epoch 4] [Batch 1700] [G loss: 1.4032]\n",
      "[Epoch 4] [Batch 1800] [G loss: 1.1127]\n",
      "[Epoch 4] [Batch 1900] [G loss: 1.3640]\n",
      "[Epoch 4] [Batch 2000] [G loss: 1.0248]\n",
      "[Epoch 4] [Batch 2100] [G loss: 1.1056]\n",
      "[Epoch 4] [Batch 2200] [G loss: 1.0506]\n",
      "[Epoch 4] [Batch 2300] [G loss: 0.8256]\n",
      "[Epoch 4] [Batch 2400] [G loss: 1.0865]\n",
      "[Epoch 4] [Batch 2500] [G loss: 1.1978]\n",
      "[Epoch 4] [Batch 2600] [G loss: 1.5670]\n",
      "[Epoch 4] [Batch 2700] [G loss: 1.2609]\n",
      "[Epoch 4] [Batch 2800] [G loss: 0.9769]\n",
      "[Epoch 4] [Batch 2900] [G loss: 1.1753]\n",
      "[Epoch 4] [Batch 3000] [G loss: 1.1546]\n",
      "[Epoch 4] [Batch 3100] [G loss: 1.1730]\n",
      "[Epoch 4] [Batch 3200] [G loss: 1.2940]\n",
      "[Epoch 4] [Batch 3300] [G loss: 1.3213]\n",
      "[Epoch 4] [Batch 3400] [G loss: 1.7556]\n",
      "[Epoch 4] [Batch 3500] [G loss: 1.3683]\n",
      "[Epoch 4] [Batch 3600] [G loss: 1.3058]\n",
      "[Epoch 4] [Batch 3700] [G loss: 1.0596]\n",
      "[Epoch 4] [Batch 3800] [G loss: 1.4423]\n",
      "[Epoch 4] [Batch 3900] [G loss: 1.2855]\n",
      "[Epoch 4] [Batch 4000] [G loss: 1.7836]\n",
      "[Epoch 4] [Batch 4100] [G loss: 1.0734]\n",
      "[Epoch 4] [Batch 4200] [G loss: 1.8166]\n",
      "[Epoch 4] [Batch 4300] [G loss: 1.1376]\n",
      "[Epoch 4] [Batch 4400] [G loss: 1.4780]\n",
      "[Epoch 4] [Batch 4500] [G loss: 1.1273]\n",
      "[Epoch 4] [Batch 4600] [G loss: 1.3948]\n",
      "[Epoch 4] [Batch 4700] [G loss: 1.2543]\n",
      "[Epoch 4] [Batch 4800] [G loss: 1.5902]\n",
      "[Epoch 4] [Batch 4900] [G loss: 1.3753]\n",
      "[Epoch 4] [Batch 5000] [G loss: 1.1196]\n",
      "[Epoch 4] [Batch 5100] [G loss: 1.2993]\n",
      "[Epoch 4] [Batch 5200] [G loss: 1.3750]\n",
      "[Epoch 4] [Batch 5300] [G loss: 1.6216]\n",
      "[Epoch 4] [Batch 5400] [G loss: 1.5471]\n",
      "[Epoch 4] [Batch 5500] [G loss: 1.5721]\n",
      "[Epoch 4] [Batch 5600] [G loss: 1.4728]\n",
      "[Epoch 4] [Batch 5700] [G loss: 1.1499]\n",
      "[Epoch 4] [Batch 5800] [G loss: 1.0879]\n",
      "[Epoch 4] [Batch 5900] [G loss: 1.5912]\n",
      "[Epoch 4] [Batch 6000] [G loss: 1.0645]\n",
      "[Epoch 4] [Batch 6100] [G loss: 1.4371]\n",
      "[Epoch 4] [Batch 6200] [G loss: 1.3749]\n",
      "[Epoch 4] [Batch 6300] [G loss: 1.0071]\n",
      "[Epoch 4] [Batch 6400] [G loss: 1.4679]\n",
      "[Epoch 4] [Batch 6500] [G loss: 1.5203]\n",
      "[Epoch 4] [Batch 6600] [G loss: 1.2437]\n",
      "[Epoch 4] [Batch 6700] [G loss: 1.6381]\n",
      "[Epoch 4] [Batch 6800] [G loss: 1.2035]\n",
      "[Epoch 4] [Batch 6900] [G loss: 1.4329]\n",
      "[Epoch 4] [Batch 7000] [G loss: 1.6456]\n",
      "[Epoch 5] [Batch 0] [G loss: 1.6418]\n",
      "[Epoch 5] [Batch 100] [G loss: 1.4728]\n",
      "[Epoch 5] [Batch 200] [G loss: 1.1011]\n",
      "[Epoch 5] [Batch 300] [G loss: 1.2295]\n",
      "[Epoch 5] [Batch 400] [G loss: 1.5528]\n",
      "[Epoch 5] [Batch 500] [G loss: 1.3490]\n",
      "[Epoch 5] [Batch 600] [G loss: 1.1392]\n",
      "[Epoch 5] [Batch 700] [G loss: 1.7044]\n",
      "[Epoch 5] [Batch 800] [G loss: 1.4280]\n",
      "[Epoch 5] [Batch 900] [G loss: 1.5723]\n",
      "[Epoch 5] [Batch 1000] [G loss: 1.4096]\n",
      "[Epoch 5] [Batch 1100] [G loss: 1.2736]\n",
      "[Epoch 5] [Batch 1200] [G loss: 1.2488]\n",
      "[Epoch 5] [Batch 1300] [G loss: 1.5162]\n",
      "[Epoch 5] [Batch 1400] [G loss: 1.4832]\n",
      "[Epoch 5] [Batch 1500] [G loss: 1.4656]\n",
      "[Epoch 5] [Batch 1600] [G loss: 2.0749]\n",
      "[Epoch 5] [Batch 1700] [G loss: 1.5900]\n",
      "[Epoch 5] [Batch 1800] [G loss: 1.5217]\n",
      "[Epoch 5] [Batch 1900] [G loss: 1.3465]\n",
      "[Epoch 5] [Batch 2000] [G loss: 1.8210]\n",
      "[Epoch 5] [Batch 2100] [G loss: 1.2069]\n",
      "[Epoch 5] [Batch 2200] [G loss: 1.6573]\n",
      "[Epoch 5] [Batch 2300] [G loss: 1.3637]\n",
      "[Epoch 5] [Batch 2400] [G loss: 1.3513]\n",
      "[Epoch 5] [Batch 2500] [G loss: 1.7784]\n",
      "[Epoch 5] [Batch 2600] [G loss: 1.0592]\n",
      "[Epoch 5] [Batch 2700] [G loss: 1.2864]\n",
      "[Epoch 5] [Batch 2800] [G loss: 1.0559]\n",
      "[Epoch 5] [Batch 2900] [G loss: 1.1757]\n",
      "[Epoch 5] [Batch 3000] [G loss: 1.2704]\n",
      "[Epoch 5] [Batch 3100] [G loss: 1.0189]\n",
      "[Epoch 5] [Batch 3200] [G loss: 1.0510]\n",
      "[Epoch 5] [Batch 3300] [G loss: 1.2154]\n",
      "[Epoch 5] [Batch 3400] [G loss: 1.0750]\n",
      "[Epoch 5] [Batch 3500] [G loss: 1.1803]\n",
      "[Epoch 5] [Batch 3600] [G loss: 1.2793]\n",
      "[Epoch 5] [Batch 3700] [G loss: 1.0123]\n",
      "[Epoch 5] [Batch 3800] [G loss: 1.4858]\n",
      "[Epoch 5] [Batch 3900] [G loss: 1.1221]\n",
      "[Epoch 5] [Batch 4000] [G loss: 1.4206]\n",
      "[Epoch 5] [Batch 4100] [G loss: 1.4803]\n",
      "[Epoch 5] [Batch 4200] [G loss: 1.4928]\n",
      "[Epoch 5] [Batch 4300] [G loss: 0.9356]\n",
      "[Epoch 5] [Batch 4400] [G loss: 1.3093]\n",
      "[Epoch 5] [Batch 4500] [G loss: 1.5173]\n",
      "[Epoch 5] [Batch 4600] [G loss: 1.7352]\n",
      "[Epoch 5] [Batch 4700] [G loss: 1.4676]\n",
      "[Epoch 5] [Batch 4800] [G loss: 1.2212]\n",
      "[Epoch 5] [Batch 4900] [G loss: 1.3668]\n",
      "[Epoch 5] [Batch 5000] [G loss: 1.4779]\n",
      "[Epoch 5] [Batch 5100] [G loss: 1.3353]\n",
      "[Epoch 5] [Batch 5200] [G loss: 1.6296]\n",
      "[Epoch 5] [Batch 5300] [G loss: 1.5593]\n",
      "[Epoch 5] [Batch 5400] [G loss: 1.3758]\n",
      "[Epoch 5] [Batch 5500] [G loss: 1.5200]\n",
      "[Epoch 5] [Batch 5600] [G loss: 1.6214]\n",
      "[Epoch 5] [Batch 5700] [G loss: 1.4400]\n",
      "[Epoch 5] [Batch 5800] [G loss: 1.2699]\n",
      "[Epoch 5] [Batch 5900] [G loss: 1.2132]\n",
      "[Epoch 5] [Batch 6000] [G loss: 1.1669]\n",
      "[Epoch 5] [Batch 6100] [G loss: 1.2608]\n",
      "[Epoch 5] [Batch 6200] [G loss: 0.7887]\n",
      "[Epoch 5] [Batch 6300] [G loss: 1.7010]\n",
      "[Epoch 5] [Batch 6400] [G loss: 1.3196]\n",
      "[Epoch 5] [Batch 6500] [G loss: 1.5052]\n",
      "[Epoch 5] [Batch 6600] [G loss: 1.1965]\n",
      "[Epoch 5] [Batch 6700] [G loss: 1.4888]\n",
      "[Epoch 5] [Batch 6800] [G loss: 1.4782]\n",
      "[Epoch 5] [Batch 6900] [G loss: 1.3992]\n",
      "[Epoch 5] [Batch 7000] [G loss: 1.2658]\n",
      "üíæ UNet Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints_unet/epoch_5.pth\n",
      "[Epoch 6] [Batch 0] [G loss: 0.9406]\n",
      "[Epoch 6] [Batch 100] [G loss: 1.4572]\n",
      "[Epoch 6] [Batch 200] [G loss: 1.2179]\n",
      "[Epoch 6] [Batch 300] [G loss: 1.5458]\n",
      "[Epoch 6] [Batch 400] [G loss: 1.2393]\n",
      "[Epoch 6] [Batch 500] [G loss: 1.5607]\n",
      "[Epoch 6] [Batch 600] [G loss: 1.9261]\n",
      "[Epoch 6] [Batch 700] [G loss: 1.4433]\n",
      "[Epoch 6] [Batch 800] [G loss: 1.5434]\n",
      "[Epoch 6] [Batch 900] [G loss: 1.4513]\n",
      "[Epoch 6] [Batch 1000] [G loss: 1.7693]\n",
      "[Epoch 6] [Batch 1100] [G loss: 1.2831]\n",
      "[Epoch 6] [Batch 1200] [G loss: 1.5065]\n",
      "[Epoch 6] [Batch 1300] [G loss: 1.8331]\n",
      "[Epoch 6] [Batch 1400] [G loss: 1.4872]\n",
      "[Epoch 6] [Batch 1500] [G loss: 1.3000]\n",
      "[Epoch 6] [Batch 1600] [G loss: 1.1036]\n",
      "[Epoch 6] [Batch 1700] [G loss: 1.4458]\n",
      "[Epoch 6] [Batch 1800] [G loss: 1.3621]\n",
      "[Epoch 6] [Batch 1900] [G loss: 1.7549]\n",
      "[Epoch 6] [Batch 2000] [G loss: 1.4714]\n",
      "[Epoch 6] [Batch 2100] [G loss: 1.1440]\n",
      "[Epoch 6] [Batch 2200] [G loss: 1.4484]\n",
      "[Epoch 6] [Batch 2300] [G loss: 1.4661]\n",
      "[Epoch 6] [Batch 2400] [G loss: 1.1631]\n",
      "[Epoch 6] [Batch 2500] [G loss: 2.0133]\n",
      "[Epoch 6] [Batch 2600] [G loss: 1.5404]\n",
      "[Epoch 6] [Batch 2700] [G loss: 1.3530]\n",
      "[Epoch 6] [Batch 2800] [G loss: 1.6161]\n",
      "[Epoch 6] [Batch 2900] [G loss: 1.3083]\n",
      "[Epoch 6] [Batch 3000] [G loss: 1.2845]\n",
      "[Epoch 6] [Batch 3100] [G loss: 1.1513]\n",
      "[Epoch 6] [Batch 3200] [G loss: 0.8348]\n",
      "[Epoch 6] [Batch 3300] [G loss: 1.8821]\n",
      "[Epoch 6] [Batch 3400] [G loss: 1.0897]\n",
      "[Epoch 6] [Batch 3500] [G loss: 1.1014]\n",
      "[Epoch 6] [Batch 3600] [G loss: 1.3650]\n",
      "[Epoch 6] [Batch 3700] [G loss: 1.4376]\n",
      "[Epoch 6] [Batch 3800] [G loss: 1.3814]\n",
      "[Epoch 6] [Batch 3900] [G loss: 1.3353]\n",
      "[Epoch 6] [Batch 4000] [G loss: 1.4357]\n",
      "[Epoch 6] [Batch 4100] [G loss: 1.6156]\n",
      "[Epoch 6] [Batch 4200] [G loss: 1.2718]\n",
      "[Epoch 6] [Batch 4300] [G loss: 1.4750]\n",
      "[Epoch 6] [Batch 4400] [G loss: 1.3185]\n",
      "[Epoch 6] [Batch 4500] [G loss: 1.4446]\n",
      "[Epoch 6] [Batch 4600] [G loss: 1.6191]\n",
      "[Epoch 6] [Batch 4700] [G loss: 1.7014]\n",
      "[Epoch 6] [Batch 4800] [G loss: 1.4206]\n",
      "[Epoch 6] [Batch 4900] [G loss: 1.6011]\n",
      "[Epoch 6] [Batch 5000] [G loss: 1.7922]\n",
      "[Epoch 6] [Batch 5100] [G loss: 1.5594]\n",
      "[Epoch 6] [Batch 5200] [G loss: 1.5202]\n",
      "[Epoch 6] [Batch 5300] [G loss: 1.1385]\n",
      "[Epoch 6] [Batch 5400] [G loss: 1.3145]\n",
      "[Epoch 6] [Batch 5500] [G loss: 1.3545]\n",
      "[Epoch 6] [Batch 5600] [G loss: 1.4872]\n",
      "[Epoch 6] [Batch 5700] [G loss: 1.3328]\n",
      "[Epoch 6] [Batch 5800] [G loss: 1.3310]\n",
      "[Epoch 6] [Batch 5900] [G loss: 1.6196]\n",
      "[Epoch 6] [Batch 6000] [G loss: 1.4079]\n",
      "[Epoch 6] [Batch 6100] [G loss: 1.5693]\n",
      "[Epoch 6] [Batch 6200] [G loss: 1.6425]\n",
      "[Epoch 6] [Batch 6300] [G loss: 1.3158]\n",
      "[Epoch 6] [Batch 6400] [G loss: 1.4815]\n",
      "[Epoch 6] [Batch 6500] [G loss: 1.1378]\n",
      "[Epoch 6] [Batch 6600] [G loss: 1.7702]\n",
      "[Epoch 6] [Batch 6700] [G loss: 1.7179]\n",
      "[Epoch 6] [Batch 6800] [G loss: 1.3153]\n",
      "[Epoch 6] [Batch 6900] [G loss: 1.6336]\n",
      "[Epoch 6] [Batch 7000] [G loss: 1.3831]\n",
      "[Epoch 7] [Batch 0] [G loss: 1.3816]\n",
      "[Epoch 7] [Batch 100] [G loss: 1.7542]\n",
      "[Epoch 7] [Batch 200] [G loss: 1.3271]\n",
      "[Epoch 7] [Batch 300] [G loss: 1.7289]\n",
      "[Epoch 7] [Batch 400] [G loss: 1.4013]\n",
      "[Epoch 7] [Batch 500] [G loss: 0.9524]\n",
      "[Epoch 7] [Batch 600] [G loss: 1.4317]\n",
      "[Epoch 7] [Batch 700] [G loss: 1.6027]\n",
      "[Epoch 7] [Batch 800] [G loss: 1.5627]\n",
      "[Epoch 7] [Batch 900] [G loss: 1.5367]\n",
      "[Epoch 7] [Batch 1000] [G loss: 1.5000]\n",
      "[Epoch 7] [Batch 1100] [G loss: 1.3898]\n",
      "[Epoch 7] [Batch 1200] [G loss: 1.2753]\n",
      "[Epoch 7] [Batch 1300] [G loss: 1.4683]\n",
      "[Epoch 7] [Batch 1400] [G loss: 1.3918]\n",
      "[Epoch 7] [Batch 1500] [G loss: 1.5432]\n",
      "[Epoch 7] [Batch 1600] [G loss: 1.2105]\n",
      "[Epoch 7] [Batch 1700] [G loss: 0.7568]\n",
      "[Epoch 7] [Batch 1800] [G loss: 1.4282]\n",
      "[Epoch 7] [Batch 1900] [G loss: 1.4899]\n",
      "[Epoch 7] [Batch 2000] [G loss: 1.5107]\n",
      "[Epoch 7] [Batch 2100] [G loss: 1.0901]\n",
      "[Epoch 7] [Batch 2200] [G loss: 1.3260]\n",
      "[Epoch 7] [Batch 2300] [G loss: 1.4053]\n",
      "[Epoch 7] [Batch 2400] [G loss: 1.3471]\n",
      "[Epoch 7] [Batch 2500] [G loss: 1.2959]\n",
      "[Epoch 7] [Batch 2600] [G loss: 1.7487]\n",
      "[Epoch 7] [Batch 2700] [G loss: 1.3494]\n",
      "[Epoch 7] [Batch 2800] [G loss: 1.9632]\n",
      "[Epoch 7] [Batch 2900] [G loss: 1.6071]\n",
      "[Epoch 7] [Batch 3000] [G loss: 1.4240]\n",
      "[Epoch 7] [Batch 3100] [G loss: 1.3495]\n",
      "[Epoch 7] [Batch 3200] [G loss: 1.7367]\n",
      "[Epoch 7] [Batch 3300] [G loss: 1.4736]\n",
      "[Epoch 7] [Batch 3400] [G loss: 1.3122]\n",
      "[Epoch 7] [Batch 3500] [G loss: 1.3160]\n",
      "[Epoch 7] [Batch 3600] [G loss: 1.8759]\n",
      "[Epoch 7] [Batch 3700] [G loss: 1.7964]\n",
      "[Epoch 7] [Batch 3800] [G loss: 1.4549]\n",
      "[Epoch 7] [Batch 3900] [G loss: 1.4933]\n",
      "[Epoch 7] [Batch 4000] [G loss: 1.4398]\n",
      "[Epoch 7] [Batch 4100] [G loss: 1.4057]\n",
      "[Epoch 7] [Batch 4200] [G loss: 1.6261]\n",
      "[Epoch 7] [Batch 4300] [G loss: 1.3569]\n",
      "[Epoch 7] [Batch 4400] [G loss: 1.3820]\n",
      "[Epoch 7] [Batch 4500] [G loss: 1.4791]\n",
      "[Epoch 7] [Batch 4600] [G loss: 1.5961]\n",
      "[Epoch 7] [Batch 4700] [G loss: 1.3121]\n",
      "[Epoch 7] [Batch 4800] [G loss: 1.5048]\n",
      "[Epoch 7] [Batch 4900] [G loss: 1.0681]\n",
      "[Epoch 7] [Batch 5000] [G loss: 1.7031]\n",
      "[Epoch 7] [Batch 5100] [G loss: 1.4328]\n",
      "[Epoch 7] [Batch 5200] [G loss: 1.2225]\n",
      "[Epoch 7] [Batch 5300] [G loss: 1.4351]\n",
      "[Epoch 7] [Batch 5400] [G loss: 1.3447]\n",
      "[Epoch 7] [Batch 5500] [G loss: 1.0752]\n",
      "[Epoch 7] [Batch 5600] [G loss: 1.6612]\n",
      "[Epoch 7] [Batch 5700] [G loss: 1.6167]\n",
      "[Epoch 7] [Batch 5800] [G loss: 1.5387]\n",
      "[Epoch 7] [Batch 5900] [G loss: 1.4176]\n",
      "[Epoch 7] [Batch 6000] [G loss: 1.3173]\n",
      "[Epoch 7] [Batch 6100] [G loss: 1.3491]\n",
      "[Epoch 7] [Batch 6200] [G loss: 1.2035]\n",
      "[Epoch 7] [Batch 6300] [G loss: 1.7663]\n",
      "[Epoch 7] [Batch 6400] [G loss: 1.3521]\n",
      "[Epoch 7] [Batch 6500] [G loss: 1.4003]\n",
      "[Epoch 7] [Batch 6600] [G loss: 1.7905]\n",
      "[Epoch 7] [Batch 6700] [G loss: 1.7368]\n",
      "[Epoch 7] [Batch 6800] [G loss: 1.3903]\n",
      "[Epoch 7] [Batch 6900] [G loss: 1.1633]\n",
      "[Epoch 7] [Batch 7000] [G loss: 1.4624]\n",
      "[Epoch 8] [Batch 0] [G loss: 1.6083]\n",
      "[Epoch 8] [Batch 100] [G loss: 1.2467]\n",
      "[Epoch 8] [Batch 200] [G loss: 1.4503]\n",
      "[Epoch 8] [Batch 300] [G loss: 1.2997]\n",
      "[Epoch 8] [Batch 400] [G loss: 1.7688]\n",
      "[Epoch 8] [Batch 500] [G loss: 1.1375]\n",
      "[Epoch 8] [Batch 600] [G loss: 1.2479]\n",
      "[Epoch 8] [Batch 700] [G loss: 2.4198]\n",
      "[Epoch 8] [Batch 800] [G loss: 1.4852]\n",
      "[Epoch 8] [Batch 900] [G loss: 1.2949]\n",
      "[Epoch 8] [Batch 1000] [G loss: 1.4902]\n",
      "[Epoch 8] [Batch 1100] [G loss: 1.4800]\n",
      "[Epoch 8] [Batch 1200] [G loss: 1.5056]\n",
      "[Epoch 8] [Batch 1300] [G loss: 1.2893]\n",
      "[Epoch 8] [Batch 1400] [G loss: 1.1876]\n",
      "[Epoch 8] [Batch 1500] [G loss: 1.4953]\n",
      "[Epoch 8] [Batch 1600] [G loss: 1.2236]\n",
      "[Epoch 8] [Batch 1700] [G loss: 1.4659]\n",
      "[Epoch 8] [Batch 1800] [G loss: 1.4097]\n",
      "[Epoch 8] [Batch 1900] [G loss: 1.3222]\n",
      "[Epoch 8] [Batch 2000] [G loss: 1.0381]\n",
      "[Epoch 8] [Batch 2100] [G loss: 1.1972]\n",
      "[Epoch 8] [Batch 2200] [G loss: 1.4042]\n",
      "[Epoch 8] [Batch 2300] [G loss: 1.2374]\n",
      "[Epoch 8] [Batch 2400] [G loss: 1.1150]\n",
      "[Epoch 8] [Batch 2500] [G loss: 1.0671]\n",
      "[Epoch 8] [Batch 2600] [G loss: 1.7068]\n",
      "[Epoch 8] [Batch 2700] [G loss: 1.4292]\n",
      "[Epoch 8] [Batch 2800] [G loss: 1.1176]\n",
      "[Epoch 8] [Batch 2900] [G loss: 1.3255]\n",
      "[Epoch 8] [Batch 3000] [G loss: 1.4340]\n",
      "[Epoch 8] [Batch 3100] [G loss: 1.4975]\n",
      "[Epoch 8] [Batch 3200] [G loss: 1.5165]\n",
      "[Epoch 8] [Batch 3300] [G loss: 1.3741]\n",
      "[Epoch 8] [Batch 3400] [G loss: 1.4892]\n",
      "[Epoch 8] [Batch 3500] [G loss: 1.4758]\n",
      "[Epoch 8] [Batch 3600] [G loss: 1.8756]\n",
      "[Epoch 8] [Batch 3700] [G loss: 1.3835]\n",
      "[Epoch 8] [Batch 3800] [G loss: 1.5287]\n",
      "[Epoch 8] [Batch 3900] [G loss: 1.3003]\n",
      "[Epoch 8] [Batch 4000] [G loss: 1.5243]\n",
      "[Epoch 8] [Batch 4100] [G loss: 1.4696]\n",
      "[Epoch 8] [Batch 4200] [G loss: 1.5646]\n",
      "[Epoch 8] [Batch 4300] [G loss: 1.4995]\n",
      "[Epoch 8] [Batch 4400] [G loss: 1.6145]\n",
      "[Epoch 8] [Batch 4500] [G loss: 1.3005]\n",
      "[Epoch 8] [Batch 4600] [G loss: 1.3088]\n",
      "[Epoch 8] [Batch 4700] [G loss: 1.4014]\n",
      "[Epoch 8] [Batch 4800] [G loss: 1.5193]\n",
      "[Epoch 8] [Batch 4900] [G loss: 1.7412]\n",
      "[Epoch 8] [Batch 5000] [G loss: 1.3788]\n",
      "[Epoch 8] [Batch 5100] [G loss: 1.3623]\n",
      "[Epoch 8] [Batch 5200] [G loss: 1.5880]\n",
      "[Epoch 8] [Batch 5300] [G loss: 1.3560]\n",
      "[Epoch 8] [Batch 5400] [G loss: 1.3145]\n",
      "[Epoch 8] [Batch 5500] [G loss: 1.2415]\n",
      "[Epoch 8] [Batch 5600] [G loss: 1.4671]\n",
      "[Epoch 8] [Batch 5700] [G loss: 1.2882]\n",
      "[Epoch 8] [Batch 5800] [G loss: 1.2682]\n",
      "[Epoch 8] [Batch 5900] [G loss: 1.7599]\n",
      "[Epoch 8] [Batch 6000] [G loss: 1.4975]\n",
      "[Epoch 8] [Batch 6100] [G loss: 1.7511]\n",
      "[Epoch 8] [Batch 6200] [G loss: 1.2136]\n",
      "[Epoch 8] [Batch 6300] [G loss: 1.9039]\n",
      "[Epoch 8] [Batch 6400] [G loss: 1.3241]\n",
      "[Epoch 8] [Batch 6500] [G loss: 1.5187]\n",
      "[Epoch 8] [Batch 6600] [G loss: 1.5133]\n",
      "[Epoch 8] [Batch 6700] [G loss: 1.7008]\n",
      "[Epoch 8] [Batch 6800] [G loss: 1.2551]\n",
      "[Epoch 8] [Batch 6900] [G loss: 1.4710]\n",
      "[Epoch 8] [Batch 7000] [G loss: 1.8940]\n",
      "[Epoch 9] [Batch 0] [G loss: 1.3680]\n",
      "[Epoch 9] [Batch 100] [G loss: 1.3440]\n",
      "[Epoch 9] [Batch 200] [G loss: 1.5547]\n",
      "[Epoch 9] [Batch 300] [G loss: 1.4409]\n",
      "[Epoch 9] [Batch 400] [G loss: 1.9297]\n",
      "[Epoch 9] [Batch 500] [G loss: 1.8648]\n",
      "[Epoch 9] [Batch 600] [G loss: 1.1811]\n",
      "[Epoch 9] [Batch 700] [G loss: 1.6552]\n",
      "[Epoch 9] [Batch 800] [G loss: 1.4701]\n",
      "[Epoch 9] [Batch 900] [G loss: 1.4432]\n",
      "[Epoch 9] [Batch 1000] [G loss: 1.6255]\n",
      "[Epoch 9] [Batch 1100] [G loss: 1.7690]\n",
      "[Epoch 9] [Batch 1200] [G loss: 1.5463]\n",
      "[Epoch 9] [Batch 1300] [G loss: 1.4964]\n",
      "[Epoch 9] [Batch 1400] [G loss: 1.6212]\n",
      "[Epoch 9] [Batch 1500] [G loss: 1.3862]\n",
      "[Epoch 9] [Batch 1600] [G loss: 1.9013]\n",
      "[Epoch 9] [Batch 1700] [G loss: 1.4098]\n",
      "[Epoch 9] [Batch 1800] [G loss: 1.3392]\n",
      "[Epoch 9] [Batch 1900] [G loss: 1.6854]\n",
      "[Epoch 9] [Batch 2000] [G loss: 1.5196]\n",
      "[Epoch 9] [Batch 2100] [G loss: 1.4890]\n",
      "[Epoch 9] [Batch 2200] [G loss: 1.5700]\n",
      "[Epoch 9] [Batch 2300] [G loss: 1.3013]\n",
      "[Epoch 9] [Batch 2400] [G loss: 1.2738]\n",
      "[Epoch 9] [Batch 2500] [G loss: 1.4251]\n",
      "[Epoch 9] [Batch 2600] [G loss: 1.5458]\n",
      "[Epoch 9] [Batch 2700] [G loss: 1.3906]\n",
      "[Epoch 9] [Batch 2800] [G loss: 1.5251]\n",
      "[Epoch 9] [Batch 2900] [G loss: 1.2295]\n",
      "[Epoch 9] [Batch 3000] [G loss: 0.8744]\n",
      "[Epoch 9] [Batch 3100] [G loss: 1.6437]\n",
      "[Epoch 9] [Batch 3200] [G loss: 1.0015]\n",
      "[Epoch 9] [Batch 3300] [G loss: 1.4720]\n",
      "[Epoch 9] [Batch 3400] [G loss: 1.2446]\n",
      "[Epoch 9] [Batch 3500] [G loss: 1.2865]\n",
      "[Epoch 9] [Batch 3600] [G loss: 1.2833]\n",
      "[Epoch 9] [Batch 3700] [G loss: 1.2661]\n",
      "[Epoch 9] [Batch 3800] [G loss: 1.2701]\n",
      "[Epoch 9] [Batch 3900] [G loss: 1.4884]\n",
      "[Epoch 9] [Batch 4000] [G loss: 1.3722]\n",
      "[Epoch 9] [Batch 4100] [G loss: 1.2412]\n",
      "[Epoch 9] [Batch 4200] [G loss: 1.5502]\n",
      "[Epoch 9] [Batch 4300] [G loss: 1.7958]\n",
      "[Epoch 9] [Batch 4400] [G loss: 1.1264]\n",
      "[Epoch 9] [Batch 4500] [G loss: 1.3765]\n",
      "[Epoch 9] [Batch 4600] [G loss: 1.2557]\n",
      "[Epoch 9] [Batch 4700] [G loss: 1.6630]\n",
      "[Epoch 9] [Batch 4800] [G loss: 1.5665]\n",
      "[Epoch 9] [Batch 4900] [G loss: 1.3133]\n",
      "[Epoch 9] [Batch 5000] [G loss: 1.3607]\n",
      "[Epoch 9] [Batch 5100] [G loss: 1.5920]\n",
      "[Epoch 9] [Batch 5200] [G loss: 1.5545]\n",
      "[Epoch 9] [Batch 5300] [G loss: 1.6419]\n",
      "[Epoch 9] [Batch 5400] [G loss: 1.3394]\n",
      "[Epoch 9] [Batch 5500] [G loss: 1.5682]\n",
      "[Epoch 9] [Batch 5600] [G loss: 1.3359]\n",
      "[Epoch 9] [Batch 5700] [G loss: 1.3793]\n",
      "[Epoch 9] [Batch 5800] [G loss: 1.5971]\n",
      "[Epoch 9] [Batch 5900] [G loss: 1.6974]\n",
      "[Epoch 9] [Batch 6000] [G loss: 2.1227]\n",
      "[Epoch 9] [Batch 6100] [G loss: 1.5154]\n",
      "[Epoch 9] [Batch 6200] [G loss: 1.2206]\n",
      "[Epoch 9] [Batch 6300] [G loss: 1.8432]\n",
      "[Epoch 9] [Batch 6400] [G loss: 1.2981]\n",
      "[Epoch 9] [Batch 6500] [G loss: 1.5471]\n",
      "[Epoch 9] [Batch 6600] [G loss: 1.2865]\n",
      "[Epoch 9] [Batch 6700] [G loss: 1.2400]\n",
      "[Epoch 9] [Batch 6800] [G loss: 1.3261]\n",
      "[Epoch 9] [Batch 6900] [G loss: 1.2358]\n",
      "[Epoch 9] [Batch 7000] [G loss: 1.4724]\n",
      "[Epoch 10] [Batch 0] [G loss: 1.2605]\n",
      "[Epoch 10] [Batch 100] [G loss: 1.8991]\n",
      "[Epoch 10] [Batch 200] [G loss: 1.5071]\n",
      "[Epoch 10] [Batch 300] [G loss: 1.5138]\n",
      "[Epoch 10] [Batch 400] [G loss: 1.4058]\n",
      "[Epoch 10] [Batch 500] [G loss: 1.3265]\n",
      "[Epoch 10] [Batch 600] [G loss: 1.1682]\n",
      "[Epoch 10] [Batch 700] [G loss: 1.2430]\n",
      "[Epoch 10] [Batch 800] [G loss: 1.6130]\n",
      "[Epoch 10] [Batch 900] [G loss: 1.4875]\n",
      "[Epoch 10] [Batch 1000] [G loss: 1.4822]\n",
      "[Epoch 10] [Batch 1100] [G loss: 1.3012]\n",
      "[Epoch 10] [Batch 1200] [G loss: 1.4423]\n",
      "[Epoch 10] [Batch 1300] [G loss: 1.4677]\n",
      "[Epoch 10] [Batch 1400] [G loss: 1.6328]\n",
      "[Epoch 10] [Batch 1500] [G loss: 1.1612]\n",
      "[Epoch 10] [Batch 1600] [G loss: 1.8257]\n",
      "[Epoch 10] [Batch 1700] [G loss: 1.8539]\n",
      "[Epoch 10] [Batch 1800] [G loss: 1.3154]\n",
      "[Epoch 10] [Batch 1900] [G loss: 1.4544]\n",
      "[Epoch 10] [Batch 2000] [G loss: 1.4867]\n",
      "[Epoch 10] [Batch 2100] [G loss: 1.2420]\n",
      "[Epoch 10] [Batch 2200] [G loss: 1.3424]\n",
      "[Epoch 10] [Batch 2300] [G loss: 1.2439]\n",
      "[Epoch 10] [Batch 2400] [G loss: 1.5259]\n",
      "[Epoch 10] [Batch 2500] [G loss: 1.3853]\n",
      "[Epoch 10] [Batch 2600] [G loss: 1.4325]\n",
      "[Epoch 10] [Batch 2700] [G loss: 1.4510]\n",
      "[Epoch 10] [Batch 2800] [G loss: 1.3459]\n",
      "[Epoch 10] [Batch 2900] [G loss: 1.5928]\n",
      "[Epoch 10] [Batch 3000] [G loss: 1.3654]\n",
      "[Epoch 10] [Batch 3100] [G loss: 1.4842]\n",
      "[Epoch 10] [Batch 3200] [G loss: 1.4621]\n",
      "[Epoch 10] [Batch 3300] [G loss: 1.2862]\n",
      "[Epoch 10] [Batch 3400] [G loss: 1.4940]\n",
      "[Epoch 10] [Batch 3500] [G loss: 1.7616]\n",
      "[Epoch 10] [Batch 3600] [G loss: 1.4048]\n",
      "[Epoch 10] [Batch 3700] [G loss: 1.7678]\n",
      "[Epoch 10] [Batch 3800] [G loss: 1.1637]\n",
      "[Epoch 10] [Batch 3900] [G loss: 1.3570]\n",
      "[Epoch 10] [Batch 4000] [G loss: 1.0945]\n",
      "[Epoch 10] [Batch 4100] [G loss: 1.4556]\n",
      "[Epoch 10] [Batch 4200] [G loss: 1.4045]\n",
      "[Epoch 10] [Batch 4300] [G loss: 1.5681]\n",
      "[Epoch 10] [Batch 4400] [G loss: 1.3107]\n",
      "[Epoch 10] [Batch 4500] [G loss: 1.3366]\n",
      "[Epoch 10] [Batch 4600] [G loss: 1.4306]\n",
      "[Epoch 10] [Batch 4700] [G loss: 1.1432]\n",
      "[Epoch 10] [Batch 4800] [G loss: 1.6087]\n",
      "[Epoch 10] [Batch 4900] [G loss: 1.1041]\n",
      "[Epoch 10] [Batch 5000] [G loss: 1.4503]\n",
      "[Epoch 10] [Batch 5100] [G loss: 1.4608]\n",
      "[Epoch 10] [Batch 5200] [G loss: 1.5095]\n",
      "[Epoch 10] [Batch 5300] [G loss: 1.5541]\n",
      "[Epoch 10] [Batch 5400] [G loss: 1.2467]\n",
      "[Epoch 10] [Batch 5500] [G loss: 1.3453]\n",
      "[Epoch 10] [Batch 5600] [G loss: 1.1355]\n",
      "[Epoch 10] [Batch 5700] [G loss: 1.4384]\n",
      "[Epoch 10] [Batch 5800] [G loss: 1.5123]\n",
      "[Epoch 10] [Batch 5900] [G loss: 1.4603]\n",
      "[Epoch 10] [Batch 6000] [G loss: 1.2314]\n",
      "[Epoch 10] [Batch 6100] [G loss: 1.3442]\n",
      "[Epoch 10] [Batch 6200] [G loss: 1.3663]\n",
      "[Epoch 10] [Batch 6300] [G loss: 1.2218]\n",
      "[Epoch 10] [Batch 6400] [G loss: 1.2477]\n",
      "[Epoch 10] [Batch 6500] [G loss: 1.2554]\n",
      "[Epoch 10] [Batch 6600] [G loss: 1.2938]\n",
      "[Epoch 10] [Batch 6700] [G loss: 1.2847]\n",
      "[Epoch 10] [Batch 6800] [G loss: 1.4531]\n",
      "[Epoch 10] [Batch 6900] [G loss: 1.7422]\n",
      "[Epoch 10] [Batch 7000] [G loss: 1.3464]\n",
      "üíæ UNet Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints_unet/epoch_10.pth\n",
      "[Epoch 11] [Batch 0] [G loss: 1.6272]\n",
      "[Epoch 11] [Batch 100] [G loss: 1.5544]\n",
      "[Epoch 11] [Batch 200] [G loss: 1.5738]\n",
      "[Epoch 11] [Batch 300] [G loss: 1.1376]\n",
      "[Epoch 11] [Batch 400] [G loss: 1.6867]\n",
      "[Epoch 11] [Batch 500] [G loss: 1.7974]\n",
      "[Epoch 11] [Batch 600] [G loss: 1.4931]\n",
      "[Epoch 11] [Batch 700] [G loss: 1.4904]\n",
      "[Epoch 11] [Batch 800] [G loss: 1.2990]\n",
      "[Epoch 11] [Batch 900] [G loss: 1.0490]\n",
      "[Epoch 11] [Batch 1000] [G loss: 1.6556]\n",
      "[Epoch 11] [Batch 1100] [G loss: 1.2968]\n",
      "[Epoch 11] [Batch 1200] [G loss: 1.1997]\n",
      "[Epoch 11] [Batch 1300] [G loss: 1.5428]\n",
      "[Epoch 11] [Batch 1400] [G loss: 1.2515]\n",
      "[Epoch 11] [Batch 1500] [G loss: 1.4789]\n",
      "[Epoch 11] [Batch 1600] [G loss: 1.4798]\n",
      "[Epoch 11] [Batch 1700] [G loss: 1.2971]\n",
      "[Epoch 11] [Batch 1800] [G loss: 1.3168]\n",
      "[Epoch 11] [Batch 1900] [G loss: 1.6324]\n",
      "[Epoch 11] [Batch 2000] [G loss: 1.3558]\n",
      "[Epoch 11] [Batch 2100] [G loss: 1.6432]\n",
      "[Epoch 11] [Batch 2200] [G loss: 1.4836]\n",
      "[Epoch 11] [Batch 2300] [G loss: 1.5803]\n",
      "[Epoch 11] [Batch 2400] [G loss: 1.7175]\n",
      "[Epoch 11] [Batch 2500] [G loss: 1.3897]\n",
      "[Epoch 11] [Batch 2600] [G loss: 1.3770]\n",
      "[Epoch 11] [Batch 2700] [G loss: 1.5815]\n",
      "[Epoch 11] [Batch 2800] [G loss: 1.2643]\n",
      "[Epoch 11] [Batch 2900] [G loss: 1.4692]\n",
      "[Epoch 11] [Batch 3000] [G loss: 1.6574]\n",
      "[Epoch 11] [Batch 3100] [G loss: 1.3204]\n",
      "[Epoch 11] [Batch 3200] [G loss: 1.3235]\n",
      "[Epoch 11] [Batch 3300] [G loss: 1.3682]\n",
      "[Epoch 11] [Batch 3400] [G loss: 1.1269]\n",
      "[Epoch 11] [Batch 3500] [G loss: 1.4713]\n",
      "[Epoch 11] [Batch 3600] [G loss: 1.3243]\n",
      "[Epoch 11] [Batch 3700] [G loss: 1.5273]\n",
      "[Epoch 11] [Batch 3800] [G loss: 1.8242]\n",
      "[Epoch 11] [Batch 3900] [G loss: 1.1910]\n",
      "[Epoch 11] [Batch 4000] [G loss: 1.4860]\n",
      "[Epoch 11] [Batch 4100] [G loss: 1.1313]\n",
      "[Epoch 11] [Batch 4200] [G loss: 1.4612]\n",
      "[Epoch 11] [Batch 4300] [G loss: 1.1028]\n",
      "[Epoch 11] [Batch 4400] [G loss: 1.0566]\n",
      "[Epoch 11] [Batch 4500] [G loss: 1.4183]\n",
      "[Epoch 11] [Batch 4600] [G loss: 1.3621]\n",
      "[Epoch 11] [Batch 4700] [G loss: 1.4114]\n",
      "[Epoch 11] [Batch 4800] [G loss: 1.4142]\n",
      "[Epoch 11] [Batch 4900] [G loss: 1.5604]\n",
      "[Epoch 11] [Batch 5000] [G loss: 1.8377]\n",
      "[Epoch 11] [Batch 5100] [G loss: 0.9362]\n",
      "[Epoch 11] [Batch 5200] [G loss: 1.4408]\n",
      "[Epoch 11] [Batch 5300] [G loss: 1.3218]\n",
      "[Epoch 11] [Batch 5400] [G loss: 1.6108]\n",
      "[Epoch 11] [Batch 5500] [G loss: 1.6860]\n",
      "[Epoch 11] [Batch 5600] [G loss: 1.6771]\n",
      "[Epoch 11] [Batch 5700] [G loss: 1.0618]\n",
      "[Epoch 11] [Batch 5800] [G loss: 1.4096]\n",
      "[Epoch 11] [Batch 5900] [G loss: 1.4043]\n",
      "[Epoch 11] [Batch 6000] [G loss: 1.4886]\n",
      "[Epoch 11] [Batch 6100] [G loss: 1.3890]\n",
      "[Epoch 11] [Batch 6200] [G loss: 1.7523]\n",
      "[Epoch 11] [Batch 6300] [G loss: 1.3764]\n",
      "[Epoch 11] [Batch 6400] [G loss: 1.6051]\n",
      "[Epoch 11] [Batch 6500] [G loss: 1.4439]\n",
      "[Epoch 11] [Batch 6600] [G loss: 1.3833]\n",
      "[Epoch 11] [Batch 6700] [G loss: 1.7405]\n",
      "[Epoch 11] [Batch 6800] [G loss: 1.4593]\n",
      "[Epoch 11] [Batch 6900] [G loss: 1.5692]\n",
      "[Epoch 11] [Batch 7000] [G loss: 2.1421]\n",
      "[Epoch 12] [Batch 0] [G loss: 1.4525]\n",
      "[Epoch 12] [Batch 100] [G loss: 1.3177]\n",
      "[Epoch 12] [Batch 200] [G loss: 1.4914]\n",
      "[Epoch 12] [Batch 300] [G loss: 1.5570]\n",
      "[Epoch 12] [Batch 400] [G loss: 1.2767]\n",
      "[Epoch 12] [Batch 500] [G loss: 1.3396]\n",
      "[Epoch 12] [Batch 600] [G loss: 1.2958]\n",
      "[Epoch 12] [Batch 700] [G loss: 1.6951]\n",
      "[Epoch 12] [Batch 800] [G loss: 1.5975]\n",
      "[Epoch 12] [Batch 900] [G loss: 1.3414]\n",
      "[Epoch 12] [Batch 1000] [G loss: 1.5382]\n",
      "[Epoch 12] [Batch 1100] [G loss: 1.3588]\n",
      "[Epoch 12] [Batch 1200] [G loss: 1.3056]\n",
      "[Epoch 12] [Batch 1300] [G loss: 1.5740]\n",
      "[Epoch 12] [Batch 1400] [G loss: 1.2537]\n",
      "[Epoch 12] [Batch 1500] [G loss: 1.2928]\n",
      "[Epoch 12] [Batch 1600] [G loss: 1.3657]\n",
      "[Epoch 12] [Batch 1700] [G loss: 1.1555]\n",
      "[Epoch 12] [Batch 1800] [G loss: 1.3418]\n",
      "[Epoch 12] [Batch 1900] [G loss: 1.5384]\n",
      "[Epoch 12] [Batch 2000] [G loss: 1.5301]\n",
      "[Epoch 12] [Batch 2100] [G loss: 1.7379]\n",
      "[Epoch 12] [Batch 2200] [G loss: 1.8213]\n",
      "[Epoch 12] [Batch 2300] [G loss: 1.6222]\n",
      "[Epoch 12] [Batch 2400] [G loss: 1.3090]\n",
      "[Epoch 12] [Batch 2500] [G loss: 1.4000]\n",
      "[Epoch 12] [Batch 2600] [G loss: 1.2685]\n",
      "[Epoch 12] [Batch 2700] [G loss: 1.6183]\n",
      "[Epoch 12] [Batch 2800] [G loss: 1.7245]\n",
      "[Epoch 12] [Batch 2900] [G loss: 1.4722]\n",
      "[Epoch 12] [Batch 3000] [G loss: 1.1727]\n",
      "[Epoch 12] [Batch 3100] [G loss: 0.5911]\n",
      "[Epoch 12] [Batch 3200] [G loss: 0.8362]\n",
      "[Epoch 12] [Batch 3300] [G loss: 1.3512]\n",
      "[Epoch 12] [Batch 3400] [G loss: 1.7254]\n",
      "[Epoch 12] [Batch 3500] [G loss: 1.5488]\n",
      "[Epoch 12] [Batch 3600] [G loss: 1.3356]\n",
      "[Epoch 12] [Batch 3700] [G loss: 1.6692]\n",
      "[Epoch 12] [Batch 3800] [G loss: 1.4603]\n",
      "[Epoch 12] [Batch 3900] [G loss: 1.4430]\n",
      "[Epoch 12] [Batch 4000] [G loss: 1.2606]\n",
      "[Epoch 12] [Batch 4100] [G loss: 1.4421]\n",
      "[Epoch 12] [Batch 4200] [G loss: 1.3682]\n",
      "[Epoch 12] [Batch 4300] [G loss: 1.3331]\n",
      "[Epoch 12] [Batch 4400] [G loss: 1.3135]\n",
      "[Epoch 12] [Batch 4500] [G loss: 1.5482]\n",
      "[Epoch 12] [Batch 4600] [G loss: 1.4363]\n",
      "[Epoch 12] [Batch 4700] [G loss: 1.4283]\n",
      "[Epoch 12] [Batch 4800] [G loss: 1.2257]\n",
      "[Epoch 12] [Batch 4900] [G loss: 1.5234]\n",
      "[Epoch 12] [Batch 5000] [G loss: 1.2472]\n",
      "[Epoch 12] [Batch 5100] [G loss: 1.4326]\n",
      "[Epoch 12] [Batch 5200] [G loss: 1.7086]\n",
      "[Epoch 12] [Batch 5300] [G loss: 1.5254]\n",
      "[Epoch 12] [Batch 5400] [G loss: 1.4434]\n",
      "[Epoch 12] [Batch 5500] [G loss: 1.4386]\n",
      "[Epoch 12] [Batch 5600] [G loss: 1.5791]\n",
      "[Epoch 12] [Batch 5700] [G loss: 1.1505]\n",
      "[Epoch 12] [Batch 5800] [G loss: 1.6183]\n",
      "[Epoch 12] [Batch 5900] [G loss: 1.4143]\n",
      "[Epoch 12] [Batch 6000] [G loss: 1.2498]\n",
      "[Epoch 12] [Batch 6100] [G loss: 1.4603]\n",
      "[Epoch 12] [Batch 6200] [G loss: 1.4533]\n",
      "[Epoch 12] [Batch 6300] [G loss: 1.2802]\n",
      "[Epoch 12] [Batch 6400] [G loss: 1.7166]\n",
      "[Epoch 12] [Batch 6500] [G loss: 1.9337]\n",
      "[Epoch 12] [Batch 6600] [G loss: 0.8977]\n",
      "[Epoch 12] [Batch 6700] [G loss: 1.1480]\n",
      "[Epoch 12] [Batch 6800] [G loss: 1.4380]\n",
      "[Epoch 12] [Batch 6900] [G loss: 1.7696]\n",
      "[Epoch 12] [Batch 7000] [G loss: 1.2068]\n",
      "[Epoch 13] [Batch 0] [G loss: 1.3128]\n",
      "[Epoch 13] [Batch 100] [G loss: 1.7867]\n",
      "[Epoch 13] [Batch 200] [G loss: 1.6333]\n",
      "[Epoch 13] [Batch 300] [G loss: 1.1717]\n",
      "[Epoch 13] [Batch 400] [G loss: 1.4632]\n",
      "[Epoch 13] [Batch 500] [G loss: 1.6501]\n",
      "[Epoch 13] [Batch 600] [G loss: 1.0614]\n",
      "[Epoch 13] [Batch 700] [G loss: 1.2005]\n",
      "[Epoch 13] [Batch 800] [G loss: 1.4206]\n",
      "[Epoch 13] [Batch 900] [G loss: 1.6694]\n",
      "[Epoch 13] [Batch 1000] [G loss: 1.2551]\n",
      "[Epoch 13] [Batch 1100] [G loss: 1.7428]\n",
      "[Epoch 13] [Batch 1200] [G loss: 1.3734]\n",
      "[Epoch 13] [Batch 1300] [G loss: 1.3531]\n",
      "[Epoch 13] [Batch 1400] [G loss: 1.3788]\n",
      "[Epoch 13] [Batch 1500] [G loss: 1.1064]\n",
      "[Epoch 13] [Batch 1600] [G loss: 1.9350]\n",
      "[Epoch 13] [Batch 1700] [G loss: 1.6772]\n",
      "[Epoch 13] [Batch 1800] [G loss: 1.1414]\n",
      "[Epoch 13] [Batch 1900] [G loss: 1.5435]\n",
      "[Epoch 13] [Batch 2000] [G loss: 1.3433]\n",
      "[Epoch 13] [Batch 2100] [G loss: 1.6792]\n",
      "[Epoch 13] [Batch 2200] [G loss: 1.3163]\n",
      "[Epoch 13] [Batch 2300] [G loss: 1.8112]\n",
      "[Epoch 13] [Batch 2400] [G loss: 1.8978]\n",
      "[Epoch 13] [Batch 2500] [G loss: 1.3709]\n",
      "[Epoch 13] [Batch 2600] [G loss: 1.2778]\n",
      "[Epoch 13] [Batch 2700] [G loss: 1.3736]\n",
      "[Epoch 13] [Batch 2800] [G loss: 1.5086]\n",
      "[Epoch 13] [Batch 2900] [G loss: 1.4703]\n",
      "[Epoch 13] [Batch 3000] [G loss: 1.2807]\n",
      "[Epoch 13] [Batch 3100] [G loss: 1.2931]\n",
      "[Epoch 13] [Batch 3200] [G loss: 1.6527]\n",
      "[Epoch 13] [Batch 3300] [G loss: 1.3797]\n",
      "[Epoch 13] [Batch 3400] [G loss: 1.4555]\n",
      "[Epoch 13] [Batch 3500] [G loss: 1.8969]\n",
      "[Epoch 13] [Batch 3600] [G loss: 1.2720]\n",
      "[Epoch 13] [Batch 3700] [G loss: 1.6241]\n",
      "[Epoch 13] [Batch 3800] [G loss: 1.4996]\n",
      "[Epoch 13] [Batch 3900] [G loss: 1.8393]\n",
      "[Epoch 13] [Batch 4000] [G loss: 1.4987]\n",
      "[Epoch 13] [Batch 4100] [G loss: 1.4213]\n",
      "[Epoch 13] [Batch 4200] [G loss: 1.3817]\n",
      "[Epoch 13] [Batch 4300] [G loss: 1.3231]\n",
      "[Epoch 13] [Batch 4400] [G loss: 1.5233]\n",
      "[Epoch 13] [Batch 4500] [G loss: 1.7220]\n",
      "[Epoch 13] [Batch 4600] [G loss: 1.2268]\n",
      "[Epoch 13] [Batch 4700] [G loss: 1.4734]\n",
      "[Epoch 13] [Batch 4800] [G loss: 1.2561]\n",
      "[Epoch 13] [Batch 4900] [G loss: 1.4869]\n",
      "[Epoch 13] [Batch 5000] [G loss: 1.7398]\n",
      "[Epoch 13] [Batch 5100] [G loss: 1.4056]\n",
      "[Epoch 13] [Batch 5200] [G loss: 1.2182]\n",
      "[Epoch 13] [Batch 5300] [G loss: 1.1950]\n",
      "[Epoch 13] [Batch 5400] [G loss: 1.4338]\n",
      "[Epoch 13] [Batch 5500] [G loss: 1.1249]\n",
      "[Epoch 13] [Batch 5600] [G loss: 1.5236]\n",
      "[Epoch 13] [Batch 5700] [G loss: 1.5857]\n",
      "[Epoch 13] [Batch 5800] [G loss: 1.1873]\n",
      "[Epoch 13] [Batch 5900] [G loss: 1.3794]\n",
      "[Epoch 13] [Batch 6000] [G loss: 1.2779]\n",
      "[Epoch 13] [Batch 6100] [G loss: 1.6682]\n",
      "[Epoch 13] [Batch 6200] [G loss: 1.6394]\n",
      "[Epoch 13] [Batch 6300] [G loss: 1.5795]\n",
      "[Epoch 13] [Batch 6400] [G loss: 1.2367]\n",
      "[Epoch 13] [Batch 6500] [G loss: 1.4727]\n",
      "[Epoch 13] [Batch 6600] [G loss: 1.1306]\n",
      "[Epoch 13] [Batch 6700] [G loss: 1.2824]\n",
      "[Epoch 13] [Batch 6800] [G loss: 1.7055]\n",
      "[Epoch 13] [Batch 6900] [G loss: 1.4395]\n",
      "[Epoch 13] [Batch 7000] [G loss: 1.6813]\n",
      "[Epoch 14] [Batch 0] [G loss: 1.5924]\n",
      "[Epoch 14] [Batch 100] [G loss: 1.4176]\n",
      "[Epoch 14] [Batch 200] [G loss: 1.1487]\n",
      "[Epoch 14] [Batch 300] [G loss: 1.5641]\n",
      "[Epoch 14] [Batch 400] [G loss: 1.3459]\n",
      "[Epoch 14] [Batch 500] [G loss: 1.6742]\n",
      "[Epoch 14] [Batch 600] [G loss: 1.6398]\n",
      "[Epoch 14] [Batch 700] [G loss: 1.8767]\n",
      "[Epoch 14] [Batch 800] [G loss: 1.4716]\n",
      "[Epoch 14] [Batch 900] [G loss: 1.6043]\n",
      "[Epoch 14] [Batch 1000] [G loss: 1.2221]\n",
      "[Epoch 14] [Batch 1100] [G loss: 1.3616]\n",
      "[Epoch 14] [Batch 1200] [G loss: 1.3226]\n",
      "[Epoch 14] [Batch 1300] [G loss: 1.4141]\n",
      "[Epoch 14] [Batch 1400] [G loss: 1.6789]\n",
      "[Epoch 14] [Batch 1500] [G loss: 1.1617]\n",
      "[Epoch 14] [Batch 1600] [G loss: 1.5140]\n",
      "[Epoch 14] [Batch 1700] [G loss: 1.3457]\n",
      "[Epoch 14] [Batch 1800] [G loss: 1.5039]\n",
      "[Epoch 14] [Batch 1900] [G loss: 1.2706]\n",
      "[Epoch 14] [Batch 2000] [G loss: 1.1696]\n",
      "[Epoch 14] [Batch 2100] [G loss: 1.3208]\n",
      "[Epoch 14] [Batch 2200] [G loss: 1.6294]\n",
      "[Epoch 14] [Batch 2300] [G loss: 1.5134]\n",
      "[Epoch 14] [Batch 2400] [G loss: 1.7821]\n",
      "[Epoch 14] [Batch 2500] [G loss: 1.1140]\n",
      "[Epoch 14] [Batch 2600] [G loss: 1.2358]\n",
      "[Epoch 14] [Batch 2700] [G loss: 1.5317]\n",
      "[Epoch 14] [Batch 2800] [G loss: 1.2297]\n",
      "[Epoch 14] [Batch 2900] [G loss: 1.3505]\n",
      "[Epoch 14] [Batch 3000] [G loss: 1.5484]\n",
      "[Epoch 14] [Batch 3100] [G loss: 1.6313]\n",
      "[Epoch 14] [Batch 3200] [G loss: 1.1855]\n",
      "[Epoch 14] [Batch 3300] [G loss: 1.8624]\n",
      "[Epoch 14] [Batch 3400] [G loss: 1.6046]\n",
      "[Epoch 14] [Batch 3500] [G loss: 1.4713]\n",
      "[Epoch 14] [Batch 3600] [G loss: 1.5340]\n",
      "[Epoch 14] [Batch 3700] [G loss: 1.2632]\n",
      "[Epoch 14] [Batch 3800] [G loss: 1.4229]\n",
      "[Epoch 14] [Batch 3900] [G loss: 1.0559]\n",
      "[Epoch 14] [Batch 4000] [G loss: 1.1762]\n",
      "[Epoch 14] [Batch 4100] [G loss: 1.6039]\n",
      "[Epoch 14] [Batch 4200] [G loss: 1.2485]\n",
      "[Epoch 14] [Batch 4300] [G loss: 1.4821]\n",
      "[Epoch 14] [Batch 4400] [G loss: 1.2890]\n",
      "[Epoch 14] [Batch 4500] [G loss: 1.5412]\n",
      "[Epoch 14] [Batch 4600] [G loss: 1.7857]\n",
      "[Epoch 14] [Batch 4700] [G loss: 1.7223]\n",
      "[Epoch 14] [Batch 4800] [G loss: 1.8048]\n",
      "[Epoch 14] [Batch 4900] [G loss: 1.6181]\n",
      "[Epoch 14] [Batch 5000] [G loss: 1.4334]\n",
      "[Epoch 14] [Batch 5100] [G loss: 1.0767]\n",
      "[Epoch 14] [Batch 5200] [G loss: 0.8986]\n",
      "[Epoch 14] [Batch 5300] [G loss: 1.3568]\n",
      "[Epoch 14] [Batch 5400] [G loss: 1.4374]\n",
      "[Epoch 14] [Batch 5500] [G loss: 1.2632]\n",
      "[Epoch 14] [Batch 5600] [G loss: 1.3260]\n",
      "[Epoch 14] [Batch 5700] [G loss: 1.6005]\n",
      "[Epoch 14] [Batch 5800] [G loss: 1.4069]\n",
      "[Epoch 14] [Batch 5900] [G loss: 1.2977]\n",
      "[Epoch 14] [Batch 6000] [G loss: 1.8948]\n",
      "[Epoch 14] [Batch 6100] [G loss: 1.2625]\n",
      "[Epoch 14] [Batch 6200] [G loss: 1.3949]\n",
      "[Epoch 14] [Batch 6300] [G loss: 1.2416]\n",
      "[Epoch 14] [Batch 6400] [G loss: 1.4536]\n",
      "[Epoch 14] [Batch 6500] [G loss: 1.3087]\n",
      "[Epoch 14] [Batch 6600] [G loss: 1.1389]\n",
      "[Epoch 14] [Batch 6700] [G loss: 1.1228]\n",
      "[Epoch 14] [Batch 6800] [G loss: 1.4198]\n",
      "[Epoch 14] [Batch 6900] [G loss: 1.4382]\n",
      "[Epoch 14] [Batch 7000] [G loss: 1.5593]\n",
      "[Epoch 15] [Batch 0] [G loss: 1.8938]\n",
      "[Epoch 15] [Batch 100] [G loss: 1.4783]\n",
      "[Epoch 15] [Batch 200] [G loss: 1.3558]\n",
      "[Epoch 15] [Batch 300] [G loss: 1.1749]\n",
      "[Epoch 15] [Batch 400] [G loss: 1.7293]\n",
      "[Epoch 15] [Batch 500] [G loss: 1.3120]\n",
      "[Epoch 15] [Batch 600] [G loss: 1.5540]\n",
      "[Epoch 15] [Batch 700] [G loss: 1.5216]\n",
      "[Epoch 15] [Batch 800] [G loss: 1.4874]\n",
      "[Epoch 15] [Batch 900] [G loss: 1.3086]\n",
      "[Epoch 15] [Batch 1000] [G loss: 1.8106]\n",
      "[Epoch 15] [Batch 1100] [G loss: 1.5738]\n",
      "[Epoch 15] [Batch 1200] [G loss: 1.4373]\n",
      "[Epoch 15] [Batch 1300] [G loss: 1.5929]\n",
      "[Epoch 15] [Batch 1400] [G loss: 1.4299]\n",
      "[Epoch 15] [Batch 1500] [G loss: 1.7040]\n",
      "[Epoch 15] [Batch 1600] [G loss: 1.6845]\n",
      "[Epoch 15] [Batch 1700] [G loss: 1.5403]\n",
      "[Epoch 15] [Batch 1800] [G loss: 1.3821]\n",
      "[Epoch 15] [Batch 1900] [G loss: 1.5189]\n",
      "[Epoch 15] [Batch 2000] [G loss: 1.3890]\n",
      "[Epoch 15] [Batch 2100] [G loss: 1.2062]\n",
      "[Epoch 15] [Batch 2200] [G loss: 1.4659]\n",
      "[Epoch 15] [Batch 2300] [G loss: 1.2928]\n",
      "[Epoch 15] [Batch 2400] [G loss: 1.2384]\n",
      "[Epoch 15] [Batch 2500] [G loss: 1.5581]\n",
      "[Epoch 15] [Batch 2600] [G loss: 1.6408]\n",
      "[Epoch 15] [Batch 2700] [G loss: 1.5487]\n",
      "[Epoch 15] [Batch 2800] [G loss: 1.6680]\n",
      "[Epoch 15] [Batch 2900] [G loss: 1.3727]\n",
      "[Epoch 15] [Batch 3000] [G loss: 1.4681]\n",
      "[Epoch 15] [Batch 3100] [G loss: 1.3613]\n",
      "[Epoch 15] [Batch 3200] [G loss: 1.5120]\n",
      "[Epoch 15] [Batch 3300] [G loss: 1.0385]\n",
      "[Epoch 15] [Batch 3400] [G loss: 1.0302]\n",
      "[Epoch 15] [Batch 3500] [G loss: 1.4022]\n",
      "[Epoch 15] [Batch 3600] [G loss: 1.4280]\n",
      "[Epoch 15] [Batch 3700] [G loss: 1.3609]\n",
      "[Epoch 15] [Batch 3800] [G loss: 1.3380]\n",
      "[Epoch 15] [Batch 3900] [G loss: 2.2011]\n",
      "[Epoch 15] [Batch 4000] [G loss: 1.3611]\n",
      "[Epoch 15] [Batch 4100] [G loss: 1.2687]\n",
      "[Epoch 15] [Batch 4200] [G loss: 1.3142]\n",
      "[Epoch 15] [Batch 4300] [G loss: 1.5236]\n",
      "[Epoch 15] [Batch 4400] [G loss: 1.3186]\n",
      "[Epoch 15] [Batch 4500] [G loss: 1.3909]\n",
      "[Epoch 15] [Batch 4600] [G loss: 1.4749]\n",
      "[Epoch 15] [Batch 4700] [G loss: 1.0585]\n",
      "[Epoch 15] [Batch 4800] [G loss: 1.5407]\n",
      "[Epoch 15] [Batch 4900] [G loss: 1.1451]\n",
      "[Epoch 15] [Batch 5000] [G loss: 1.3851]\n",
      "[Epoch 15] [Batch 5100] [G loss: 1.2062]\n",
      "[Epoch 15] [Batch 5200] [G loss: 1.4014]\n",
      "[Epoch 15] [Batch 5300] [G loss: 1.6035]\n",
      "[Epoch 15] [Batch 5400] [G loss: 1.5991]\n",
      "[Epoch 15] [Batch 5500] [G loss: 1.1016]\n",
      "[Epoch 15] [Batch 5600] [G loss: 1.5870]\n",
      "[Epoch 15] [Batch 5700] [G loss: 1.4685]\n",
      "[Epoch 15] [Batch 5800] [G loss: 1.5374]\n",
      "[Epoch 15] [Batch 5900] [G loss: 0.9956]\n",
      "[Epoch 15] [Batch 6000] [G loss: 1.3723]\n",
      "[Epoch 15] [Batch 6100] [G loss: 1.3197]\n",
      "[Epoch 15] [Batch 6200] [G loss: 1.3967]\n",
      "[Epoch 15] [Batch 6300] [G loss: 0.9991]\n",
      "[Epoch 15] [Batch 6400] [G loss: 1.2246]\n",
      "[Epoch 15] [Batch 6500] [G loss: 1.4445]\n",
      "[Epoch 15] [Batch 6600] [G loss: 1.2443]\n",
      "[Epoch 15] [Batch 6700] [G loss: 1.8041]\n",
      "[Epoch 15] [Batch 6800] [G loss: 1.3629]\n",
      "[Epoch 15] [Batch 6900] [G loss: 1.1998]\n",
      "[Epoch 15] [Batch 7000] [G loss: 1.3090]\n",
      "üíæ UNet Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints_unet/epoch_15.pth\n",
      "[Epoch 16] [Batch 0] [G loss: 1.7830]\n",
      "[Epoch 16] [Batch 100] [G loss: 1.6740]\n",
      "[Epoch 16] [Batch 200] [G loss: 1.3984]\n",
      "[Epoch 16] [Batch 300] [G loss: 1.2563]\n",
      "[Epoch 16] [Batch 400] [G loss: 1.0827]\n",
      "[Epoch 16] [Batch 500] [G loss: 1.8303]\n",
      "[Epoch 16] [Batch 600] [G loss: 1.2712]\n",
      "[Epoch 16] [Batch 700] [G loss: 1.5582]\n",
      "[Epoch 16] [Batch 800] [G loss: 1.5122]\n",
      "[Epoch 16] [Batch 900] [G loss: 1.6146]\n",
      "[Epoch 16] [Batch 1000] [G loss: 1.3565]\n",
      "[Epoch 16] [Batch 1100] [G loss: 1.0248]\n",
      "[Epoch 16] [Batch 1200] [G loss: 1.1373]\n",
      "[Epoch 16] [Batch 1300] [G loss: 1.5389]\n",
      "[Epoch 16] [Batch 1400] [G loss: 1.4533]\n",
      "[Epoch 16] [Batch 1500] [G loss: 1.2471]\n",
      "[Epoch 16] [Batch 1600] [G loss: 1.5131]\n",
      "[Epoch 16] [Batch 1700] [G loss: 1.5037]\n",
      "[Epoch 16] [Batch 1800] [G loss: 1.4065]\n",
      "[Epoch 16] [Batch 1900] [G loss: 1.2992]\n",
      "[Epoch 16] [Batch 2000] [G loss: 1.3284]\n",
      "[Epoch 16] [Batch 2100] [G loss: 1.1131]\n",
      "[Epoch 16] [Batch 2200] [G loss: 1.2726]\n",
      "[Epoch 16] [Batch 2300] [G loss: 1.7081]\n",
      "[Epoch 16] [Batch 2400] [G loss: 1.2576]\n",
      "[Epoch 16] [Batch 2500] [G loss: 1.7331]\n",
      "[Epoch 16] [Batch 2600] [G loss: 1.6202]\n",
      "[Epoch 16] [Batch 2700] [G loss: 1.5450]\n",
      "[Epoch 16] [Batch 2800] [G loss: 1.3297]\n",
      "[Epoch 16] [Batch 2900] [G loss: 1.3716]\n",
      "[Epoch 16] [Batch 3000] [G loss: 1.4811]\n",
      "[Epoch 16] [Batch 3100] [G loss: 1.3018]\n",
      "[Epoch 16] [Batch 3200] [G loss: 1.4284]\n",
      "[Epoch 16] [Batch 3300] [G loss: 1.4380]\n",
      "[Epoch 16] [Batch 3400] [G loss: 1.4204]\n",
      "[Epoch 16] [Batch 3500] [G loss: 1.5439]\n",
      "[Epoch 16] [Batch 3600] [G loss: 1.1302]\n",
      "[Epoch 16] [Batch 3700] [G loss: 1.2817]\n",
      "[Epoch 16] [Batch 3800] [G loss: 1.4708]\n",
      "[Epoch 16] [Batch 3900] [G loss: 1.4378]\n",
      "[Epoch 16] [Batch 4000] [G loss: 1.3776]\n",
      "[Epoch 16] [Batch 4100] [G loss: 1.6112]\n",
      "[Epoch 16] [Batch 4200] [G loss: 1.4342]\n",
      "[Epoch 16] [Batch 4300] [G loss: 1.3617]\n",
      "[Epoch 16] [Batch 4400] [G loss: 1.4232]\n",
      "[Epoch 16] [Batch 4500] [G loss: 1.3556]\n",
      "[Epoch 16] [Batch 4600] [G loss: 1.5655]\n",
      "[Epoch 16] [Batch 4700] [G loss: 1.3800]\n",
      "[Epoch 16] [Batch 4800] [G loss: 1.5212]\n",
      "[Epoch 16] [Batch 4900] [G loss: 1.4179]\n",
      "[Epoch 16] [Batch 5000] [G loss: 1.2542]\n",
      "[Epoch 16] [Batch 5100] [G loss: 1.3330]\n",
      "[Epoch 16] [Batch 5200] [G loss: 1.0998]\n",
      "[Epoch 16] [Batch 5300] [G loss: 0.9913]\n",
      "[Epoch 16] [Batch 5400] [G loss: 1.3740]\n",
      "[Epoch 16] [Batch 5500] [G loss: 1.1739]\n",
      "[Epoch 16] [Batch 5600] [G loss: 1.7175]\n",
      "[Epoch 16] [Batch 5700] [G loss: 1.6423]\n",
      "[Epoch 16] [Batch 5800] [G loss: 1.4888]\n",
      "[Epoch 16] [Batch 5900] [G loss: 1.6295]\n",
      "[Epoch 16] [Batch 6000] [G loss: 1.5711]\n",
      "[Epoch 16] [Batch 6100] [G loss: 1.1106]\n",
      "[Epoch 16] [Batch 6200] [G loss: 1.2708]\n",
      "[Epoch 16] [Batch 6300] [G loss: 1.4220]\n",
      "[Epoch 16] [Batch 6400] [G loss: 1.3739]\n",
      "[Epoch 16] [Batch 6500] [G loss: 1.4243]\n",
      "[Epoch 16] [Batch 6600] [G loss: 1.2057]\n",
      "[Epoch 16] [Batch 6700] [G loss: 1.1975]\n",
      "[Epoch 16] [Batch 6800] [G loss: 1.1950]\n",
      "[Epoch 16] [Batch 6900] [G loss: 1.6368]\n",
      "[Epoch 16] [Batch 7000] [G loss: 1.2485]\n",
      "[Epoch 17] [Batch 0] [G loss: 0.9868]\n",
      "[Epoch 17] [Batch 100] [G loss: 1.6564]\n",
      "[Epoch 17] [Batch 200] [G loss: 1.6983]\n",
      "[Epoch 17] [Batch 300] [G loss: 1.6740]\n",
      "[Epoch 17] [Batch 400] [G loss: 1.0888]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(ConfigUNet.START_EPOCH, ConfigUNet.EPOCHS):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        real_A = batch[\"photo\"].to(ConfigUNet.DEVICE)\n",
    "        real_B = batch[\"monet\"].to(ConfigUNet.DEVICE)\n",
    "\n",
    "        valid = torch.ones((real_A.size(0), 1, 16, 16), requires_grad=False).to(ConfigUNet.DEVICE)\n",
    "        fake = torch.zeros((real_A.size(0), 1, 16, 16), requires_grad=False).to(ConfigUNet.DEVICE)\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        with autocast('cuda'):\n",
    "            # Identity\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2 * ConfigUNet.LAMBDA_ID\n",
    "\n",
    "            # GAN\n",
    "            fake_B = G_AB(real_A)\n",
    "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_BA(real_B)\n",
    "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "            # Cycle\n",
    "            recov_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "            recov_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2 * ConfigUNet.LAMBDA_CYCLE\n",
    "\n",
    "            loss_G = loss_GAN + loss_cycle + loss_identity\n",
    "\n",
    "        scaler.scale(loss_G).backward()\n",
    "        scaler.step(optimizer_G)\n",
    "        scaler.update()\n",
    "\n",
    "        optimizer_D_A.zero_grad()\n",
    "        with autocast('cuda'):\n",
    "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "        scaler.scale(loss_D_A).backward()\n",
    "        scaler.step(optimizer_D_A)\n",
    "        scaler.update()\n",
    "\n",
    "        optimizer_D_B.zero_grad()\n",
    "        with autocast('cuda'):\n",
    "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "        scaler.scale(loss_D_B).backward()\n",
    "        scaler.step(optimizer_D_B)\n",
    "        scaler.update()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            wandb.log({\"Loss/G\": loss_G.item(), \"Loss/D\": loss_D_A.item()+loss_D_B.item(), \"Epoch\": epoch})\n",
    "            print(f\"[Epoch {epoch}] [Batch {i}] [G loss: {loss_G.item():.4f}]\")\n",
    "\n",
    "\n",
    "    img_real_A = real_A[0].detach().cpu() * 0.5 + 0.5\n",
    "    img_fake_B = fake_B[0].detach().cpu() * 0.5 + 0.5\n",
    "    img_real_B = real_B[0].detach().cpu() * 0.5 + 0.5\n",
    "    img_fake_A = fake_A[0].detach().cpu() * 0.5 + 0.5\n",
    "\n",
    "    wandb.log({\n",
    "        \"Visual/Real Photo\": wandb.Image(img_real_A, caption=f\"Real Photo (Epoch {epoch})\"),\n",
    "        \"Visual/Generated Monet\": wandb.Image(img_fake_B, caption=f\"Generated Monet (Epoch {epoch})\"),\n",
    "        \"Visual/Real Monet\": wandb.Image(img_real_B, caption=f\"Real Monet (Epoch {epoch})\"),\n",
    "        \"Visual/Generated Photo\": wandb.Image(img_fake_A, caption=f\"Reconstructed Photo (Epoch {epoch})\")\n",
    "    })\n",
    "\n",
    "    if epoch % ConfigUNet.SAVE_EPOCH_FREQ == 0:\n",
    "        save_path = f\"{ConfigUNet.CHECKPOINT_DIR}/epoch_{epoch}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'G_AB': G_AB.state_dict(),\n",
    "            'G_BA': G_BA.state_dict(),\n",
    "            'D_A': D_A.state_dict(),\n",
    "            'D_B': D_B.state_dict(),\n",
    "            'optimizer_G': optimizer_G.state_dict(),\n",
    "        }, save_path)\n",
    "        print(f\"üíæ UNet Checkpoint saved: {save_path}\")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN1y2DQyvIoLe+dp1ntNAEm",
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
