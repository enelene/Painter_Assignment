{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYcFgo7IeQTD"
      },
      "source": [
        "#RES-NET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjOQ-WYSbo3W",
        "outputId": "d10446ed-2b1a-4b41-fab9-85bcb8385a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/Painter_Assignment\"\n",
        "SRC_PATH = os.path.join(PROJECT_PATH, \"src\")\n",
        "CHECKPOINT_PATH = os.path.join(PROJECT_PATH, \"checkpoints\")\n",
        "KAGGLE_JSON_PATH = os.path.join(PROJECT_PATH, \"kaggle.json\")\n",
        "\n",
        "os.makedirs(SRC_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiLjPihgEngk",
        "outputId": "89da5f3a-a2ce-492c-dfd7-090cc4bb9cf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset from Kaggle...\n",
            "Downloading gan-getting-started.zip to /content\n",
            " 83% 305M/367M [00:06<00:01, 41.5MB/s]\n",
            "100% 367M/367M [00:06<00:00, 58.0MB/s]\n",
            "Dataset downloaded and extracted successfully!\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(KAGGLE_JSON_PATH):\n",
        "    shutil.copy(KAGGLE_JSON_PATH, \"/content/kaggle.json\")\n",
        "    os.chmod(\"/content/kaggle.json\", 0o600)\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n",
        "else:\n",
        "    print(f\"ERROR: kaggle.json not found in {PROJECT_PATH}\")\n",
        "\n",
        "if not os.path.exists(\"/content/dataset\"):\n",
        "    print(\"Downloading dataset from Kaggle...\")\n",
        "    !kaggle competitions download -c gan-getting-started\n",
        "    !unzip -q gan-getting-started.zip -d /content/dataset\n",
        "    print(\"Dataset downloaded and extracted successfully!\")\n",
        "else:\n",
        "    print(\"Dataset already exists on local disk.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJHjfeWGE9dO",
        "outputId": "32516fbc-75d3-44ea-813d-616253bb9cab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melene-gabeskiria2004\u001b[0m (\u001b[33melene-gabeskiria2004-free-univiersity-of-tbilisi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -q\n",
        "import wandb\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    wandb.login(key=userdata.get('wandb_api_key'))\n",
        "except:\n",
        "    wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WBpXpGnL11v"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qcjpc56UL4Bu"
      },
      "outputs": [],
      "source": [
        "sys.path.append(\"/content/drive/MyDrive/Painter_Assignment/src\")\n",
        "\n",
        "from config import Config\n",
        "from dataset import UnpairedDataset\n",
        "from models import GeneratorResNet, Discriminator\n",
        "from utils import weights_init_normal, ReplayBuffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "JwE9J8p9L7YC",
        "outputId": "6c4036bd-37ac-40e8-f56e-152098a9ce5e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251220_081214-m4wfymx2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter/runs/m4wfymx2' target=\"_blank\">stellar-dawn-10</a></strong> to <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter/runs/m4wfymx2' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter/runs/m4wfymx2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter/runs/m4wfymx2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fee256674a0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(\n",
        "    project=Config.PROJECT_NAME,\n",
        "    config={k:v for k,v in Config.__dict__.items() if not k.startswith('__')},\n",
        "    resume=\"allow\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRlzjOIXMEb7"
      },
      "outputs": [],
      "source": [
        "# Initialize the 4 Networks\n",
        "G_AB = GeneratorResNet().to(Config.DEVICE) # Photo -> Monet\n",
        "G_BA = GeneratorResNet().to(Config.DEVICE) # Monet -> Photo\n",
        "D_A = Discriminator().to(Config.DEVICE)    # Discriminator for Photos\n",
        "D_B = Discriminator().to(Config.DEVICE)    # Discriminator for Monets\n",
        "\n",
        "# Initialize Optimizers\n",
        "optimizer_G = torch.optim.Adam(\n",
        "    list(G_AB.parameters()) + list(G_BA.parameters()),\n",
        "    lr=Config.LR_G, betas=(Config.B1, Config.B2)\n",
        ")\n",
        "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=Config.LR_D, betas=(Config.B1, Config.B2))\n",
        "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=Config.LR_D, betas=(Config.B1, Config.B2))\n",
        "\n",
        "# Define Loss Functions\n",
        "criterion_GAN = nn.MSELoss()\n",
        "criterion_cycle = nn.L1Loss()\n",
        "criterion_identity = nn.L1Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v8dXDMZ77ea",
        "outputId": "4583ee1e-21f7-4dbb-908a-d93dc6c93971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(Config.LOAD_MODEL)\n",
        "print(Config.SAVE_EPOCH_FREQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "1f03a40a",
        "outputId": "7203b0d4-df75-4e0a-b039-71b97f3eeda6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config module was not previously imported.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'config'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-112920769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Re-import Config to ensure we're using the reloaded module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import sys\n",
        "\n",
        "# Check if 'config' is in sys.modules (meaning it has been imported)\n",
        "if 'config' in sys.modules:\n",
        "    # Reload the config module to reflect any changes\n",
        "    importlib.reload(sys.modules['config'])\n",
        "    importlib.reload(sys.modules['models'])\n",
        "\n",
        "    print(\"Config module reloaded successfully.\")\n",
        "else:\n",
        "    print(\"Config module was not previously imported.\")\n",
        "\n",
        "# Re-import Config to ensure we're using the reloaded module\n",
        "from config import Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-01WjtWMYvf",
        "outputId": "9ca6d3f8-af0e-4831-8543-d385eb73f8a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Resuming training from: /content/drive/MyDrive/Painter_Assignment/checkpoints/epoch_5.pth\n",
            "üì∏ Data Loaded. Monet: 300, Photo: 7038\n"
          ]
        }
      ],
      "source": [
        "start_epoch = 0\n",
        "\n",
        "# Check if we want to load a saved model (Config.LOAD_MODEL is in config.py)\n",
        "if Config.LOAD_MODEL:\n",
        "    load_path = f\"{Config.CHECKPOINT_DIR}/epoch_{Config.START_EPOCH}.pth\"\n",
        "    print(f\"üîÑ Resuming training from: {load_path}\")\n",
        "\n",
        "    # Load Weights\n",
        "    checkpoint = torch.load(load_path, map_location=Config.DEVICE)\n",
        "    G_AB.load_state_dict(checkpoint['G_AB'])\n",
        "    G_BA.load_state_dict(checkpoint['G_BA'])\n",
        "    D_A.load_state_dict(checkpoint['D_A'])\n",
        "    D_B.load_state_dict(checkpoint['D_B'])\n",
        "    optimizer_G.load_state_dict(checkpoint['optimizer_G'])\n",
        "    optimizer_D_A.load_state_dict(checkpoint['optimizer_D_A'])\n",
        "    optimizer_D_B.load_state_dict(checkpoint['optimizer_D_B'])\n",
        "\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "else:\n",
        "    print(\"‚ú® Starting Fresh Training...\")\n",
        "    # Initialize weights using the \"Normal Distribution\" trick\n",
        "    G_AB.apply(weights_init_normal)\n",
        "    G_BA.apply(weights_init_normal)\n",
        "    D_A.apply(weights_init_normal)\n",
        "    D_B.apply(weights_init_normal)\n",
        "\n",
        "# --- 5. Data Loaders ---\n",
        "# Create the dataset interface\n",
        "dataset = UnpairedDataset(Config.TRAIN_MONET, Config.TRAIN_PHOTO)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=Config.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=Config.NUM_WORKERS\n",
        ")\n",
        "\n",
        "# Image buffers to stabilize the discriminator\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()\n",
        "\n",
        "print(f\"üì∏ Data Loaded. Monet: {len(dataset.monet_files)}, Photo: {len(dataset.photo_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmaoFy-cFsNX",
        "outputId": "d7bc61d3-94ae-4e10-881d-e43f809a8802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Training Started with Mixed Precision (AMP), Soft Labels, and Instance Noise...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2721993729.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-2721993729.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/tmp/ipython-input-2721993729.py:56: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/tmp/ipython-input-2721993729.py:82: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 6] [Batch 0] [G loss: 1.6232] [D loss: 0.3251]\n",
            "[Epoch 6] [Batch 100] [G loss: 2.4958] [D loss: 0.0797]\n",
            "[Epoch 6] [Batch 200] [G loss: 2.1762] [D loss: 0.1746]\n",
            "[Epoch 6] [Batch 300] [G loss: 1.8289] [D loss: 0.2608]\n",
            "[Epoch 6] [Batch 400] [G loss: 2.2896] [D loss: 0.4124]\n",
            "[Epoch 6] [Batch 500] [G loss: 2.5710] [D loss: 0.1300]\n",
            "[Epoch 6] [Batch 600] [G loss: 2.8669] [D loss: 0.0977]\n",
            "[Epoch 6] [Batch 700] [G loss: 2.7652] [D loss: 0.1246]\n",
            "[Epoch 6] [Batch 800] [G loss: 2.3175] [D loss: 0.1347]\n",
            "[Epoch 6] [Batch 900] [G loss: 2.5670] [D loss: 0.0603]\n",
            "[Epoch 6] [Batch 1000] [G loss: 2.1580] [D loss: 0.1303]\n",
            "[Epoch 6] [Batch 1100] [G loss: 1.4449] [D loss: 0.0772]\n",
            "[Epoch 6] [Batch 1200] [G loss: 2.6945] [D loss: 0.1572]\n",
            "[Epoch 6] [Batch 1300] [G loss: 2.6889] [D loss: 0.2389]\n",
            "[Epoch 6] [Batch 1400] [G loss: 1.4748] [D loss: 0.0861]\n",
            "[Epoch 6] [Batch 1500] [G loss: 2.0488] [D loss: 0.2625]\n",
            "[Epoch 6] [Batch 1600] [G loss: 3.0297] [D loss: 0.1093]\n",
            "[Epoch 6] [Batch 1700] [G loss: 2.2209] [D loss: 0.0635]\n",
            "[Epoch 6] [Batch 1800] [G loss: 1.8195] [D loss: 0.2465]\n",
            "[Epoch 6] [Batch 1900] [G loss: 1.5797] [D loss: 0.0908]\n",
            "[Epoch 6] [Batch 2000] [G loss: 1.9176] [D loss: 0.3299]\n",
            "[Epoch 6] [Batch 2100] [G loss: 1.7599] [D loss: 0.1379]\n",
            "[Epoch 6] [Batch 2200] [G loss: 1.8612] [D loss: 0.1244]\n",
            "[Epoch 6] [Batch 2300] [G loss: 2.7579] [D loss: 0.0687]\n",
            "[Epoch 6] [Batch 2400] [G loss: 1.8963] [D loss: 0.3212]\n",
            "[Epoch 6] [Batch 2500] [G loss: 2.1063] [D loss: 0.2317]\n",
            "[Epoch 6] [Batch 2600] [G loss: 2.2953] [D loss: 0.1280]\n",
            "[Epoch 6] [Batch 2700] [G loss: 1.5492] [D loss: 0.2078]\n",
            "[Epoch 6] [Batch 2800] [G loss: 2.0144] [D loss: 0.0931]\n",
            "[Epoch 6] [Batch 2900] [G loss: 1.8206] [D loss: 0.2423]\n",
            "[Epoch 6] [Batch 3000] [G loss: 1.8474] [D loss: 0.1143]\n",
            "[Epoch 6] [Batch 3100] [G loss: 1.9591] [D loss: 0.2228]\n",
            "[Epoch 6] [Batch 3200] [G loss: 3.8791] [D loss: 0.0526]\n",
            "[Epoch 6] [Batch 3300] [G loss: 2.0441] [D loss: 0.0874]\n",
            "[Epoch 6] [Batch 3400] [G loss: 1.5527] [D loss: 0.1637]\n",
            "[Epoch 6] [Batch 3500] [G loss: 1.4540] [D loss: 0.2284]\n",
            "[Epoch 6] [Batch 3600] [G loss: 1.8086] [D loss: 0.1153]\n",
            "[Epoch 6] [Batch 3700] [G loss: 2.2309] [D loss: 0.1287]\n",
            "[Epoch 6] [Batch 3800] [G loss: 2.4227] [D loss: 0.1433]\n",
            "[Epoch 6] [Batch 3900] [G loss: 2.1177] [D loss: 0.1912]\n",
            "[Epoch 6] [Batch 4000] [G loss: 1.5306] [D loss: 0.1261]\n",
            "[Epoch 6] [Batch 4100] [G loss: 2.3443] [D loss: 0.1725]\n",
            "[Epoch 6] [Batch 4200] [G loss: 2.5033] [D loss: 0.1384]\n",
            "[Epoch 6] [Batch 4300] [G loss: 2.1482] [D loss: 0.1241]\n",
            "[Epoch 6] [Batch 4400] [G loss: 2.0904] [D loss: 0.1729]\n",
            "[Epoch 6] [Batch 4500] [G loss: 1.9675] [D loss: 0.2306]\n",
            "[Epoch 6] [Batch 4600] [G loss: 3.1125] [D loss: 0.1465]\n",
            "[Epoch 6] [Batch 4700] [G loss: 2.2386] [D loss: 0.1109]\n",
            "[Epoch 6] [Batch 4800] [G loss: 1.7780] [D loss: 0.0936]\n",
            "[Epoch 6] [Batch 4900] [G loss: 1.9894] [D loss: 0.2299]\n",
            "[Epoch 6] [Batch 5000] [G loss: 2.0153] [D loss: 0.0767]\n",
            "[Epoch 6] [Batch 5100] [G loss: 3.5731] [D loss: 0.0401]\n",
            "[Epoch 6] [Batch 5200] [G loss: 1.8588] [D loss: 0.1143]\n",
            "[Epoch 6] [Batch 5300] [G loss: 2.5557] [D loss: 0.1150]\n",
            "[Epoch 6] [Batch 5400] [G loss: 2.4563] [D loss: 0.1229]\n",
            "[Epoch 6] [Batch 5500] [G loss: 3.2412] [D loss: 0.1488]\n",
            "[Epoch 6] [Batch 5600] [G loss: 1.9788] [D loss: 0.1162]\n",
            "[Epoch 6] [Batch 5700] [G loss: 2.1695] [D loss: 0.0399]\n",
            "[Epoch 6] [Batch 5800] [G loss: 2.2674] [D loss: 0.1656]\n",
            "[Epoch 6] [Batch 5900] [G loss: 2.1702] [D loss: 0.1588]\n",
            "[Epoch 6] [Batch 6000] [G loss: 2.4192] [D loss: 0.1943]\n",
            "[Epoch 6] [Batch 6100] [G loss: 1.8876] [D loss: 0.1713]\n",
            "[Epoch 6] [Batch 6200] [G loss: 2.0353] [D loss: 0.0877]\n",
            "[Epoch 6] [Batch 6300] [G loss: 2.1291] [D loss: 0.3553]\n",
            "[Epoch 6] [Batch 6400] [G loss: 2.5046] [D loss: 0.1996]\n",
            "[Epoch 6] [Batch 6500] [G loss: 1.9072] [D loss: 0.1289]\n",
            "[Epoch 6] [Batch 6600] [G loss: 2.6926] [D loss: 0.0564]\n",
            "[Epoch 6] [Batch 6700] [G loss: 1.9355] [D loss: 0.2014]\n",
            "[Epoch 6] [Batch 6800] [G loss: 2.2000] [D loss: 0.1666]\n",
            "[Epoch 6] [Batch 6900] [G loss: 1.8604] [D loss: 0.0545]\n",
            "[Epoch 6] [Batch 7000] [G loss: 2.8619] [D loss: 0.0445]\n",
            "Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints/epoch_6.pth\n",
            "[Epoch 7] [Batch 0] [G loss: 1.4919] [D loss: 0.1839]\n",
            "[Epoch 7] [Batch 100] [G loss: 2.1296] [D loss: 0.1716]\n",
            "[Epoch 7] [Batch 200] [G loss: 2.3412] [D loss: 0.2072]\n",
            "[Epoch 7] [Batch 300] [G loss: 1.7639] [D loss: 0.2107]\n",
            "[Epoch 7] [Batch 400] [G loss: 1.4620] [D loss: 0.0781]\n",
            "[Epoch 7] [Batch 500] [G loss: 2.7845] [D loss: 0.0918]\n",
            "[Epoch 7] [Batch 600] [G loss: 1.7437] [D loss: 0.2152]\n",
            "[Epoch 7] [Batch 700] [G loss: 1.9677] [D loss: 0.1864]\n",
            "[Epoch 7] [Batch 800] [G loss: 2.2055] [D loss: 0.1848]\n",
            "[Epoch 7] [Batch 900] [G loss: 2.0581] [D loss: 0.2782]\n",
            "[Epoch 7] [Batch 1000] [G loss: 1.9869] [D loss: 0.3632]\n",
            "[Epoch 7] [Batch 1100] [G loss: 2.1876] [D loss: 0.0505]\n",
            "[Epoch 7] [Batch 1200] [G loss: 1.6144] [D loss: 0.2599]\n",
            "[Epoch 7] [Batch 1300] [G loss: 2.2827] [D loss: 0.1806]\n",
            "[Epoch 7] [Batch 1400] [G loss: 2.6048] [D loss: 0.0785]\n",
            "[Epoch 7] [Batch 1500] [G loss: 1.6254] [D loss: 0.3274]\n",
            "[Epoch 7] [Batch 1600] [G loss: 2.1829] [D loss: 0.0671]\n",
            "[Epoch 7] [Batch 1700] [G loss: 1.5966] [D loss: 0.1745]\n",
            "[Epoch 7] [Batch 1800] [G loss: 2.1369] [D loss: 0.1988]\n",
            "[Epoch 7] [Batch 1900] [G loss: 2.1713] [D loss: 0.1928]\n",
            "[Epoch 7] [Batch 2000] [G loss: 1.9923] [D loss: 0.2682]\n",
            "[Epoch 7] [Batch 2100] [G loss: 2.0541] [D loss: 0.1161]\n",
            "[Epoch 7] [Batch 2200] [G loss: 1.9565] [D loss: 0.0565]\n",
            "[Epoch 7] [Batch 2300] [G loss: 1.9064] [D loss: 0.0981]\n",
            "[Epoch 7] [Batch 2400] [G loss: 1.5661] [D loss: 0.3218]\n",
            "[Epoch 7] [Batch 2500] [G loss: 1.5388] [D loss: 0.1287]\n",
            "[Epoch 7] [Batch 2600] [G loss: 1.9396] [D loss: 0.0572]\n",
            "[Epoch 7] [Batch 2700] [G loss: 1.5624] [D loss: 0.1553]\n",
            "[Epoch 7] [Batch 2800] [G loss: 2.1149] [D loss: 0.3341]\n",
            "[Epoch 7] [Batch 2900] [G loss: 1.7183] [D loss: 0.2083]\n",
            "[Epoch 7] [Batch 3000] [G loss: 1.8323] [D loss: 0.1306]\n",
            "[Epoch 7] [Batch 3100] [G loss: 2.8000] [D loss: 0.1246]\n",
            "[Epoch 7] [Batch 3200] [G loss: 2.1401] [D loss: 0.1158]\n",
            "[Epoch 7] [Batch 3300] [G loss: 2.2880] [D loss: 0.0776]\n",
            "[Epoch 7] [Batch 3400] [G loss: 2.3555] [D loss: 0.1359]\n",
            "[Epoch 7] [Batch 3500] [G loss: 2.0264] [D loss: 0.2317]\n",
            "[Epoch 7] [Batch 3600] [G loss: 1.8031] [D loss: 0.0525]\n",
            "[Epoch 7] [Batch 3700] [G loss: 1.9536] [D loss: 0.0432]\n",
            "[Epoch 7] [Batch 3800] [G loss: 1.8725] [D loss: 0.1577]\n",
            "[Epoch 7] [Batch 3900] [G loss: 1.6882] [D loss: 0.0787]\n",
            "[Epoch 7] [Batch 4000] [G loss: 2.0015] [D loss: 0.1375]\n",
            "[Epoch 7] [Batch 4100] [G loss: 1.7811] [D loss: 0.1374]\n",
            "[Epoch 7] [Batch 4200] [G loss: 1.8620] [D loss: 0.0817]\n",
            "[Epoch 7] [Batch 4300] [G loss: 2.3941] [D loss: 0.1229]\n",
            "[Epoch 7] [Batch 4400] [G loss: 2.0371] [D loss: 0.1323]\n",
            "[Epoch 7] [Batch 4500] [G loss: 1.9256] [D loss: 0.1123]\n",
            "[Epoch 7] [Batch 4600] [G loss: 2.6384] [D loss: 0.2383]\n",
            "[Epoch 7] [Batch 4700] [G loss: 2.1193] [D loss: 0.1123]\n",
            "[Epoch 7] [Batch 4800] [G loss: 1.9078] [D loss: 0.0880]\n",
            "[Epoch 7] [Batch 4900] [G loss: 2.5732] [D loss: 0.1189]\n",
            "[Epoch 7] [Batch 5000] [G loss: 1.9834] [D loss: 0.1540]\n",
            "[Epoch 7] [Batch 5100] [G loss: 2.5155] [D loss: 0.0498]\n",
            "[Epoch 7] [Batch 5200] [G loss: 1.9113] [D loss: 0.1514]\n",
            "[Epoch 7] [Batch 5300] [G loss: 1.9281] [D loss: 0.2873]\n",
            "[Epoch 7] [Batch 5400] [G loss: 2.4045] [D loss: 0.1821]\n",
            "[Epoch 7] [Batch 5500] [G loss: 2.3442] [D loss: 0.1025]\n",
            "[Epoch 7] [Batch 5600] [G loss: 1.6739] [D loss: 0.1543]\n",
            "[Epoch 7] [Batch 5700] [G loss: 2.1266] [D loss: 0.1374]\n",
            "[Epoch 7] [Batch 5800] [G loss: 2.0576] [D loss: 0.1587]\n",
            "[Epoch 7] [Batch 5900] [G loss: 1.9791] [D loss: 0.0467]\n",
            "[Epoch 7] [Batch 6000] [G loss: 1.5483] [D loss: 0.1033]\n",
            "[Epoch 7] [Batch 6100] [G loss: 1.4933] [D loss: 0.1026]\n",
            "[Epoch 7] [Batch 6200] [G loss: 2.0058] [D loss: 0.1835]\n",
            "[Epoch 7] [Batch 6300] [G loss: 3.3957] [D loss: 0.2125]\n",
            "[Epoch 7] [Batch 6400] [G loss: 1.4255] [D loss: 0.3140]\n",
            "[Epoch 7] [Batch 6500] [G loss: 2.4029] [D loss: 0.0583]\n",
            "[Epoch 7] [Batch 6600] [G loss: 1.9010] [D loss: 0.1562]\n",
            "[Epoch 7] [Batch 6700] [G loss: 1.8542] [D loss: 0.1294]\n",
            "[Epoch 7] [Batch 6800] [G loss: 2.0285] [D loss: 0.1505]\n",
            "[Epoch 7] [Batch 6900] [G loss: 1.5873] [D loss: 0.1151]\n",
            "[Epoch 7] [Batch 7000] [G loss: 1.8653] [D loss: 0.1020]\n",
            "Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints/epoch_7.pth\n",
            "[Epoch 8] [Batch 0] [G loss: 1.7998] [D loss: 0.1346]\n",
            "[Epoch 8] [Batch 100] [G loss: 2.2957] [D loss: 0.1724]\n",
            "[Epoch 8] [Batch 200] [G loss: 2.0475] [D loss: 0.0842]\n",
            "[Epoch 8] [Batch 300] [G loss: 1.6245] [D loss: 0.1150]\n",
            "[Epoch 8] [Batch 400] [G loss: 1.7485] [D loss: 0.1877]\n",
            "[Epoch 8] [Batch 500] [G loss: 1.8354] [D loss: 0.1019]\n",
            "[Epoch 8] [Batch 600] [G loss: 1.9052] [D loss: 0.0855]\n",
            "[Epoch 8] [Batch 700] [G loss: 1.7115] [D loss: 0.1337]\n",
            "[Epoch 8] [Batch 800] [G loss: 2.0109] [D loss: 0.2078]\n",
            "[Epoch 8] [Batch 900] [G loss: 1.8558] [D loss: 0.1100]\n",
            "[Epoch 8] [Batch 1000] [G loss: 1.2891] [D loss: 0.0938]\n",
            "[Epoch 8] [Batch 1100] [G loss: 2.2157] [D loss: 0.1572]\n",
            "[Epoch 8] [Batch 1200] [G loss: 2.1432] [D loss: 0.1600]\n",
            "[Epoch 8] [Batch 1300] [G loss: 2.0005] [D loss: 0.1309]\n",
            "[Epoch 8] [Batch 1400] [G loss: 1.5624] [D loss: 0.1893]\n",
            "[Epoch 8] [Batch 1500] [G loss: 1.5124] [D loss: 0.0561]\n",
            "[Epoch 8] [Batch 1600] [G loss: 2.4042] [D loss: 0.1674]\n",
            "[Epoch 8] [Batch 1700] [G loss: 2.9905] [D loss: 0.1293]\n",
            "[Epoch 8] [Batch 1800] [G loss: 2.5626] [D loss: 0.0897]\n",
            "[Epoch 8] [Batch 1900] [G loss: 1.5959] [D loss: 0.1359]\n",
            "[Epoch 8] [Batch 2000] [G loss: 1.6185] [D loss: 0.0919]\n",
            "[Epoch 8] [Batch 2100] [G loss: 1.6640] [D loss: 0.0745]\n",
            "[Epoch 8] [Batch 2200] [G loss: 1.9128] [D loss: 0.0745]\n",
            "[Epoch 8] [Batch 2300] [G loss: 1.4778] [D loss: 0.3227]\n",
            "[Epoch 8] [Batch 2400] [G loss: 2.0272] [D loss: 0.1499]\n",
            "[Epoch 8] [Batch 2500] [G loss: 1.7555] [D loss: 0.1201]\n",
            "[Epoch 8] [Batch 2600] [G loss: 2.2542] [D loss: 0.1903]\n",
            "[Epoch 8] [Batch 2700] [G loss: 1.9519] [D loss: 0.1779]\n",
            "[Epoch 8] [Batch 2800] [G loss: 2.0125] [D loss: 0.2003]\n",
            "[Epoch 8] [Batch 2900] [G loss: 2.2209] [D loss: 0.0506]\n",
            "[Epoch 8] [Batch 3000] [G loss: 2.5040] [D loss: 0.1691]\n",
            "[Epoch 8] [Batch 3100] [G loss: 2.1043] [D loss: 0.1690]\n",
            "[Epoch 8] [Batch 3200] [G loss: 2.0346] [D loss: 0.1584]\n",
            "[Epoch 8] [Batch 3300] [G loss: 2.2742] [D loss: 0.1438]\n",
            "[Epoch 8] [Batch 3400] [G loss: 1.8287] [D loss: 0.0830]\n",
            "[Epoch 8] [Batch 3500] [G loss: 1.8613] [D loss: 0.1334]\n",
            "[Epoch 8] [Batch 3600] [G loss: 2.0185] [D loss: 0.0457]\n",
            "[Epoch 8] [Batch 3700] [G loss: 2.0674] [D loss: 0.0610]\n",
            "[Epoch 8] [Batch 3800] [G loss: 1.4721] [D loss: 0.0926]\n",
            "[Epoch 8] [Batch 3900] [G loss: 1.9499] [D loss: 0.1016]\n",
            "[Epoch 8] [Batch 4000] [G loss: 2.0372] [D loss: 0.2968]\n",
            "[Epoch 8] [Batch 4100] [G loss: 2.8793] [D loss: 0.1496]\n",
            "[Epoch 8] [Batch 4200] [G loss: 2.3593] [D loss: 0.1180]\n",
            "[Epoch 8] [Batch 4300] [G loss: 1.3792] [D loss: 0.0904]\n",
            "[Epoch 8] [Batch 4400] [G loss: 1.8855] [D loss: 0.0415]\n",
            "[Epoch 8] [Batch 4500] [G loss: 2.0246] [D loss: 0.1117]\n",
            "[Epoch 8] [Batch 4600] [G loss: 2.8121] [D loss: 0.1209]\n",
            "[Epoch 8] [Batch 4700] [G loss: 1.6733] [D loss: 0.0822]\n",
            "[Epoch 8] [Batch 4800] [G loss: 1.8268] [D loss: 0.0438]\n",
            "[Epoch 8] [Batch 4900] [G loss: 1.6606] [D loss: 0.0840]\n",
            "[Epoch 8] [Batch 5000] [G loss: 2.0057] [D loss: 0.1142]\n",
            "[Epoch 8] [Batch 5100] [G loss: 1.8348] [D loss: 0.0352]\n",
            "[Epoch 8] [Batch 5200] [G loss: 2.0495] [D loss: 0.1452]\n",
            "[Epoch 8] [Batch 5300] [G loss: 1.8068] [D loss: 0.1922]\n",
            "[Epoch 8] [Batch 5400] [G loss: 2.1883] [D loss: 0.1518]\n",
            "[Epoch 8] [Batch 5500] [G loss: 1.8268] [D loss: 0.1117]\n",
            "[Epoch 8] [Batch 5600] [G loss: 2.3233] [D loss: 0.0900]\n",
            "[Epoch 8] [Batch 5700] [G loss: 2.1080] [D loss: 0.2810]\n",
            "[Epoch 8] [Batch 5800] [G loss: 3.2452] [D loss: 0.2584]\n",
            "[Epoch 8] [Batch 5900] [G loss: 1.8748] [D loss: 0.0763]\n",
            "[Epoch 8] [Batch 6000] [G loss: 2.2808] [D loss: 0.1090]\n",
            "[Epoch 8] [Batch 6100] [G loss: 2.6091] [D loss: 0.1784]\n",
            "[Epoch 8] [Batch 6200] [G loss: 2.0511] [D loss: 0.2297]\n",
            "[Epoch 8] [Batch 6300] [G loss: 2.9725] [D loss: 0.2910]\n",
            "[Epoch 8] [Batch 6400] [G loss: 1.1895] [D loss: 0.1601]\n",
            "[Epoch 8] [Batch 6500] [G loss: 2.3585] [D loss: 0.0672]\n",
            "[Epoch 8] [Batch 6600] [G loss: 2.1161] [D loss: 0.1494]\n",
            "[Epoch 8] [Batch 6700] [G loss: 1.8815] [D loss: 0.0864]\n",
            "[Epoch 8] [Batch 6800] [G loss: 2.0008] [D loss: 0.1460]\n",
            "[Epoch 8] [Batch 6900] [G loss: 2.0806] [D loss: 0.2346]\n",
            "[Epoch 8] [Batch 7000] [G loss: 2.1653] [D loss: 0.2641]\n",
            "Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints/epoch_8.pth\n",
            "[Epoch 9] [Batch 0] [G loss: 1.6628] [D loss: 0.1475]\n",
            "[Epoch 9] [Batch 100] [G loss: 2.5531] [D loss: 0.0728]\n",
            "[Epoch 9] [Batch 200] [G loss: 1.7597] [D loss: 0.2162]\n",
            "[Epoch 9] [Batch 300] [G loss: 1.5111] [D loss: 0.2039]\n",
            "[Epoch 9] [Batch 400] [G loss: 1.3487] [D loss: 0.1686]\n",
            "[Epoch 9] [Batch 500] [G loss: 1.1977] [D loss: 0.0915]\n",
            "[Epoch 9] [Batch 600] [G loss: 2.2920] [D loss: 0.0687]\n",
            "[Epoch 9] [Batch 700] [G loss: 1.6836] [D loss: 0.2805]\n",
            "[Epoch 9] [Batch 800] [G loss: 2.0202] [D loss: 0.2058]\n",
            "[Epoch 9] [Batch 900] [G loss: 2.1847] [D loss: 0.1404]\n",
            "[Epoch 9] [Batch 1000] [G loss: 1.9392] [D loss: 0.1465]\n",
            "[Epoch 9] [Batch 1100] [G loss: 1.9843] [D loss: 0.0832]\n",
            "[Epoch 9] [Batch 1200] [G loss: 1.9112] [D loss: 0.1440]\n",
            "[Epoch 9] [Batch 1300] [G loss: 2.1358] [D loss: 0.0791]\n",
            "[Epoch 9] [Batch 1400] [G loss: 2.2511] [D loss: 0.0929]\n",
            "[Epoch 9] [Batch 1500] [G loss: 1.8825] [D loss: 0.0969]\n",
            "[Epoch 9] [Batch 1600] [G loss: 2.0461] [D loss: 0.0735]\n",
            "[Epoch 9] [Batch 1700] [G loss: 2.1032] [D loss: 0.1922]\n",
            "[Epoch 9] [Batch 1800] [G loss: 2.2113] [D loss: 0.0838]\n",
            "[Epoch 9] [Batch 1900] [G loss: 1.5189] [D loss: 0.2035]\n",
            "[Epoch 9] [Batch 2000] [G loss: 2.0500] [D loss: 0.0735]\n",
            "[Epoch 9] [Batch 2100] [G loss: 2.4259] [D loss: 0.1104]\n",
            "[Epoch 9] [Batch 2200] [G loss: 1.7911] [D loss: 0.0808]\n",
            "[Epoch 9] [Batch 2300] [G loss: 2.3373] [D loss: 0.2457]\n",
            "[Epoch 9] [Batch 2400] [G loss: 2.0882] [D loss: 0.0838]\n",
            "[Epoch 9] [Batch 2500] [G loss: 1.5905] [D loss: 0.0738]\n",
            "[Epoch 9] [Batch 2600] [G loss: 1.8856] [D loss: 0.0828]\n",
            "[Epoch 9] [Batch 2700] [G loss: 1.3912] [D loss: 0.1675]\n",
            "[Epoch 9] [Batch 2800] [G loss: 1.0410] [D loss: 0.1306]\n",
            "[Epoch 9] [Batch 2900] [G loss: 1.5574] [D loss: 0.3104]\n",
            "[Epoch 9] [Batch 3000] [G loss: 1.4914] [D loss: 0.1077]\n",
            "[Epoch 9] [Batch 3100] [G loss: 1.3526] [D loss: 0.1018]\n",
            "[Epoch 9] [Batch 3200] [G loss: 1.9365] [D loss: 0.0816]\n",
            "[Epoch 9] [Batch 3300] [G loss: 1.7465] [D loss: 0.0327]\n",
            "[Epoch 9] [Batch 3400] [G loss: 1.0865] [D loss: 0.0900]\n",
            "[Epoch 9] [Batch 3500] [G loss: 1.5165] [D loss: 0.2086]\n",
            "[Epoch 9] [Batch 3600] [G loss: 1.6756] [D loss: 0.0510]\n",
            "[Epoch 9] [Batch 3700] [G loss: 1.5668] [D loss: 0.1318]\n",
            "[Epoch 9] [Batch 3800] [G loss: 1.5641] [D loss: 0.0810]\n",
            "[Epoch 9] [Batch 3900] [G loss: 2.2564] [D loss: 0.2542]\n",
            "[Epoch 9] [Batch 4000] [G loss: 1.2654] [D loss: 0.1950]\n",
            "[Epoch 9] [Batch 4100] [G loss: 1.8211] [D loss: 0.0380]\n",
            "[Epoch 9] [Batch 4200] [G loss: 1.9141] [D loss: 0.1676]\n",
            "[Epoch 9] [Batch 4300] [G loss: 2.1136] [D loss: 0.1477]\n",
            "[Epoch 9] [Batch 4400] [G loss: 1.4658] [D loss: 0.1629]\n",
            "[Epoch 9] [Batch 4500] [G loss: 1.9445] [D loss: 0.1907]\n",
            "[Epoch 9] [Batch 4600] [G loss: 1.4971] [D loss: 0.1446]\n",
            "[Epoch 9] [Batch 4700] [G loss: 1.5615] [D loss: 0.0592]\n",
            "[Epoch 9] [Batch 4800] [G loss: 2.4004] [D loss: 0.0750]\n",
            "[Epoch 9] [Batch 4900] [G loss: 2.1834] [D loss: 0.0991]\n",
            "[Epoch 9] [Batch 5000] [G loss: 1.6319] [D loss: 0.1011]\n",
            "[Epoch 9] [Batch 5100] [G loss: 1.5625] [D loss: 0.0759]\n",
            "[Epoch 9] [Batch 5200] [G loss: 1.8524] [D loss: 0.2756]\n",
            "[Epoch 9] [Batch 5300] [G loss: 1.5435] [D loss: 0.2266]\n",
            "[Epoch 9] [Batch 5400] [G loss: 1.7537] [D loss: 0.1433]\n",
            "[Epoch 9] [Batch 5500] [G loss: 1.8495] [D loss: 0.1065]\n",
            "[Epoch 9] [Batch 5600] [G loss: 1.9253] [D loss: 0.0929]\n",
            "[Epoch 9] [Batch 5700] [G loss: 2.1934] [D loss: 0.0474]\n",
            "[Epoch 9] [Batch 5800] [G loss: 2.1198] [D loss: 0.0421]\n",
            "[Epoch 9] [Batch 5900] [G loss: 1.4695] [D loss: 0.1466]\n",
            "[Epoch 9] [Batch 6000] [G loss: 2.4707] [D loss: 0.1167]\n",
            "[Epoch 9] [Batch 6100] [G loss: 1.4954] [D loss: 0.0650]\n",
            "[Epoch 9] [Batch 6200] [G loss: 1.4702] [D loss: 0.0941]\n",
            "[Epoch 9] [Batch 6300] [G loss: 1.7436] [D loss: 0.1339]\n",
            "[Epoch 9] [Batch 6400] [G loss: 1.4597] [D loss: 0.0758]\n",
            "[Epoch 9] [Batch 6500] [G loss: 2.1104] [D loss: 0.0774]\n",
            "[Epoch 9] [Batch 6600] [G loss: 2.1896] [D loss: 0.1276]\n",
            "[Epoch 9] [Batch 6700] [G loss: 1.6748] [D loss: 0.0506]\n",
            "[Epoch 9] [Batch 6800] [G loss: 2.2857] [D loss: 0.0955]\n",
            "[Epoch 9] [Batch 6900] [G loss: 2.3923] [D loss: 0.1107]\n",
            "[Epoch 9] [Batch 7000] [G loss: 1.5506] [D loss: 0.0870]\n",
            "Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints/epoch_9.pth\n"
          ]
        }
      ],
      "source": [
        "# --- UPDATED TRAINING LOOP (FIXED ORDER) ---\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "print(\"‚è≥ Training Started with Mixed Precision (AMP), Soft Labels, and Instance Noise...\")\n",
        "\n",
        "for epoch in range(start_epoch, Config.EPOCHS):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        real_A = batch[\"photo\"].to(Config.DEVICE)\n",
        "        real_B = batch[\"monet\"].to(Config.DEVICE)\n",
        "\n",
        "        # --- TARGETS ---\n",
        "        valid = torch.ones((real_A.size(0), 1, 16, 16), requires_grad=False).to(Config.DEVICE)\n",
        "        fake = torch.zeros((real_A.size(0), 1, 16, 16), requires_grad=False).to(Config.DEVICE)\n",
        "        valid_smooth = torch.full((real_A.size(0), 1, 16, 16), Config.REAL_LABEL_SMOOTH, requires_grad=False).to(Config.DEVICE)\n",
        "\n",
        "        # ------------------\n",
        "        #  Train Generators\n",
        "        # ------------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            # Identity loss\n",
        "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
        "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
        "            loss_identity = (loss_id_A + loss_id_B) / 2 * Config.LAMBDA_ID\n",
        "\n",
        "            # GAN loss\n",
        "            fake_B = G_AB(real_A)\n",
        "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
        "            fake_A = G_BA(real_B)\n",
        "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
        "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
        "\n",
        "            # Cycle loss\n",
        "            recov_A = G_BA(fake_B)\n",
        "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
        "            recov_B = G_AB(fake_A)\n",
        "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
        "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2 * Config.LAMBDA_CYCLE\n",
        "\n",
        "            # Total loss\n",
        "            loss_G = loss_GAN + loss_cycle + loss_identity\n",
        "\n",
        "        scaler.scale(loss_G).backward()\n",
        "        scaler.step(optimizer_G)\n",
        "        scaler.update()\n",
        "\n",
        "        # ==========================================\n",
        "        #  Train Discriminator A (FIXED)\n",
        "        # ==========================================\n",
        "        optimizer_D_A.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            # 1. Get the buffered image FIRST\n",
        "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
        "\n",
        "            # 2. Generate noise\n",
        "            noise_real = torch.randn_like(real_A) * 0.05\n",
        "            noise_fake = torch.randn_like(fake_A_) * 0.05\n",
        "\n",
        "            # 3. Add noise\n",
        "            real_A_noisy = real_A + noise_real\n",
        "            fake_A_noisy = fake_A_.detach() + noise_fake\n",
        "\n",
        "            # 4. Calculate Loss\n",
        "            loss_real = criterion_GAN(D_A(real_A_noisy), valid_smooth)\n",
        "            loss_fake = criterion_GAN(D_A(fake_A_noisy), fake)\n",
        "            loss_D_A = (loss_real + loss_fake) / 2\n",
        "\n",
        "        scaler.scale(loss_D_A).backward()\n",
        "        scaler.step(optimizer_D_A)\n",
        "        scaler.update()\n",
        "\n",
        "        # ==========================================\n",
        "        #  Train Discriminator B (FIXED)\n",
        "        # ==========================================\n",
        "        optimizer_D_B.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            # 1. Get the buffered image FIRST\n",
        "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
        "\n",
        "            # 2. Generate noise\n",
        "            noise_real = torch.randn_like(real_B) * 0.05\n",
        "            noise_fake = torch.randn_like(fake_B_) * 0.05\n",
        "\n",
        "            # 3. Add noise\n",
        "            real_B_noisy = real_B + noise_real\n",
        "            fake_B_noisy = fake_B_.detach() + noise_fake\n",
        "\n",
        "            # 4. Calculate Loss\n",
        "            loss_real = criterion_GAN(D_B(real_B_noisy), valid_smooth)\n",
        "            loss_fake = criterion_GAN(D_B(fake_B_noisy), fake)\n",
        "            loss_D_B = (loss_real + loss_fake) / 2\n",
        "\n",
        "        scaler.scale(loss_D_B).backward()\n",
        "        scaler.step(optimizer_D_B)\n",
        "        scaler.update()\n",
        "\n",
        "        # --- Logging Losses (Inside Loop) ---\n",
        "        if i % 100 == 0:\n",
        "            wandb.log({\n",
        "                \"Loss/Generator\": loss_G.item(),\n",
        "                \"Loss/Discriminator\": (loss_D_A.item() + loss_D_B.item()),\n",
        "                \"Epoch\": epoch\n",
        "            })\n",
        "            print(f\"[Epoch {epoch}] [Batch {i}] [G loss: {loss_G.item():.4f}] [D loss: {(loss_D_A.item() + loss_D_B.item()):.4f}]\")\n",
        "\n",
        "    # --- Logging Images (Once per Epoch) ---\n",
        "    img_real_A = real_A[0].detach().cpu() * 0.5 + 0.5\n",
        "    img_fake_B = fake_B[0].detach().cpu() * 0.5 + 0.5\n",
        "    img_real_B = real_B[0].detach().cpu() * 0.5 + 0.5\n",
        "    img_fake_A = fake_A[0].detach().cpu() * 0.5 + 0.5\n",
        "\n",
        "    wandb.log({\n",
        "        \"Visual/Real Photo\": wandb.Image(img_real_A, caption=f\"Real Photo (Epoch {epoch})\"),\n",
        "        \"Visual/Generated Monet\": wandb.Image(img_fake_B, caption=f\"Generated Monet (Epoch {epoch})\"),\n",
        "        \"Visual/Real Monet\": wandb.Image(img_real_B, caption=f\"Real Monet (Epoch {epoch})\"),\n",
        "        \"Visual/Generated Photo\": wandb.Image(img_fake_A, caption=f\"Reconstructed Photo (Epoch {epoch})\")\n",
        "    })\n",
        "\n",
        "    # --- Save Checkpoint ---\n",
        "    if epoch % Config.SAVE_EPOCH_FREQ == 0:\n",
        "        save_path = f\"{Config.CHECKPOINT_DIR}/epoch_{epoch}.pth\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'G_AB': G_AB.state_dict(),\n",
        "            'G_BA': G_BA.state_dict(),\n",
        "            'D_A': D_A.state_dict(),\n",
        "            'D_B': D_B.state_dict(),\n",
        "            'optimizer_G': optimizer_G.state_dict(),\n",
        "            'optimizer_D_A': optimizer_D_A.state_dict(),\n",
        "            'optimizer_D_B': optimizer_D_B.state_dict()\n",
        "        }, save_path)\n",
        "        print(f\"Checkpoint saved: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "e0472210",
        "outputId": "8e343cb3-bb83-4014-db12-7bc31515a8a2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ</td></tr><tr><td>Loss/Discriminator</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Loss/Generator</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>9</td></tr><tr><td>Loss/Discriminator</td><td>0.08697</td></tr><tr><td>Loss/Generator</td><td>1.55063</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">giddy-galaxy-9</strong> at: <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter/runs/de1fj2k2' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter/runs/de1fj2k2</a><br> View project at: <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter</a><br>Synced 5 W&B file(s), 64 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251219_053506-de1fj2k2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCHnUGMf1ImM"
      },
      "source": [
        "#**U**-net experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekmdhzIm2L95"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.amp import GradScaler, autocast # Mixed Precision\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtmRCjFV2sFx"
      },
      "outputs": [],
      "source": [
        "sys.path.append(\"/content/drive/MyDrive/University/Painter_Assignment/src\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFEY45xD2v9C"
      },
      "outputs": [],
      "source": [
        "from config_unet import ConfigUNet\n",
        "from dataset import UnpairedDataset\n",
        "from models import GeneratorUNet, Discriminator\n",
        "from utils import weights_init_normal, ReplayBuffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "e78J03dK21ol",
        "outputId": "9c052cfd-8692-4e58-fa16-1e83447acf76"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251220_084136-s9pgls7g</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET/runs/s9pgls7g' target=\"_blank\">absurd-cloud-2</a></strong> to <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET/runs/s9pgls7g' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET/runs/s9pgls7g</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET/runs/s9pgls7g?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fee248f5eb0>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(\n",
        "    project=ConfigUNet.PROJECT_NAME,\n",
        "    config={k:v for k,v in ConfigUNet.__dict__.items() if not k.startswith('__')},\n",
        "    reinit=True # Allows starting a new run without closing the old one\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umhk6_cw133K",
        "outputId": "ea167c74-26ef-4d6d-b44c-905ffd2c0e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting EXPERIMENT 2: U-Net Architecture on cuda...\n"
          ]
        }
      ],
      "source": [
        "print(f\"Starting EXPERIMENT 2: U-Net Architecture on {ConfigUNet.DEVICE}...\")\n",
        "\n",
        "# 4. Initialize U-Net Generators\n",
        "G_AB = GeneratorUNet().to(ConfigUNet.DEVICE) # Photo -> Monet\n",
        "G_BA = GeneratorUNet().to(ConfigUNet.DEVICE) # Monet -> Photo\n",
        "D_A = Discriminator().to(ConfigUNet.DEVICE)\n",
        "D_B = Discriminator().to(ConfigUNet.DEVICE)\n",
        "\n",
        "# 5. Optimizers & Loss\n",
        "optimizer_G = torch.optim.Adam(\n",
        "    list(G_AB.parameters()) + list(G_BA.parameters()),\n",
        "    lr=ConfigUNet.LR, betas=(ConfigUNet.B1, ConfigUNet.B2)\n",
        ")\n",
        "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=ConfigUNet.LR, betas=(ConfigUNet.B1, ConfigUNet.B2))\n",
        "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=ConfigUNet.LR, betas=(ConfigUNet.B1, ConfigUNet.B2))\n",
        "\n",
        "criterion_GAN = nn.MSELoss()\n",
        "criterion_cycle = nn.L1Loss()\n",
        "criterion_identity = nn.L1Loss()\n",
        "\n",
        "# 6. Initialize Weights (Fresh Start)\n",
        "G_AB.apply(weights_init_normal)\n",
        "G_BA.apply(weights_init_normal)\n",
        "D_A.apply(weights_init_normal)\n",
        "D_B.apply(weights_init_normal)\n",
        "\n",
        "# 7. Data Loader\n",
        "dataset = UnpairedDataset(ConfigUNet.TRAIN_MONET, ConfigUNet.TRAIN_PHOTO)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=ConfigUNet.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=ConfigUNet.NUM_WORKERS\n",
        ")\n",
        "\n",
        "# 8. Buffers & Scaler\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()\n",
        "scaler = GradScaler('cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWmG4OO_28x1",
        "outputId": "0637426c-e3a5-4ec9-e9f3-832a25fcb14b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 0] [Batch 0] [G loss: 11.0310]\n",
            "[Epoch 0] [Batch 100] [G loss: 2.5375]\n",
            "[Epoch 0] [Batch 200] [G loss: 3.7616]\n",
            "[Epoch 0] [Batch 300] [G loss: 1.6436]\n",
            "[Epoch 0] [Batch 400] [G loss: 1.9185]\n",
            "[Epoch 0] [Batch 500] [G loss: 2.2433]\n",
            "[Epoch 0] [Batch 600] [G loss: 1.5672]\n",
            "[Epoch 0] [Batch 700] [G loss: 1.4593]\n",
            "[Epoch 0] [Batch 800] [G loss: 1.5946]\n",
            "[Epoch 0] [Batch 900] [G loss: 1.4376]\n",
            "[Epoch 0] [Batch 1000] [G loss: 1.2024]\n",
            "[Epoch 0] [Batch 1100] [G loss: 1.5474]\n",
            "[Epoch 0] [Batch 1200] [G loss: 1.2385]\n",
            "[Epoch 0] [Batch 1300] [G loss: 1.6008]\n",
            "[Epoch 0] [Batch 1400] [G loss: 1.9840]\n",
            "[Epoch 0] [Batch 1500] [G loss: 1.4590]\n",
            "[Epoch 0] [Batch 1600] [G loss: 1.5041]\n",
            "[Epoch 0] [Batch 1700] [G loss: 1.0736]\n",
            "[Epoch 0] [Batch 1800] [G loss: 1.1419]\n",
            "[Epoch 0] [Batch 1900] [G loss: 1.1036]\n",
            "[Epoch 0] [Batch 2000] [G loss: 1.1801]\n",
            "[Epoch 0] [Batch 2100] [G loss: 1.5649]\n",
            "[Epoch 0] [Batch 2200] [G loss: 1.1619]\n",
            "[Epoch 0] [Batch 2300] [G loss: 1.3058]\n",
            "[Epoch 0] [Batch 2400] [G loss: 1.1057]\n",
            "[Epoch 0] [Batch 2500] [G loss: 1.4710]\n",
            "[Epoch 0] [Batch 2600] [G loss: 1.5119]\n",
            "[Epoch 0] [Batch 2700] [G loss: 1.1775]\n",
            "[Epoch 0] [Batch 2800] [G loss: 1.4669]\n",
            "[Epoch 0] [Batch 2900] [G loss: 1.4929]\n",
            "[Epoch 0] [Batch 3000] [G loss: 1.5087]\n",
            "[Epoch 0] [Batch 3100] [G loss: 1.3398]\n",
            "[Epoch 0] [Batch 3200] [G loss: 1.2277]\n",
            "[Epoch 0] [Batch 3300] [G loss: 1.0512]\n",
            "[Epoch 0] [Batch 3400] [G loss: 1.5058]\n",
            "[Epoch 0] [Batch 3500] [G loss: 1.1340]\n",
            "[Epoch 0] [Batch 3600] [G loss: 1.4128]\n",
            "[Epoch 0] [Batch 3700] [G loss: 0.6962]\n",
            "[Epoch 0] [Batch 3800] [G loss: 1.1730]\n",
            "[Epoch 0] [Batch 3900] [G loss: 1.3582]\n",
            "[Epoch 0] [Batch 4000] [G loss: 0.9402]\n",
            "[Epoch 0] [Batch 4100] [G loss: 1.4749]\n",
            "[Epoch 0] [Batch 4200] [G loss: 0.9387]\n",
            "[Epoch 0] [Batch 4300] [G loss: 1.1703]\n",
            "[Epoch 0] [Batch 4400] [G loss: 1.2362]\n",
            "[Epoch 0] [Batch 4500] [G loss: 1.3750]\n",
            "[Epoch 0] [Batch 4600] [G loss: 1.6516]\n",
            "[Epoch 0] [Batch 4700] [G loss: 0.9485]\n",
            "[Epoch 0] [Batch 4800] [G loss: 1.3900]\n",
            "[Epoch 0] [Batch 4900] [G loss: 1.1735]\n",
            "[Epoch 0] [Batch 5000] [G loss: 1.5115]\n",
            "[Epoch 0] [Batch 5100] [G loss: 1.2935]\n",
            "[Epoch 0] [Batch 5200] [G loss: 1.5329]\n",
            "[Epoch 0] [Batch 5300] [G loss: 1.0842]\n",
            "[Epoch 0] [Batch 5400] [G loss: 1.1323]\n",
            "[Epoch 0] [Batch 5500] [G loss: 1.2079]\n",
            "[Epoch 0] [Batch 5600] [G loss: 1.0326]\n",
            "[Epoch 0] [Batch 5700] [G loss: 1.0789]\n",
            "[Epoch 0] [Batch 5800] [G loss: 1.3734]\n",
            "[Epoch 0] [Batch 5900] [G loss: 1.2342]\n",
            "[Epoch 0] [Batch 6000] [G loss: 1.0070]\n",
            "[Epoch 0] [Batch 6100] [G loss: 1.1358]\n",
            "[Epoch 0] [Batch 6200] [G loss: 0.8241]\n",
            "[Epoch 0] [Batch 6300] [G loss: 1.0267]\n",
            "[Epoch 0] [Batch 6400] [G loss: 1.0917]\n",
            "[Epoch 0] [Batch 6500] [G loss: 1.4351]\n",
            "[Epoch 0] [Batch 6600] [G loss: 1.0687]\n",
            "[Epoch 0] [Batch 6700] [G loss: 0.9199]\n",
            "[Epoch 0] [Batch 6800] [G loss: 1.1692]\n",
            "[Epoch 0] [Batch 6900] [G loss: 1.1550]\n",
            "[Epoch 0] [Batch 7000] [G loss: 1.5624]\n",
            "üíæ UNet Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints_unet/epoch_0.pth\n",
            "[Epoch 1] [Batch 0] [G loss: 1.2371]\n",
            "[Epoch 1] [Batch 100] [G loss: 1.3762]\n",
            "[Epoch 1] [Batch 200] [G loss: 1.3412]\n",
            "[Epoch 1] [Batch 300] [G loss: 1.0697]\n",
            "[Epoch 1] [Batch 400] [G loss: 1.3961]\n",
            "[Epoch 1] [Batch 500] [G loss: 1.5950]\n",
            "[Epoch 1] [Batch 600] [G loss: 1.4401]\n",
            "[Epoch 1] [Batch 700] [G loss: 1.1453]\n",
            "[Epoch 1] [Batch 800] [G loss: 2.0030]\n",
            "[Epoch 1] [Batch 900] [G loss: 1.9273]\n",
            "[Epoch 1] [Batch 1000] [G loss: 1.4044]\n",
            "[Epoch 1] [Batch 1100] [G loss: 1.2827]\n",
            "[Epoch 1] [Batch 1200] [G loss: 1.2252]\n",
            "[Epoch 1] [Batch 1300] [G loss: 0.8928]\n",
            "[Epoch 1] [Batch 1400] [G loss: 1.0974]\n",
            "[Epoch 1] [Batch 1500] [G loss: 1.1470]\n",
            "[Epoch 1] [Batch 1600] [G loss: 1.0391]\n",
            "[Epoch 1] [Batch 1700] [G loss: 1.2172]\n",
            "[Epoch 1] [Batch 1800] [G loss: 1.3111]\n",
            "[Epoch 1] [Batch 1900] [G loss: 1.3373]\n",
            "[Epoch 1] [Batch 2000] [G loss: 1.3800]\n",
            "[Epoch 1] [Batch 2100] [G loss: 1.1924]\n",
            "[Epoch 1] [Batch 2200] [G loss: 1.4957]\n",
            "[Epoch 1] [Batch 2300] [G loss: 1.0609]\n",
            "[Epoch 1] [Batch 2400] [G loss: 1.1851]\n",
            "[Epoch 1] [Batch 2500] [G loss: 1.0208]\n",
            "[Epoch 1] [Batch 2600] [G loss: 0.9855]\n",
            "[Epoch 1] [Batch 2700] [G loss: 1.2849]\n",
            "[Epoch 1] [Batch 2800] [G loss: 1.1853]\n",
            "[Epoch 1] [Batch 2900] [G loss: 1.2429]\n",
            "[Epoch 1] [Batch 3000] [G loss: 0.9602]\n",
            "[Epoch 1] [Batch 3100] [G loss: 1.3470]\n",
            "[Epoch 1] [Batch 3200] [G loss: 1.2022]\n",
            "[Epoch 1] [Batch 3300] [G loss: 1.1298]\n",
            "[Epoch 1] [Batch 3400] [G loss: 1.2107]\n",
            "[Epoch 1] [Batch 3500] [G loss: 1.1715]\n",
            "[Epoch 1] [Batch 3600] [G loss: 1.1049]\n",
            "[Epoch 1] [Batch 3700] [G loss: 1.1143]\n",
            "[Epoch 1] [Batch 3800] [G loss: 1.4026]\n",
            "[Epoch 1] [Batch 3900] [G loss: 1.3894]\n",
            "[Epoch 1] [Batch 4000] [G loss: 1.1541]\n",
            "[Epoch 1] [Batch 4100] [G loss: 1.4699]\n",
            "[Epoch 1] [Batch 4200] [G loss: 1.3883]\n",
            "[Epoch 1] [Batch 4300] [G loss: 0.8217]\n",
            "[Epoch 1] [Batch 4400] [G loss: 1.2439]\n",
            "[Epoch 1] [Batch 4500] [G loss: 0.9813]\n",
            "[Epoch 1] [Batch 4600] [G loss: 1.1454]\n",
            "[Epoch 1] [Batch 4700] [G loss: 1.4115]\n",
            "[Epoch 1] [Batch 4800] [G loss: 0.7819]\n",
            "[Epoch 1] [Batch 4900] [G loss: 1.0947]\n",
            "[Epoch 1] [Batch 5000] [G loss: 1.1524]\n",
            "[Epoch 1] [Batch 5100] [G loss: 1.4248]\n",
            "[Epoch 1] [Batch 5200] [G loss: 1.1418]\n",
            "[Epoch 1] [Batch 5300] [G loss: 1.0084]\n",
            "[Epoch 1] [Batch 5400] [G loss: 1.3065]\n",
            "[Epoch 1] [Batch 5500] [G loss: 1.1069]\n",
            "[Epoch 1] [Batch 5600] [G loss: 1.2558]\n",
            "[Epoch 1] [Batch 5700] [G loss: 1.1002]\n",
            "[Epoch 1] [Batch 5800] [G loss: 0.8735]\n",
            "[Epoch 1] [Batch 5900] [G loss: 1.2485]\n",
            "[Epoch 1] [Batch 6000] [G loss: 1.3113]\n",
            "[Epoch 1] [Batch 6100] [G loss: 1.3132]\n",
            "[Epoch 1] [Batch 6200] [G loss: 0.8770]\n",
            "[Epoch 1] [Batch 6300] [G loss: 0.9042]\n",
            "[Epoch 1] [Batch 6400] [G loss: 1.2463]\n",
            "[Epoch 1] [Batch 6500] [G loss: 1.0924]\n",
            "[Epoch 1] [Batch 6600] [G loss: 1.2114]\n",
            "[Epoch 1] [Batch 6700] [G loss: 1.5799]\n",
            "[Epoch 1] [Batch 6800] [G loss: 1.4505]\n",
            "[Epoch 1] [Batch 6900] [G loss: 0.9111]\n",
            "[Epoch 1] [Batch 7000] [G loss: 1.6463]\n",
            "[Epoch 2] [Batch 0] [G loss: 1.5428]\n",
            "[Epoch 2] [Batch 100] [G loss: 1.5727]\n",
            "[Epoch 2] [Batch 200] [G loss: 1.5676]\n",
            "[Epoch 2] [Batch 300] [G loss: 1.4794]\n",
            "[Epoch 2] [Batch 400] [G loss: 1.2531]\n",
            "[Epoch 2] [Batch 500] [G loss: 1.0990]\n",
            "[Epoch 2] [Batch 600] [G loss: 1.4070]\n",
            "[Epoch 2] [Batch 700] [G loss: 1.1545]\n",
            "[Epoch 2] [Batch 800] [G loss: 1.4855]\n",
            "[Epoch 2] [Batch 900] [G loss: 1.3202]\n",
            "[Epoch 2] [Batch 1000] [G loss: 1.4701]\n",
            "[Epoch 2] [Batch 1100] [G loss: 1.2545]\n",
            "[Epoch 2] [Batch 1200] [G loss: 1.6776]\n",
            "[Epoch 2] [Batch 1300] [G loss: 1.0330]\n",
            "[Epoch 2] [Batch 1400] [G loss: 1.2857]\n",
            "[Epoch 2] [Batch 1500] [G loss: 1.1736]\n",
            "[Epoch 2] [Batch 1600] [G loss: 1.3915]\n",
            "[Epoch 2] [Batch 1700] [G loss: 1.2745]\n",
            "[Epoch 2] [Batch 1800] [G loss: 1.3664]\n",
            "[Epoch 2] [Batch 1900] [G loss: 1.0462]\n",
            "[Epoch 2] [Batch 2000] [G loss: 1.3364]\n",
            "[Epoch 2] [Batch 2100] [G loss: 2.1767]\n",
            "[Epoch 2] [Batch 2200] [G loss: 1.1117]\n",
            "[Epoch 2] [Batch 2300] [G loss: 1.1078]\n",
            "[Epoch 2] [Batch 2400] [G loss: 0.7584]\n",
            "[Epoch 2] [Batch 2500] [G loss: 1.0778]\n",
            "[Epoch 2] [Batch 2600] [G loss: 1.3250]\n",
            "[Epoch 2] [Batch 2700] [G loss: 1.2257]\n",
            "[Epoch 2] [Batch 2800] [G loss: 1.4089]\n",
            "[Epoch 2] [Batch 2900] [G loss: 1.3816]\n",
            "[Epoch 2] [Batch 3000] [G loss: 1.1036]\n",
            "[Epoch 2] [Batch 3100] [G loss: 0.9511]\n",
            "[Epoch 2] [Batch 3200] [G loss: 1.4803]\n",
            "[Epoch 2] [Batch 3300] [G loss: 1.4260]\n",
            "[Epoch 2] [Batch 3400] [G loss: 1.3308]\n",
            "[Epoch 2] [Batch 3500] [G loss: 0.9567]\n",
            "[Epoch 2] [Batch 3600] [G loss: 1.4520]\n",
            "[Epoch 2] [Batch 3700] [G loss: 1.0090]\n",
            "[Epoch 2] [Batch 3800] [G loss: 1.6836]\n",
            "[Epoch 2] [Batch 3900] [G loss: 1.5363]\n",
            "[Epoch 2] [Batch 4000] [G loss: 1.1830]\n",
            "[Epoch 2] [Batch 4100] [G loss: 1.0025]\n",
            "[Epoch 2] [Batch 4200] [G loss: 1.4985]\n",
            "[Epoch 2] [Batch 4300] [G loss: 1.5245]\n",
            "[Epoch 2] [Batch 4400] [G loss: 0.9307]\n",
            "[Epoch 2] [Batch 4500] [G loss: 1.3394]\n",
            "[Epoch 2] [Batch 4600] [G loss: 1.7518]\n",
            "[Epoch 2] [Batch 4700] [G loss: 1.6416]\n",
            "[Epoch 2] [Batch 4800] [G loss: 1.0599]\n",
            "[Epoch 2] [Batch 4900] [G loss: 0.9317]\n",
            "[Epoch 2] [Batch 5000] [G loss: 1.3724]\n",
            "[Epoch 2] [Batch 5100] [G loss: 0.8650]\n",
            "[Epoch 2] [Batch 5200] [G loss: 1.3276]\n",
            "[Epoch 2] [Batch 5300] [G loss: 1.5178]\n",
            "[Epoch 2] [Batch 5400] [G loss: 1.0470]\n",
            "[Epoch 2] [Batch 5500] [G loss: 1.4484]\n",
            "[Epoch 2] [Batch 5600] [G loss: 1.1359]\n",
            "[Epoch 2] [Batch 5700] [G loss: 1.2925]\n",
            "[Epoch 2] [Batch 5800] [G loss: 1.5115]\n",
            "[Epoch 2] [Batch 5900] [G loss: 1.7149]\n",
            "[Epoch 2] [Batch 6000] [G loss: 0.9734]\n",
            "[Epoch 2] [Batch 6100] [G loss: 1.1289]\n",
            "[Epoch 2] [Batch 6200] [G loss: 0.9057]\n",
            "[Epoch 2] [Batch 6300] [G loss: 1.1510]\n",
            "[Epoch 2] [Batch 6400] [G loss: 1.3981]\n",
            "[Epoch 2] [Batch 6500] [G loss: 1.5797]\n",
            "[Epoch 2] [Batch 6600] [G loss: 1.5707]\n",
            "[Epoch 2] [Batch 6700] [G loss: 1.5199]\n",
            "[Epoch 2] [Batch 6800] [G loss: 1.7588]\n",
            "[Epoch 2] [Batch 6900] [G loss: 1.0139]\n",
            "[Epoch 2] [Batch 7000] [G loss: 1.0879]\n",
            "[Epoch 3] [Batch 0] [G loss: 1.2226]\n",
            "[Epoch 3] [Batch 100] [G loss: 1.6558]\n",
            "[Epoch 3] [Batch 200] [G loss: 1.5710]\n",
            "[Epoch 3] [Batch 300] [G loss: 1.3150]\n",
            "[Epoch 3] [Batch 400] [G loss: 1.7428]\n",
            "[Epoch 3] [Batch 500] [G loss: 2.1671]\n",
            "[Epoch 3] [Batch 600] [G loss: 1.6207]\n",
            "[Epoch 3] [Batch 700] [G loss: 1.3952]\n",
            "[Epoch 3] [Batch 800] [G loss: 1.5785]\n",
            "[Epoch 3] [Batch 900] [G loss: 1.3103]\n",
            "[Epoch 3] [Batch 1000] [G loss: 1.8252]\n",
            "[Epoch 3] [Batch 1100] [G loss: 1.3238]\n",
            "[Epoch 3] [Batch 1200] [G loss: 1.4143]\n",
            "[Epoch 3] [Batch 1300] [G loss: 1.4990]\n",
            "[Epoch 3] [Batch 1400] [G loss: 1.0500]\n",
            "[Epoch 3] [Batch 1500] [G loss: 1.3187]\n",
            "[Epoch 3] [Batch 1600] [G loss: 0.8754]\n",
            "[Epoch 3] [Batch 1700] [G loss: 1.3128]\n",
            "[Epoch 3] [Batch 1800] [G loss: 1.7076]\n",
            "[Epoch 3] [Batch 1900] [G loss: 1.3684]\n",
            "[Epoch 3] [Batch 2000] [G loss: 1.0477]\n",
            "[Epoch 3] [Batch 2100] [G loss: 1.3405]\n",
            "[Epoch 3] [Batch 2200] [G loss: 1.4560]\n",
            "[Epoch 3] [Batch 2300] [G loss: 1.0820]\n",
            "[Epoch 3] [Batch 2400] [G loss: 1.5864]\n",
            "[Epoch 3] [Batch 2500] [G loss: 1.4413]\n",
            "[Epoch 3] [Batch 2600] [G loss: 1.2867]\n",
            "[Epoch 3] [Batch 2700] [G loss: 1.5423]\n",
            "[Epoch 3] [Batch 2800] [G loss: 1.3491]\n",
            "[Epoch 3] [Batch 2900] [G loss: 1.2216]\n",
            "[Epoch 3] [Batch 3000] [G loss: 1.0299]\n",
            "[Epoch 3] [Batch 3100] [G loss: 1.3826]\n",
            "[Epoch 3] [Batch 3200] [G loss: 1.5708]\n",
            "[Epoch 3] [Batch 3300] [G loss: 1.4490]\n",
            "[Epoch 3] [Batch 3400] [G loss: 1.6585]\n",
            "[Epoch 3] [Batch 3500] [G loss: 0.9817]\n",
            "[Epoch 3] [Batch 3600] [G loss: 1.4114]\n",
            "[Epoch 3] [Batch 3700] [G loss: 1.0379]\n",
            "[Epoch 3] [Batch 3800] [G loss: 1.4535]\n",
            "[Epoch 3] [Batch 3900] [G loss: 1.0376]\n",
            "[Epoch 3] [Batch 4000] [G loss: 1.3889]\n",
            "[Epoch 3] [Batch 4100] [G loss: 1.4904]\n",
            "[Epoch 3] [Batch 4200] [G loss: 1.3359]\n",
            "[Epoch 3] [Batch 4300] [G loss: 1.7404]\n",
            "[Epoch 3] [Batch 4400] [G loss: 1.4189]\n",
            "[Epoch 3] [Batch 4500] [G loss: 1.1961]\n",
            "[Epoch 3] [Batch 4600] [G loss: 1.1298]\n",
            "[Epoch 3] [Batch 4700] [G loss: 1.3413]\n",
            "[Epoch 3] [Batch 4800] [G loss: 1.5789]\n",
            "[Epoch 3] [Batch 4900] [G loss: 1.0594]\n",
            "[Epoch 3] [Batch 5000] [G loss: 1.6880]\n",
            "[Epoch 3] [Batch 5100] [G loss: 1.4212]\n",
            "[Epoch 3] [Batch 5200] [G loss: 1.2981]\n",
            "[Epoch 3] [Batch 5300] [G loss: 1.9802]\n",
            "[Epoch 3] [Batch 5400] [G loss: 1.3688]\n",
            "[Epoch 3] [Batch 5500] [G loss: 1.4229]\n",
            "[Epoch 3] [Batch 5600] [G loss: 1.1505]\n",
            "[Epoch 3] [Batch 5700] [G loss: 1.3468]\n",
            "[Epoch 3] [Batch 5800] [G loss: 1.1224]\n",
            "[Epoch 3] [Batch 5900] [G loss: 1.5276]\n",
            "[Epoch 3] [Batch 6000] [G loss: 1.6541]\n",
            "[Epoch 3] [Batch 6100] [G loss: 1.6463]\n",
            "[Epoch 3] [Batch 6200] [G loss: 1.0369]\n",
            "[Epoch 3] [Batch 6300] [G loss: 1.2061]\n",
            "[Epoch 3] [Batch 6400] [G loss: 1.7488]\n",
            "[Epoch 3] [Batch 6500] [G loss: 1.5191]\n",
            "[Epoch 3] [Batch 6600] [G loss: 1.4423]\n",
            "[Epoch 3] [Batch 6700] [G loss: 1.4945]\n",
            "[Epoch 3] [Batch 6800] [G loss: 1.4292]\n",
            "[Epoch 3] [Batch 6900] [G loss: 1.2810]\n",
            "[Epoch 3] [Batch 7000] [G loss: 1.3721]\n",
            "[Epoch 4] [Batch 0] [G loss: 1.5154]\n",
            "[Epoch 4] [Batch 100] [G loss: 1.7034]\n",
            "[Epoch 4] [Batch 200] [G loss: 1.6070]\n",
            "[Epoch 4] [Batch 300] [G loss: 1.5267]\n",
            "[Epoch 4] [Batch 400] [G loss: 1.3714]\n",
            "[Epoch 4] [Batch 500] [G loss: 1.2787]\n",
            "[Epoch 4] [Batch 600] [G loss: 1.1691]\n",
            "[Epoch 4] [Batch 700] [G loss: 1.3259]\n",
            "[Epoch 4] [Batch 800] [G loss: 1.2257]\n",
            "[Epoch 4] [Batch 900] [G loss: 1.4633]\n",
            "[Epoch 4] [Batch 1000] [G loss: 1.5992]\n",
            "[Epoch 4] [Batch 1100] [G loss: 1.1866]\n",
            "[Epoch 4] [Batch 1200] [G loss: 1.5633]\n",
            "[Epoch 4] [Batch 1300] [G loss: 1.1004]\n",
            "[Epoch 4] [Batch 1400] [G loss: 1.1471]\n",
            "[Epoch 4] [Batch 1500] [G loss: 1.1738]\n",
            "[Epoch 4] [Batch 1600] [G loss: 1.8417]\n",
            "[Epoch 4] [Batch 1700] [G loss: 1.4032]\n",
            "[Epoch 4] [Batch 1800] [G loss: 1.1127]\n",
            "[Epoch 4] [Batch 1900] [G loss: 1.3640]\n",
            "[Epoch 4] [Batch 2000] [G loss: 1.0248]\n",
            "[Epoch 4] [Batch 2100] [G loss: 1.1056]\n",
            "[Epoch 4] [Batch 2200] [G loss: 1.0506]\n",
            "[Epoch 4] [Batch 2300] [G loss: 0.8256]\n",
            "[Epoch 4] [Batch 2400] [G loss: 1.0865]\n",
            "[Epoch 4] [Batch 2500] [G loss: 1.1978]\n",
            "[Epoch 4] [Batch 2600] [G loss: 1.5670]\n",
            "[Epoch 4] [Batch 2700] [G loss: 1.2609]\n",
            "[Epoch 4] [Batch 2800] [G loss: 0.9769]\n",
            "[Epoch 4] [Batch 2900] [G loss: 1.1753]\n",
            "[Epoch 4] [Batch 3000] [G loss: 1.1546]\n",
            "[Epoch 4] [Batch 3100] [G loss: 1.1730]\n",
            "[Epoch 4] [Batch 3200] [G loss: 1.2940]\n",
            "[Epoch 4] [Batch 3300] [G loss: 1.3213]\n",
            "[Epoch 4] [Batch 3400] [G loss: 1.7556]\n",
            "[Epoch 4] [Batch 3500] [G loss: 1.3683]\n",
            "[Epoch 4] [Batch 3600] [G loss: 1.3058]\n",
            "[Epoch 4] [Batch 3700] [G loss: 1.0596]\n",
            "[Epoch 4] [Batch 3800] [G loss: 1.4423]\n",
            "[Epoch 4] [Batch 3900] [G loss: 1.2855]\n",
            "[Epoch 4] [Batch 4000] [G loss: 1.7836]\n",
            "[Epoch 4] [Batch 4100] [G loss: 1.0734]\n",
            "[Epoch 4] [Batch 4200] [G loss: 1.8166]\n",
            "[Epoch 4] [Batch 4300] [G loss: 1.1376]\n",
            "[Epoch 4] [Batch 4400] [G loss: 1.4780]\n",
            "[Epoch 4] [Batch 4500] [G loss: 1.1273]\n",
            "[Epoch 4] [Batch 4600] [G loss: 1.3948]\n",
            "[Epoch 4] [Batch 4700] [G loss: 1.2543]\n",
            "[Epoch 4] [Batch 4800] [G loss: 1.5902]\n",
            "[Epoch 4] [Batch 4900] [G loss: 1.3753]\n",
            "[Epoch 4] [Batch 5000] [G loss: 1.1196]\n",
            "[Epoch 4] [Batch 5100] [G loss: 1.2993]\n",
            "[Epoch 4] [Batch 5200] [G loss: 1.3750]\n",
            "[Epoch 4] [Batch 5300] [G loss: 1.6216]\n",
            "[Epoch 4] [Batch 5400] [G loss: 1.5471]\n",
            "[Epoch 4] [Batch 5500] [G loss: 1.5721]\n",
            "[Epoch 4] [Batch 5600] [G loss: 1.4728]\n",
            "[Epoch 4] [Batch 5700] [G loss: 1.1499]\n",
            "[Epoch 4] [Batch 5800] [G loss: 1.0879]\n",
            "[Epoch 4] [Batch 5900] [G loss: 1.5912]\n",
            "[Epoch 4] [Batch 6000] [G loss: 1.0645]\n",
            "[Epoch 4] [Batch 6100] [G loss: 1.4371]\n",
            "[Epoch 4] [Batch 6200] [G loss: 1.3749]\n",
            "[Epoch 4] [Batch 6300] [G loss: 1.0071]\n",
            "[Epoch 4] [Batch 6400] [G loss: 1.4679]\n",
            "[Epoch 4] [Batch 6500] [G loss: 1.5203]\n",
            "[Epoch 4] [Batch 6600] [G loss: 1.2437]\n",
            "[Epoch 4] [Batch 6700] [G loss: 1.6381]\n",
            "[Epoch 4] [Batch 6800] [G loss: 1.2035]\n",
            "[Epoch 4] [Batch 6900] [G loss: 1.4329]\n",
            "[Epoch 4] [Batch 7000] [G loss: 1.6456]\n",
            "[Epoch 5] [Batch 0] [G loss: 1.6418]\n",
            "[Epoch 5] [Batch 100] [G loss: 1.4728]\n",
            "[Epoch 5] [Batch 200] [G loss: 1.1011]\n",
            "[Epoch 5] [Batch 300] [G loss: 1.2295]\n",
            "[Epoch 5] [Batch 400] [G loss: 1.5528]\n",
            "[Epoch 5] [Batch 500] [G loss: 1.3490]\n",
            "[Epoch 5] [Batch 600] [G loss: 1.1392]\n",
            "[Epoch 5] [Batch 700] [G loss: 1.7044]\n",
            "[Epoch 5] [Batch 800] [G loss: 1.4280]\n",
            "[Epoch 5] [Batch 900] [G loss: 1.5723]\n",
            "[Epoch 5] [Batch 1000] [G loss: 1.4096]\n",
            "[Epoch 5] [Batch 1100] [G loss: 1.2736]\n",
            "[Epoch 5] [Batch 1200] [G loss: 1.2488]\n",
            "[Epoch 5] [Batch 1300] [G loss: 1.5162]\n",
            "[Epoch 5] [Batch 1400] [G loss: 1.4832]\n",
            "[Epoch 5] [Batch 1500] [G loss: 1.4656]\n",
            "[Epoch 5] [Batch 1600] [G loss: 2.0749]\n",
            "[Epoch 5] [Batch 1700] [G loss: 1.5900]\n",
            "[Epoch 5] [Batch 1800] [G loss: 1.5217]\n",
            "[Epoch 5] [Batch 1900] [G loss: 1.3465]\n",
            "[Epoch 5] [Batch 2000] [G loss: 1.8210]\n",
            "[Epoch 5] [Batch 2100] [G loss: 1.2069]\n",
            "[Epoch 5] [Batch 2200] [G loss: 1.6573]\n",
            "[Epoch 5] [Batch 2300] [G loss: 1.3637]\n",
            "[Epoch 5] [Batch 2400] [G loss: 1.3513]\n",
            "[Epoch 5] [Batch 2500] [G loss: 1.7784]\n",
            "[Epoch 5] [Batch 2600] [G loss: 1.0592]\n",
            "[Epoch 5] [Batch 2700] [G loss: 1.2864]\n",
            "[Epoch 5] [Batch 2800] [G loss: 1.0559]\n",
            "[Epoch 5] [Batch 2900] [G loss: 1.1757]\n",
            "[Epoch 5] [Batch 3000] [G loss: 1.2704]\n",
            "[Epoch 5] [Batch 3100] [G loss: 1.0189]\n",
            "[Epoch 5] [Batch 3200] [G loss: 1.0510]\n",
            "[Epoch 5] [Batch 3300] [G loss: 1.2154]\n",
            "[Epoch 5] [Batch 3400] [G loss: 1.0750]\n",
            "[Epoch 5] [Batch 3500] [G loss: 1.1803]\n",
            "[Epoch 5] [Batch 3600] [G loss: 1.2793]\n",
            "[Epoch 5] [Batch 3700] [G loss: 1.0123]\n",
            "[Epoch 5] [Batch 3800] [G loss: 1.4858]\n",
            "[Epoch 5] [Batch 3900] [G loss: 1.1221]\n",
            "[Epoch 5] [Batch 4000] [G loss: 1.4206]\n",
            "[Epoch 5] [Batch 4100] [G loss: 1.4803]\n",
            "[Epoch 5] [Batch 4200] [G loss: 1.4928]\n",
            "[Epoch 5] [Batch 4300] [G loss: 0.9356]\n",
            "[Epoch 5] [Batch 4400] [G loss: 1.3093]\n",
            "[Epoch 5] [Batch 4500] [G loss: 1.5173]\n",
            "[Epoch 5] [Batch 4600] [G loss: 1.7352]\n",
            "[Epoch 5] [Batch 4700] [G loss: 1.4676]\n",
            "[Epoch 5] [Batch 4800] [G loss: 1.2212]\n",
            "[Epoch 5] [Batch 4900] [G loss: 1.3668]\n",
            "[Epoch 5] [Batch 5000] [G loss: 1.4779]\n",
            "[Epoch 5] [Batch 5100] [G loss: 1.3353]\n",
            "[Epoch 5] [Batch 5200] [G loss: 1.6296]\n",
            "[Epoch 5] [Batch 5300] [G loss: 1.5593]\n",
            "[Epoch 5] [Batch 5400] [G loss: 1.3758]\n",
            "[Epoch 5] [Batch 5500] [G loss: 1.5200]\n",
            "[Epoch 5] [Batch 5600] [G loss: 1.6214]\n",
            "[Epoch 5] [Batch 5700] [G loss: 1.4400]\n",
            "[Epoch 5] [Batch 5800] [G loss: 1.2699]\n",
            "[Epoch 5] [Batch 5900] [G loss: 1.2132]\n",
            "[Epoch 5] [Batch 6000] [G loss: 1.1669]\n",
            "[Epoch 5] [Batch 6100] [G loss: 1.2608]\n",
            "[Epoch 5] [Batch 6200] [G loss: 0.7887]\n",
            "[Epoch 5] [Batch 6300] [G loss: 1.7010]\n",
            "[Epoch 5] [Batch 6400] [G loss: 1.3196]\n",
            "[Epoch 5] [Batch 6500] [G loss: 1.5052]\n",
            "[Epoch 5] [Batch 6600] [G loss: 1.1965]\n",
            "[Epoch 5] [Batch 6700] [G loss: 1.4888]\n",
            "[Epoch 5] [Batch 6800] [G loss: 1.4782]\n",
            "[Epoch 5] [Batch 6900] [G loss: 1.3992]\n",
            "[Epoch 5] [Batch 7000] [G loss: 1.2658]\n",
            "üíæ UNet Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints_unet/epoch_5.pth\n",
            "[Epoch 6] [Batch 0] [G loss: 0.9406]\n",
            "[Epoch 6] [Batch 100] [G loss: 1.4572]\n",
            "[Epoch 6] [Batch 200] [G loss: 1.2179]\n",
            "[Epoch 6] [Batch 300] [G loss: 1.5458]\n",
            "[Epoch 6] [Batch 400] [G loss: 1.2393]\n",
            "[Epoch 6] [Batch 500] [G loss: 1.5607]\n",
            "[Epoch 6] [Batch 600] [G loss: 1.9261]\n",
            "[Epoch 6] [Batch 700] [G loss: 1.4433]\n",
            "[Epoch 6] [Batch 800] [G loss: 1.5434]\n",
            "[Epoch 6] [Batch 900] [G loss: 1.4513]\n",
            "[Epoch 6] [Batch 1000] [G loss: 1.7693]\n",
            "[Epoch 6] [Batch 1100] [G loss: 1.2831]\n",
            "[Epoch 6] [Batch 1200] [G loss: 1.5065]\n",
            "[Epoch 6] [Batch 1300] [G loss: 1.8331]\n",
            "[Epoch 6] [Batch 1400] [G loss: 1.4872]\n",
            "[Epoch 6] [Batch 1500] [G loss: 1.3000]\n",
            "[Epoch 6] [Batch 1600] [G loss: 1.1036]\n",
            "[Epoch 6] [Batch 1700] [G loss: 1.4458]\n",
            "[Epoch 6] [Batch 1800] [G loss: 1.3621]\n",
            "[Epoch 6] [Batch 1900] [G loss: 1.7549]\n",
            "[Epoch 6] [Batch 2000] [G loss: 1.4714]\n",
            "[Epoch 6] [Batch 2100] [G loss: 1.1440]\n",
            "[Epoch 6] [Batch 2200] [G loss: 1.4484]\n",
            "[Epoch 6] [Batch 2300] [G loss: 1.4661]\n",
            "[Epoch 6] [Batch 2400] [G loss: 1.1631]\n",
            "[Epoch 6] [Batch 2500] [G loss: 2.0133]\n",
            "[Epoch 6] [Batch 2600] [G loss: 1.5404]\n",
            "[Epoch 6] [Batch 2700] [G loss: 1.3530]\n",
            "[Epoch 6] [Batch 2800] [G loss: 1.6161]\n",
            "[Epoch 6] [Batch 2900] [G loss: 1.3083]\n",
            "[Epoch 6] [Batch 3000] [G loss: 1.2845]\n",
            "[Epoch 6] [Batch 3100] [G loss: 1.1513]\n",
            "[Epoch 6] [Batch 3200] [G loss: 0.8348]\n",
            "[Epoch 6] [Batch 3300] [G loss: 1.8821]\n",
            "[Epoch 6] [Batch 3400] [G loss: 1.0897]\n",
            "[Epoch 6] [Batch 3500] [G loss: 1.1014]\n",
            "[Epoch 6] [Batch 3600] [G loss: 1.3650]\n",
            "[Epoch 6] [Batch 3700] [G loss: 1.4376]\n",
            "[Epoch 6] [Batch 3800] [G loss: 1.3814]\n",
            "[Epoch 6] [Batch 3900] [G loss: 1.3353]\n",
            "[Epoch 6] [Batch 4000] [G loss: 1.4357]\n",
            "[Epoch 6] [Batch 4100] [G loss: 1.6156]\n",
            "[Epoch 6] [Batch 4200] [G loss: 1.2718]\n",
            "[Epoch 6] [Batch 4300] [G loss: 1.4750]\n",
            "[Epoch 6] [Batch 4400] [G loss: 1.3185]\n",
            "[Epoch 6] [Batch 4500] [G loss: 1.4446]\n",
            "[Epoch 6] [Batch 4600] [G loss: 1.6191]\n",
            "[Epoch 6] [Batch 4700] [G loss: 1.7014]\n",
            "[Epoch 6] [Batch 4800] [G loss: 1.4206]\n",
            "[Epoch 6] [Batch 4900] [G loss: 1.6011]\n",
            "[Epoch 6] [Batch 5000] [G loss: 1.7922]\n",
            "[Epoch 6] [Batch 5100] [G loss: 1.5594]\n",
            "[Epoch 6] [Batch 5200] [G loss: 1.5202]\n",
            "[Epoch 6] [Batch 5300] [G loss: 1.1385]\n",
            "[Epoch 6] [Batch 5400] [G loss: 1.3145]\n",
            "[Epoch 6] [Batch 5500] [G loss: 1.3545]\n",
            "[Epoch 6] [Batch 5600] [G loss: 1.4872]\n",
            "[Epoch 6] [Batch 5700] [G loss: 1.3328]\n",
            "[Epoch 6] [Batch 5800] [G loss: 1.3310]\n",
            "[Epoch 6] [Batch 5900] [G loss: 1.6196]\n",
            "[Epoch 6] [Batch 6000] [G loss: 1.4079]\n",
            "[Epoch 6] [Batch 6100] [G loss: 1.5693]\n",
            "[Epoch 6] [Batch 6200] [G loss: 1.6425]\n",
            "[Epoch 6] [Batch 6300] [G loss: 1.3158]\n",
            "[Epoch 6] [Batch 6400] [G loss: 1.4815]\n",
            "[Epoch 6] [Batch 6500] [G loss: 1.1378]\n",
            "[Epoch 6] [Batch 6600] [G loss: 1.7702]\n",
            "[Epoch 6] [Batch 6700] [G loss: 1.7179]\n",
            "[Epoch 6] [Batch 6800] [G loss: 1.3153]\n",
            "[Epoch 6] [Batch 6900] [G loss: 1.6336]\n",
            "[Epoch 6] [Batch 7000] [G loss: 1.3831]\n",
            "[Epoch 7] [Batch 0] [G loss: 1.3816]\n",
            "[Epoch 7] [Batch 100] [G loss: 1.7542]\n",
            "[Epoch 7] [Batch 200] [G loss: 1.3271]\n",
            "[Epoch 7] [Batch 300] [G loss: 1.7289]\n",
            "[Epoch 7] [Batch 400] [G loss: 1.4013]\n",
            "[Epoch 7] [Batch 500] [G loss: 0.9524]\n",
            "[Epoch 7] [Batch 600] [G loss: 1.4317]\n",
            "[Epoch 7] [Batch 700] [G loss: 1.6027]\n",
            "[Epoch 7] [Batch 800] [G loss: 1.5627]\n",
            "[Epoch 7] [Batch 900] [G loss: 1.5367]\n",
            "[Epoch 7] [Batch 1000] [G loss: 1.5000]\n",
            "[Epoch 7] [Batch 1100] [G loss: 1.3898]\n",
            "[Epoch 7] [Batch 1200] [G loss: 1.2753]\n",
            "[Epoch 7] [Batch 1300] [G loss: 1.4683]\n",
            "[Epoch 7] [Batch 1400] [G loss: 1.3918]\n",
            "[Epoch 7] [Batch 1500] [G loss: 1.5432]\n",
            "[Epoch 7] [Batch 1600] [G loss: 1.2105]\n",
            "[Epoch 7] [Batch 1700] [G loss: 0.7568]\n",
            "[Epoch 7] [Batch 1800] [G loss: 1.4282]\n",
            "[Epoch 7] [Batch 1900] [G loss: 1.4899]\n",
            "[Epoch 7] [Batch 2000] [G loss: 1.5107]\n",
            "[Epoch 7] [Batch 2100] [G loss: 1.0901]\n",
            "[Epoch 7] [Batch 2200] [G loss: 1.3260]\n",
            "[Epoch 7] [Batch 2300] [G loss: 1.4053]\n",
            "[Epoch 7] [Batch 2400] [G loss: 1.3471]\n",
            "[Epoch 7] [Batch 2500] [G loss: 1.2959]\n",
            "[Epoch 7] [Batch 2600] [G loss: 1.7487]\n",
            "[Epoch 7] [Batch 2700] [G loss: 1.3494]\n",
            "[Epoch 7] [Batch 2800] [G loss: 1.9632]\n",
            "[Epoch 7] [Batch 2900] [G loss: 1.6071]\n",
            "[Epoch 7] [Batch 3000] [G loss: 1.4240]\n",
            "[Epoch 7] [Batch 3100] [G loss: 1.3495]\n",
            "[Epoch 7] [Batch 3200] [G loss: 1.7367]\n",
            "[Epoch 7] [Batch 3300] [G loss: 1.4736]\n",
            "[Epoch 7] [Batch 3400] [G loss: 1.3122]\n",
            "[Epoch 7] [Batch 3500] [G loss: 1.3160]\n",
            "[Epoch 7] [Batch 3600] [G loss: 1.8759]\n",
            "[Epoch 7] [Batch 3700] [G loss: 1.7964]\n",
            "[Epoch 7] [Batch 3800] [G loss: 1.4549]\n",
            "[Epoch 7] [Batch 3900] [G loss: 1.4933]\n",
            "[Epoch 7] [Batch 4000] [G loss: 1.4398]\n",
            "[Epoch 7] [Batch 4100] [G loss: 1.4057]\n",
            "[Epoch 7] [Batch 4200] [G loss: 1.6261]\n",
            "[Epoch 7] [Batch 4300] [G loss: 1.3569]\n",
            "[Epoch 7] [Batch 4400] [G loss: 1.3820]\n",
            "[Epoch 7] [Batch 4500] [G loss: 1.4791]\n",
            "[Epoch 7] [Batch 4600] [G loss: 1.5961]\n",
            "[Epoch 7] [Batch 4700] [G loss: 1.3121]\n",
            "[Epoch 7] [Batch 4800] [G loss: 1.5048]\n",
            "[Epoch 7] [Batch 4900] [G loss: 1.0681]\n",
            "[Epoch 7] [Batch 5000] [G loss: 1.7031]\n",
            "[Epoch 7] [Batch 5100] [G loss: 1.4328]\n",
            "[Epoch 7] [Batch 5200] [G loss: 1.2225]\n",
            "[Epoch 7] [Batch 5300] [G loss: 1.4351]\n",
            "[Epoch 7] [Batch 5400] [G loss: 1.3447]\n",
            "[Epoch 7] [Batch 5500] [G loss: 1.0752]\n",
            "[Epoch 7] [Batch 5600] [G loss: 1.6612]\n",
            "[Epoch 7] [Batch 5700] [G loss: 1.6167]\n",
            "[Epoch 7] [Batch 5800] [G loss: 1.5387]\n",
            "[Epoch 7] [Batch 5900] [G loss: 1.4176]\n",
            "[Epoch 7] [Batch 6000] [G loss: 1.3173]\n",
            "[Epoch 7] [Batch 6100] [G loss: 1.3491]\n",
            "[Epoch 7] [Batch 6200] [G loss: 1.2035]\n",
            "[Epoch 7] [Batch 6300] [G loss: 1.7663]\n",
            "[Epoch 7] [Batch 6400] [G loss: 1.3521]\n",
            "[Epoch 7] [Batch 6500] [G loss: 1.4003]\n",
            "[Epoch 7] [Batch 6600] [G loss: 1.7905]\n",
            "[Epoch 7] [Batch 6700] [G loss: 1.7368]\n",
            "[Epoch 7] [Batch 6800] [G loss: 1.3903]\n",
            "[Epoch 7] [Batch 6900] [G loss: 1.1633]\n",
            "[Epoch 7] [Batch 7000] [G loss: 1.4624]\n",
            "[Epoch 8] [Batch 0] [G loss: 1.6083]\n",
            "[Epoch 8] [Batch 100] [G loss: 1.2467]\n",
            "[Epoch 8] [Batch 200] [G loss: 1.4503]\n",
            "[Epoch 8] [Batch 300] [G loss: 1.2997]\n",
            "[Epoch 8] [Batch 400] [G loss: 1.7688]\n",
            "[Epoch 8] [Batch 500] [G loss: 1.1375]\n",
            "[Epoch 8] [Batch 600] [G loss: 1.2479]\n",
            "[Epoch 8] [Batch 700] [G loss: 2.4198]\n",
            "[Epoch 8] [Batch 800] [G loss: 1.4852]\n",
            "[Epoch 8] [Batch 900] [G loss: 1.2949]\n",
            "[Epoch 8] [Batch 1000] [G loss: 1.4902]\n",
            "[Epoch 8] [Batch 1100] [G loss: 1.4800]\n",
            "[Epoch 8] [Batch 1200] [G loss: 1.5056]\n",
            "[Epoch 8] [Batch 1300] [G loss: 1.2893]\n",
            "[Epoch 8] [Batch 1400] [G loss: 1.1876]\n",
            "[Epoch 8] [Batch 1500] [G loss: 1.4953]\n",
            "[Epoch 8] [Batch 1600] [G loss: 1.2236]\n",
            "[Epoch 8] [Batch 1700] [G loss: 1.4659]\n",
            "[Epoch 8] [Batch 1800] [G loss: 1.4097]\n",
            "[Epoch 8] [Batch 1900] [G loss: 1.3222]\n",
            "[Epoch 8] [Batch 2000] [G loss: 1.0381]\n",
            "[Epoch 8] [Batch 2100] [G loss: 1.1972]\n",
            "[Epoch 8] [Batch 2200] [G loss: 1.4042]\n",
            "[Epoch 8] [Batch 2300] [G loss: 1.2374]\n",
            "[Epoch 8] [Batch 2400] [G loss: 1.1150]\n",
            "[Epoch 8] [Batch 2500] [G loss: 1.0671]\n",
            "[Epoch 8] [Batch 2600] [G loss: 1.7068]\n",
            "[Epoch 8] [Batch 2700] [G loss: 1.4292]\n",
            "[Epoch 8] [Batch 2800] [G loss: 1.1176]\n",
            "[Epoch 8] [Batch 2900] [G loss: 1.3255]\n",
            "[Epoch 8] [Batch 3000] [G loss: 1.4340]\n",
            "[Epoch 8] [Batch 3100] [G loss: 1.4975]\n",
            "[Epoch 8] [Batch 3200] [G loss: 1.5165]\n",
            "[Epoch 8] [Batch 3300] [G loss: 1.3741]\n",
            "[Epoch 8] [Batch 3400] [G loss: 1.4892]\n",
            "[Epoch 8] [Batch 3500] [G loss: 1.4758]\n",
            "[Epoch 8] [Batch 3600] [G loss: 1.8756]\n",
            "[Epoch 8] [Batch 3700] [G loss: 1.3835]\n",
            "[Epoch 8] [Batch 3800] [G loss: 1.5287]\n",
            "[Epoch 8] [Batch 3900] [G loss: 1.3003]\n",
            "[Epoch 8] [Batch 4000] [G loss: 1.5243]\n",
            "[Epoch 8] [Batch 4100] [G loss: 1.4696]\n",
            "[Epoch 8] [Batch 4200] [G loss: 1.5646]\n",
            "[Epoch 8] [Batch 4300] [G loss: 1.4995]\n",
            "[Epoch 8] [Batch 4400] [G loss: 1.6145]\n",
            "[Epoch 8] [Batch 4500] [G loss: 1.3005]\n",
            "[Epoch 8] [Batch 4600] [G loss: 1.3088]\n",
            "[Epoch 8] [Batch 4700] [G loss: 1.4014]\n",
            "[Epoch 8] [Batch 4800] [G loss: 1.5193]\n",
            "[Epoch 8] [Batch 4900] [G loss: 1.7412]\n",
            "[Epoch 8] [Batch 5000] [G loss: 1.3788]\n",
            "[Epoch 8] [Batch 5100] [G loss: 1.3623]\n",
            "[Epoch 8] [Batch 5200] [G loss: 1.5880]\n",
            "[Epoch 8] [Batch 5300] [G loss: 1.3560]\n",
            "[Epoch 8] [Batch 5400] [G loss: 1.3145]\n",
            "[Epoch 8] [Batch 5500] [G loss: 1.2415]\n",
            "[Epoch 8] [Batch 5600] [G loss: 1.4671]\n",
            "[Epoch 8] [Batch 5700] [G loss: 1.2882]\n",
            "[Epoch 8] [Batch 5800] [G loss: 1.2682]\n",
            "[Epoch 8] [Batch 5900] [G loss: 1.7599]\n",
            "[Epoch 8] [Batch 6000] [G loss: 1.4975]\n",
            "[Epoch 8] [Batch 6100] [G loss: 1.7511]\n",
            "[Epoch 8] [Batch 6200] [G loss: 1.2136]\n",
            "[Epoch 8] [Batch 6300] [G loss: 1.9039]\n",
            "[Epoch 8] [Batch 6400] [G loss: 1.3241]\n",
            "[Epoch 8] [Batch 6500] [G loss: 1.5187]\n",
            "[Epoch 8] [Batch 6600] [G loss: 1.5133]\n",
            "[Epoch 8] [Batch 6700] [G loss: 1.7008]\n",
            "[Epoch 8] [Batch 6800] [G loss: 1.2551]\n",
            "[Epoch 8] [Batch 6900] [G loss: 1.4710]\n",
            "[Epoch 8] [Batch 7000] [G loss: 1.8940]\n",
            "[Epoch 9] [Batch 0] [G loss: 1.3680]\n",
            "[Epoch 9] [Batch 100] [G loss: 1.3440]\n",
            "[Epoch 9] [Batch 200] [G loss: 1.5547]\n",
            "[Epoch 9] [Batch 300] [G loss: 1.4409]\n",
            "[Epoch 9] [Batch 400] [G loss: 1.9297]\n",
            "[Epoch 9] [Batch 500] [G loss: 1.8648]\n",
            "[Epoch 9] [Batch 600] [G loss: 1.1811]\n",
            "[Epoch 9] [Batch 700] [G loss: 1.6552]\n",
            "[Epoch 9] [Batch 800] [G loss: 1.4701]\n",
            "[Epoch 9] [Batch 900] [G loss: 1.4432]\n",
            "[Epoch 9] [Batch 1000] [G loss: 1.6255]\n",
            "[Epoch 9] [Batch 1100] [G loss: 1.7690]\n",
            "[Epoch 9] [Batch 1200] [G loss: 1.5463]\n",
            "[Epoch 9] [Batch 1300] [G loss: 1.4964]\n",
            "[Epoch 9] [Batch 1400] [G loss: 1.6212]\n",
            "[Epoch 9] [Batch 1500] [G loss: 1.3862]\n",
            "[Epoch 9] [Batch 1600] [G loss: 1.9013]\n",
            "[Epoch 9] [Batch 1700] [G loss: 1.4098]\n",
            "[Epoch 9] [Batch 1800] [G loss: 1.3392]\n",
            "[Epoch 9] [Batch 1900] [G loss: 1.6854]\n",
            "[Epoch 9] [Batch 2000] [G loss: 1.5196]\n",
            "[Epoch 9] [Batch 2100] [G loss: 1.4890]\n",
            "[Epoch 9] [Batch 2200] [G loss: 1.5700]\n",
            "[Epoch 9] [Batch 2300] [G loss: 1.3013]\n",
            "[Epoch 9] [Batch 2400] [G loss: 1.2738]\n",
            "[Epoch 9] [Batch 2500] [G loss: 1.4251]\n",
            "[Epoch 9] [Batch 2600] [G loss: 1.5458]\n",
            "[Epoch 9] [Batch 2700] [G loss: 1.3906]\n",
            "[Epoch 9] [Batch 2800] [G loss: 1.5251]\n",
            "[Epoch 9] [Batch 2900] [G loss: 1.2295]\n",
            "[Epoch 9] [Batch 3000] [G loss: 0.8744]\n",
            "[Epoch 9] [Batch 3100] [G loss: 1.6437]\n",
            "[Epoch 9] [Batch 3200] [G loss: 1.0015]\n",
            "[Epoch 9] [Batch 3300] [G loss: 1.4720]\n",
            "[Epoch 9] [Batch 3400] [G loss: 1.2446]\n",
            "[Epoch 9] [Batch 3500] [G loss: 1.2865]\n",
            "[Epoch 9] [Batch 3600] [G loss: 1.2833]\n",
            "[Epoch 9] [Batch 3700] [G loss: 1.2661]\n",
            "[Epoch 9] [Batch 3800] [G loss: 1.2701]\n",
            "[Epoch 9] [Batch 3900] [G loss: 1.4884]\n",
            "[Epoch 9] [Batch 4000] [G loss: 1.3722]\n",
            "[Epoch 9] [Batch 4100] [G loss: 1.2412]\n",
            "[Epoch 9] [Batch 4200] [G loss: 1.5502]\n",
            "[Epoch 9] [Batch 4300] [G loss: 1.7958]\n",
            "[Epoch 9] [Batch 4400] [G loss: 1.1264]\n",
            "[Epoch 9] [Batch 4500] [G loss: 1.3765]\n",
            "[Epoch 9] [Batch 4600] [G loss: 1.2557]\n",
            "[Epoch 9] [Batch 4700] [G loss: 1.6630]\n",
            "[Epoch 9] [Batch 4800] [G loss: 1.5665]\n",
            "[Epoch 9] [Batch 4900] [G loss: 1.3133]\n",
            "[Epoch 9] [Batch 5000] [G loss: 1.3607]\n",
            "[Epoch 9] [Batch 5100] [G loss: 1.5920]\n",
            "[Epoch 9] [Batch 5200] [G loss: 1.5545]\n",
            "[Epoch 9] [Batch 5300] [G loss: 1.6419]\n",
            "[Epoch 9] [Batch 5400] [G loss: 1.3394]\n",
            "[Epoch 9] [Batch 5500] [G loss: 1.5682]\n",
            "[Epoch 9] [Batch 5600] [G loss: 1.3359]\n",
            "[Epoch 9] [Batch 5700] [G loss: 1.3793]\n",
            "[Epoch 9] [Batch 5800] [G loss: 1.5971]\n",
            "[Epoch 9] [Batch 5900] [G loss: 1.6974]\n",
            "[Epoch 9] [Batch 6000] [G loss: 2.1227]\n",
            "[Epoch 9] [Batch 6100] [G loss: 1.5154]\n",
            "[Epoch 9] [Batch 6200] [G loss: 1.2206]\n",
            "[Epoch 9] [Batch 6300] [G loss: 1.8432]\n",
            "[Epoch 9] [Batch 6400] [G loss: 1.2981]\n",
            "[Epoch 9] [Batch 6500] [G loss: 1.5471]\n",
            "[Epoch 9] [Batch 6600] [G loss: 1.2865]\n",
            "[Epoch 9] [Batch 6700] [G loss: 1.2400]\n",
            "[Epoch 9] [Batch 6800] [G loss: 1.3261]\n",
            "[Epoch 9] [Batch 6900] [G loss: 1.2358]\n",
            "[Epoch 9] [Batch 7000] [G loss: 1.4724]\n",
            "[Epoch 10] [Batch 0] [G loss: 1.2605]\n",
            "[Epoch 10] [Batch 100] [G loss: 1.8991]\n",
            "[Epoch 10] [Batch 200] [G loss: 1.5071]\n",
            "[Epoch 10] [Batch 300] [G loss: 1.5138]\n",
            "[Epoch 10] [Batch 400] [G loss: 1.4058]\n",
            "[Epoch 10] [Batch 500] [G loss: 1.3265]\n",
            "[Epoch 10] [Batch 600] [G loss: 1.1682]\n",
            "[Epoch 10] [Batch 700] [G loss: 1.2430]\n",
            "[Epoch 10] [Batch 800] [G loss: 1.6130]\n",
            "[Epoch 10] [Batch 900] [G loss: 1.4875]\n",
            "[Epoch 10] [Batch 1000] [G loss: 1.4822]\n",
            "[Epoch 10] [Batch 1100] [G loss: 1.3012]\n",
            "[Epoch 10] [Batch 1200] [G loss: 1.4423]\n",
            "[Epoch 10] [Batch 1300] [G loss: 1.4677]\n",
            "[Epoch 10] [Batch 1400] [G loss: 1.6328]\n",
            "[Epoch 10] [Batch 1500] [G loss: 1.1612]\n",
            "[Epoch 10] [Batch 1600] [G loss: 1.8257]\n",
            "[Epoch 10] [Batch 1700] [G loss: 1.8539]\n",
            "[Epoch 10] [Batch 1800] [G loss: 1.3154]\n",
            "[Epoch 10] [Batch 1900] [G loss: 1.4544]\n",
            "[Epoch 10] [Batch 2000] [G loss: 1.4867]\n",
            "[Epoch 10] [Batch 2100] [G loss: 1.2420]\n",
            "[Epoch 10] [Batch 2200] [G loss: 1.3424]\n",
            "[Epoch 10] [Batch 2300] [G loss: 1.2439]\n",
            "[Epoch 10] [Batch 2400] [G loss: 1.5259]\n",
            "[Epoch 10] [Batch 2500] [G loss: 1.3853]\n",
            "[Epoch 10] [Batch 2600] [G loss: 1.4325]\n",
            "[Epoch 10] [Batch 2700] [G loss: 1.4510]\n",
            "[Epoch 10] [Batch 2800] [G loss: 1.3459]\n",
            "[Epoch 10] [Batch 2900] [G loss: 1.5928]\n",
            "[Epoch 10] [Batch 3000] [G loss: 1.3654]\n",
            "[Epoch 10] [Batch 3100] [G loss: 1.4842]\n",
            "[Epoch 10] [Batch 3200] [G loss: 1.4621]\n",
            "[Epoch 10] [Batch 3300] [G loss: 1.2862]\n",
            "[Epoch 10] [Batch 3400] [G loss: 1.4940]\n",
            "[Epoch 10] [Batch 3500] [G loss: 1.7616]\n",
            "[Epoch 10] [Batch 3600] [G loss: 1.4048]\n",
            "[Epoch 10] [Batch 3700] [G loss: 1.7678]\n",
            "[Epoch 10] [Batch 3800] [G loss: 1.1637]\n",
            "[Epoch 10] [Batch 3900] [G loss: 1.3570]\n",
            "[Epoch 10] [Batch 4000] [G loss: 1.0945]\n",
            "[Epoch 10] [Batch 4100] [G loss: 1.4556]\n",
            "[Epoch 10] [Batch 4200] [G loss: 1.4045]\n",
            "[Epoch 10] [Batch 4300] [G loss: 1.5681]\n",
            "[Epoch 10] [Batch 4400] [G loss: 1.3107]\n",
            "[Epoch 10] [Batch 4500] [G loss: 1.3366]\n",
            "[Epoch 10] [Batch 4600] [G loss: 1.4306]\n",
            "[Epoch 10] [Batch 4700] [G loss: 1.1432]\n",
            "[Epoch 10] [Batch 4800] [G loss: 1.6087]\n",
            "[Epoch 10] [Batch 4900] [G loss: 1.1041]\n",
            "[Epoch 10] [Batch 5000] [G loss: 1.4503]\n",
            "[Epoch 10] [Batch 5100] [G loss: 1.4608]\n",
            "[Epoch 10] [Batch 5200] [G loss: 1.5095]\n",
            "[Epoch 10] [Batch 5300] [G loss: 1.5541]\n",
            "[Epoch 10] [Batch 5400] [G loss: 1.2467]\n",
            "[Epoch 10] [Batch 5500] [G loss: 1.3453]\n",
            "[Epoch 10] [Batch 5600] [G loss: 1.1355]\n",
            "[Epoch 10] [Batch 5700] [G loss: 1.4384]\n",
            "[Epoch 10] [Batch 5800] [G loss: 1.5123]\n",
            "[Epoch 10] [Batch 5900] [G loss: 1.4603]\n",
            "[Epoch 10] [Batch 6000] [G loss: 1.2314]\n",
            "[Epoch 10] [Batch 6100] [G loss: 1.3442]\n",
            "[Epoch 10] [Batch 6200] [G loss: 1.3663]\n",
            "[Epoch 10] [Batch 6300] [G loss: 1.2218]\n",
            "[Epoch 10] [Batch 6400] [G loss: 1.2477]\n",
            "[Epoch 10] [Batch 6500] [G loss: 1.2554]\n",
            "[Epoch 10] [Batch 6600] [G loss: 1.2938]\n",
            "[Epoch 10] [Batch 6700] [G loss: 1.2847]\n",
            "[Epoch 10] [Batch 6800] [G loss: 1.4531]\n",
            "[Epoch 10] [Batch 6900] [G loss: 1.7422]\n",
            "[Epoch 10] [Batch 7000] [G loss: 1.3464]\n",
            "üíæ UNet Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints_unet/epoch_10.pth\n",
            "[Epoch 11] [Batch 0] [G loss: 1.6272]\n",
            "[Epoch 11] [Batch 100] [G loss: 1.5544]\n",
            "[Epoch 11] [Batch 200] [G loss: 1.5738]\n",
            "[Epoch 11] [Batch 300] [G loss: 1.1376]\n",
            "[Epoch 11] [Batch 400] [G loss: 1.6867]\n",
            "[Epoch 11] [Batch 500] [G loss: 1.7974]\n",
            "[Epoch 11] [Batch 600] [G loss: 1.4931]\n",
            "[Epoch 11] [Batch 700] [G loss: 1.4904]\n",
            "[Epoch 11] [Batch 800] [G loss: 1.2990]\n",
            "[Epoch 11] [Batch 900] [G loss: 1.0490]\n",
            "[Epoch 11] [Batch 1000] [G loss: 1.6556]\n",
            "[Epoch 11] [Batch 1100] [G loss: 1.2968]\n",
            "[Epoch 11] [Batch 1200] [G loss: 1.1997]\n",
            "[Epoch 11] [Batch 1300] [G loss: 1.5428]\n",
            "[Epoch 11] [Batch 1400] [G loss: 1.2515]\n",
            "[Epoch 11] [Batch 1500] [G loss: 1.4789]\n",
            "[Epoch 11] [Batch 1600] [G loss: 1.4798]\n",
            "[Epoch 11] [Batch 1700] [G loss: 1.2971]\n",
            "[Epoch 11] [Batch 1800] [G loss: 1.3168]\n",
            "[Epoch 11] [Batch 1900] [G loss: 1.6324]\n",
            "[Epoch 11] [Batch 2000] [G loss: 1.3558]\n",
            "[Epoch 11] [Batch 2100] [G loss: 1.6432]\n",
            "[Epoch 11] [Batch 2200] [G loss: 1.4836]\n",
            "[Epoch 11] [Batch 2300] [G loss: 1.5803]\n",
            "[Epoch 11] [Batch 2400] [G loss: 1.7175]\n",
            "[Epoch 11] [Batch 2500] [G loss: 1.3897]\n",
            "[Epoch 11] [Batch 2600] [G loss: 1.3770]\n",
            "[Epoch 11] [Batch 2700] [G loss: 1.5815]\n",
            "[Epoch 11] [Batch 2800] [G loss: 1.2643]\n",
            "[Epoch 11] [Batch 2900] [G loss: 1.4692]\n",
            "[Epoch 11] [Batch 3000] [G loss: 1.6574]\n",
            "[Epoch 11] [Batch 3100] [G loss: 1.3204]\n",
            "[Epoch 11] [Batch 3200] [G loss: 1.3235]\n",
            "[Epoch 11] [Batch 3300] [G loss: 1.3682]\n",
            "[Epoch 11] [Batch 3400] [G loss: 1.1269]\n",
            "[Epoch 11] [Batch 3500] [G loss: 1.4713]\n",
            "[Epoch 11] [Batch 3600] [G loss: 1.3243]\n",
            "[Epoch 11] [Batch 3700] [G loss: 1.5273]\n",
            "[Epoch 11] [Batch 3800] [G loss: 1.8242]\n",
            "[Epoch 11] [Batch 3900] [G loss: 1.1910]\n",
            "[Epoch 11] [Batch 4000] [G loss: 1.4860]\n",
            "[Epoch 11] [Batch 4100] [G loss: 1.1313]\n",
            "[Epoch 11] [Batch 4200] [G loss: 1.4612]\n",
            "[Epoch 11] [Batch 4300] [G loss: 1.1028]\n",
            "[Epoch 11] [Batch 4400] [G loss: 1.0566]\n",
            "[Epoch 11] [Batch 4500] [G loss: 1.4183]\n",
            "[Epoch 11] [Batch 4600] [G loss: 1.3621]\n",
            "[Epoch 11] [Batch 4700] [G loss: 1.4114]\n",
            "[Epoch 11] [Batch 4800] [G loss: 1.4142]\n",
            "[Epoch 11] [Batch 4900] [G loss: 1.5604]\n",
            "[Epoch 11] [Batch 5000] [G loss: 1.8377]\n",
            "[Epoch 11] [Batch 5100] [G loss: 0.9362]\n",
            "[Epoch 11] [Batch 5200] [G loss: 1.4408]\n",
            "[Epoch 11] [Batch 5300] [G loss: 1.3218]\n",
            "[Epoch 11] [Batch 5400] [G loss: 1.6108]\n",
            "[Epoch 11] [Batch 5500] [G loss: 1.6860]\n",
            "[Epoch 11] [Batch 5600] [G loss: 1.6771]\n",
            "[Epoch 11] [Batch 5700] [G loss: 1.0618]\n",
            "[Epoch 11] [Batch 5800] [G loss: 1.4096]\n",
            "[Epoch 11] [Batch 5900] [G loss: 1.4043]\n",
            "[Epoch 11] [Batch 6000] [G loss: 1.4886]\n",
            "[Epoch 11] [Batch 6100] [G loss: 1.3890]\n",
            "[Epoch 11] [Batch 6200] [G loss: 1.7523]\n",
            "[Epoch 11] [Batch 6300] [G loss: 1.3764]\n",
            "[Epoch 11] [Batch 6400] [G loss: 1.6051]\n",
            "[Epoch 11] [Batch 6500] [G loss: 1.4439]\n",
            "[Epoch 11] [Batch 6600] [G loss: 1.3833]\n",
            "[Epoch 11] [Batch 6700] [G loss: 1.7405]\n",
            "[Epoch 11] [Batch 6800] [G loss: 1.4593]\n",
            "[Epoch 11] [Batch 6900] [G loss: 1.5692]\n",
            "[Epoch 11] [Batch 7000] [G loss: 2.1421]\n",
            "[Epoch 12] [Batch 0] [G loss: 1.4525]\n",
            "[Epoch 12] [Batch 100] [G loss: 1.3177]\n",
            "[Epoch 12] [Batch 200] [G loss: 1.4914]\n",
            "[Epoch 12] [Batch 300] [G loss: 1.5570]\n",
            "[Epoch 12] [Batch 400] [G loss: 1.2767]\n",
            "[Epoch 12] [Batch 500] [G loss: 1.3396]\n",
            "[Epoch 12] [Batch 600] [G loss: 1.2958]\n",
            "[Epoch 12] [Batch 700] [G loss: 1.6951]\n",
            "[Epoch 12] [Batch 800] [G loss: 1.5975]\n",
            "[Epoch 12] [Batch 900] [G loss: 1.3414]\n",
            "[Epoch 12] [Batch 1000] [G loss: 1.5382]\n",
            "[Epoch 12] [Batch 1100] [G loss: 1.3588]\n",
            "[Epoch 12] [Batch 1200] [G loss: 1.3056]\n",
            "[Epoch 12] [Batch 1300] [G loss: 1.5740]\n",
            "[Epoch 12] [Batch 1400] [G loss: 1.2537]\n",
            "[Epoch 12] [Batch 1500] [G loss: 1.2928]\n",
            "[Epoch 12] [Batch 1600] [G loss: 1.3657]\n",
            "[Epoch 12] [Batch 1700] [G loss: 1.1555]\n",
            "[Epoch 12] [Batch 1800] [G loss: 1.3418]\n",
            "[Epoch 12] [Batch 1900] [G loss: 1.5384]\n",
            "[Epoch 12] [Batch 2000] [G loss: 1.5301]\n",
            "[Epoch 12] [Batch 2100] [G loss: 1.7379]\n",
            "[Epoch 12] [Batch 2200] [G loss: 1.8213]\n",
            "[Epoch 12] [Batch 2300] [G loss: 1.6222]\n",
            "[Epoch 12] [Batch 2400] [G loss: 1.3090]\n",
            "[Epoch 12] [Batch 2500] [G loss: 1.4000]\n",
            "[Epoch 12] [Batch 2600] [G loss: 1.2685]\n",
            "[Epoch 12] [Batch 2700] [G loss: 1.6183]\n",
            "[Epoch 12] [Batch 2800] [G loss: 1.7245]\n",
            "[Epoch 12] [Batch 2900] [G loss: 1.4722]\n",
            "[Epoch 12] [Batch 3000] [G loss: 1.1727]\n",
            "[Epoch 12] [Batch 3100] [G loss: 0.5911]\n",
            "[Epoch 12] [Batch 3200] [G loss: 0.8362]\n",
            "[Epoch 12] [Batch 3300] [G loss: 1.3512]\n",
            "[Epoch 12] [Batch 3400] [G loss: 1.7254]\n",
            "[Epoch 12] [Batch 3500] [G loss: 1.5488]\n",
            "[Epoch 12] [Batch 3600] [G loss: 1.3356]\n",
            "[Epoch 12] [Batch 3700] [G loss: 1.6692]\n",
            "[Epoch 12] [Batch 3800] [G loss: 1.4603]\n",
            "[Epoch 12] [Batch 3900] [G loss: 1.4430]\n",
            "[Epoch 12] [Batch 4000] [G loss: 1.2606]\n",
            "[Epoch 12] [Batch 4100] [G loss: 1.4421]\n",
            "[Epoch 12] [Batch 4200] [G loss: 1.3682]\n",
            "[Epoch 12] [Batch 4300] [G loss: 1.3331]\n",
            "[Epoch 12] [Batch 4400] [G loss: 1.3135]\n",
            "[Epoch 12] [Batch 4500] [G loss: 1.5482]\n",
            "[Epoch 12] [Batch 4600] [G loss: 1.4363]\n",
            "[Epoch 12] [Batch 4700] [G loss: 1.4283]\n",
            "[Epoch 12] [Batch 4800] [G loss: 1.2257]\n",
            "[Epoch 12] [Batch 4900] [G loss: 1.5234]\n",
            "[Epoch 12] [Batch 5000] [G loss: 1.2472]\n",
            "[Epoch 12] [Batch 5100] [G loss: 1.4326]\n",
            "[Epoch 12] [Batch 5200] [G loss: 1.7086]\n",
            "[Epoch 12] [Batch 5300] [G loss: 1.5254]\n",
            "[Epoch 12] [Batch 5400] [G loss: 1.4434]\n",
            "[Epoch 12] [Batch 5500] [G loss: 1.4386]\n",
            "[Epoch 12] [Batch 5600] [G loss: 1.5791]\n",
            "[Epoch 12] [Batch 5700] [G loss: 1.1505]\n",
            "[Epoch 12] [Batch 5800] [G loss: 1.6183]\n",
            "[Epoch 12] [Batch 5900] [G loss: 1.4143]\n",
            "[Epoch 12] [Batch 6000] [G loss: 1.2498]\n",
            "[Epoch 12] [Batch 6100] [G loss: 1.4603]\n",
            "[Epoch 12] [Batch 6200] [G loss: 1.4533]\n",
            "[Epoch 12] [Batch 6300] [G loss: 1.2802]\n",
            "[Epoch 12] [Batch 6400] [G loss: 1.7166]\n",
            "[Epoch 12] [Batch 6500] [G loss: 1.9337]\n",
            "[Epoch 12] [Batch 6600] [G loss: 0.8977]\n",
            "[Epoch 12] [Batch 6700] [G loss: 1.1480]\n",
            "[Epoch 12] [Batch 6800] [G loss: 1.4380]\n",
            "[Epoch 12] [Batch 6900] [G loss: 1.7696]\n",
            "[Epoch 12] [Batch 7000] [G loss: 1.2068]\n",
            "[Epoch 13] [Batch 0] [G loss: 1.3128]\n",
            "[Epoch 13] [Batch 100] [G loss: 1.7867]\n",
            "[Epoch 13] [Batch 200] [G loss: 1.6333]\n",
            "[Epoch 13] [Batch 300] [G loss: 1.1717]\n",
            "[Epoch 13] [Batch 400] [G loss: 1.4632]\n",
            "[Epoch 13] [Batch 500] [G loss: 1.6501]\n",
            "[Epoch 13] [Batch 600] [G loss: 1.0614]\n",
            "[Epoch 13] [Batch 700] [G loss: 1.2005]\n",
            "[Epoch 13] [Batch 800] [G loss: 1.4206]\n",
            "[Epoch 13] [Batch 900] [G loss: 1.6694]\n",
            "[Epoch 13] [Batch 1000] [G loss: 1.2551]\n",
            "[Epoch 13] [Batch 1100] [G loss: 1.7428]\n",
            "[Epoch 13] [Batch 1200] [G loss: 1.3734]\n",
            "[Epoch 13] [Batch 1300] [G loss: 1.3531]\n",
            "[Epoch 13] [Batch 1400] [G loss: 1.3788]\n",
            "[Epoch 13] [Batch 1500] [G loss: 1.1064]\n",
            "[Epoch 13] [Batch 1600] [G loss: 1.9350]\n",
            "[Epoch 13] [Batch 1700] [G loss: 1.6772]\n",
            "[Epoch 13] [Batch 1800] [G loss: 1.1414]\n",
            "[Epoch 13] [Batch 1900] [G loss: 1.5435]\n",
            "[Epoch 13] [Batch 2000] [G loss: 1.3433]\n",
            "[Epoch 13] [Batch 2100] [G loss: 1.6792]\n",
            "[Epoch 13] [Batch 2200] [G loss: 1.3163]\n",
            "[Epoch 13] [Batch 2300] [G loss: 1.8112]\n",
            "[Epoch 13] [Batch 2400] [G loss: 1.8978]\n",
            "[Epoch 13] [Batch 2500] [G loss: 1.3709]\n",
            "[Epoch 13] [Batch 2600] [G loss: 1.2778]\n",
            "[Epoch 13] [Batch 2700] [G loss: 1.3736]\n",
            "[Epoch 13] [Batch 2800] [G loss: 1.5086]\n",
            "[Epoch 13] [Batch 2900] [G loss: 1.4703]\n",
            "[Epoch 13] [Batch 3000] [G loss: 1.2807]\n",
            "[Epoch 13] [Batch 3100] [G loss: 1.2931]\n",
            "[Epoch 13] [Batch 3200] [G loss: 1.6527]\n",
            "[Epoch 13] [Batch 3300] [G loss: 1.3797]\n",
            "[Epoch 13] [Batch 3400] [G loss: 1.4555]\n",
            "[Epoch 13] [Batch 3500] [G loss: 1.8969]\n",
            "[Epoch 13] [Batch 3600] [G loss: 1.2720]\n",
            "[Epoch 13] [Batch 3700] [G loss: 1.6241]\n",
            "[Epoch 13] [Batch 3800] [G loss: 1.4996]\n",
            "[Epoch 13] [Batch 3900] [G loss: 1.8393]\n",
            "[Epoch 13] [Batch 4000] [G loss: 1.4987]\n",
            "[Epoch 13] [Batch 4100] [G loss: 1.4213]\n",
            "[Epoch 13] [Batch 4200] [G loss: 1.3817]\n",
            "[Epoch 13] [Batch 4300] [G loss: 1.3231]\n",
            "[Epoch 13] [Batch 4400] [G loss: 1.5233]\n",
            "[Epoch 13] [Batch 4500] [G loss: 1.7220]\n",
            "[Epoch 13] [Batch 4600] [G loss: 1.2268]\n",
            "[Epoch 13] [Batch 4700] [G loss: 1.4734]\n",
            "[Epoch 13] [Batch 4800] [G loss: 1.2561]\n",
            "[Epoch 13] [Batch 4900] [G loss: 1.4869]\n",
            "[Epoch 13] [Batch 5000] [G loss: 1.7398]\n",
            "[Epoch 13] [Batch 5100] [G loss: 1.4056]\n",
            "[Epoch 13] [Batch 5200] [G loss: 1.2182]\n",
            "[Epoch 13] [Batch 5300] [G loss: 1.1950]\n",
            "[Epoch 13] [Batch 5400] [G loss: 1.4338]\n",
            "[Epoch 13] [Batch 5500] [G loss: 1.1249]\n",
            "[Epoch 13] [Batch 5600] [G loss: 1.5236]\n",
            "[Epoch 13] [Batch 5700] [G loss: 1.5857]\n",
            "[Epoch 13] [Batch 5800] [G loss: 1.1873]\n",
            "[Epoch 13] [Batch 5900] [G loss: 1.3794]\n",
            "[Epoch 13] [Batch 6000] [G loss: 1.2779]\n",
            "[Epoch 13] [Batch 6100] [G loss: 1.6682]\n",
            "[Epoch 13] [Batch 6200] [G loss: 1.6394]\n",
            "[Epoch 13] [Batch 6300] [G loss: 1.5795]\n",
            "[Epoch 13] [Batch 6400] [G loss: 1.2367]\n",
            "[Epoch 13] [Batch 6500] [G loss: 1.4727]\n",
            "[Epoch 13] [Batch 6600] [G loss: 1.1306]\n",
            "[Epoch 13] [Batch 6700] [G loss: 1.2824]\n",
            "[Epoch 13] [Batch 6800] [G loss: 1.7055]\n",
            "[Epoch 13] [Batch 6900] [G loss: 1.4395]\n",
            "[Epoch 13] [Batch 7000] [G loss: 1.6813]\n",
            "[Epoch 14] [Batch 0] [G loss: 1.5924]\n",
            "[Epoch 14] [Batch 100] [G loss: 1.4176]\n",
            "[Epoch 14] [Batch 200] [G loss: 1.1487]\n",
            "[Epoch 14] [Batch 300] [G loss: 1.5641]\n",
            "[Epoch 14] [Batch 400] [G loss: 1.3459]\n",
            "[Epoch 14] [Batch 500] [G loss: 1.6742]\n",
            "[Epoch 14] [Batch 600] [G loss: 1.6398]\n",
            "[Epoch 14] [Batch 700] [G loss: 1.8767]\n",
            "[Epoch 14] [Batch 800] [G loss: 1.4716]\n",
            "[Epoch 14] [Batch 900] [G loss: 1.6043]\n",
            "[Epoch 14] [Batch 1000] [G loss: 1.2221]\n",
            "[Epoch 14] [Batch 1100] [G loss: 1.3616]\n",
            "[Epoch 14] [Batch 1200] [G loss: 1.3226]\n",
            "[Epoch 14] [Batch 1300] [G loss: 1.4141]\n",
            "[Epoch 14] [Batch 1400] [G loss: 1.6789]\n",
            "[Epoch 14] [Batch 1500] [G loss: 1.1617]\n",
            "[Epoch 14] [Batch 1600] [G loss: 1.5140]\n",
            "[Epoch 14] [Batch 1700] [G loss: 1.3457]\n",
            "[Epoch 14] [Batch 1800] [G loss: 1.5039]\n",
            "[Epoch 14] [Batch 1900] [G loss: 1.2706]\n",
            "[Epoch 14] [Batch 2000] [G loss: 1.1696]\n",
            "[Epoch 14] [Batch 2100] [G loss: 1.3208]\n",
            "[Epoch 14] [Batch 2200] [G loss: 1.6294]\n",
            "[Epoch 14] [Batch 2300] [G loss: 1.5134]\n",
            "[Epoch 14] [Batch 2400] [G loss: 1.7821]\n",
            "[Epoch 14] [Batch 2500] [G loss: 1.1140]\n",
            "[Epoch 14] [Batch 2600] [G loss: 1.2358]\n",
            "[Epoch 14] [Batch 2700] [G loss: 1.5317]\n",
            "[Epoch 14] [Batch 2800] [G loss: 1.2297]\n",
            "[Epoch 14] [Batch 2900] [G loss: 1.3505]\n",
            "[Epoch 14] [Batch 3000] [G loss: 1.5484]\n",
            "[Epoch 14] [Batch 3100] [G loss: 1.6313]\n",
            "[Epoch 14] [Batch 3200] [G loss: 1.1855]\n",
            "[Epoch 14] [Batch 3300] [G loss: 1.8624]\n",
            "[Epoch 14] [Batch 3400] [G loss: 1.6046]\n",
            "[Epoch 14] [Batch 3500] [G loss: 1.4713]\n",
            "[Epoch 14] [Batch 3600] [G loss: 1.5340]\n",
            "[Epoch 14] [Batch 3700] [G loss: 1.2632]\n",
            "[Epoch 14] [Batch 3800] [G loss: 1.4229]\n",
            "[Epoch 14] [Batch 3900] [G loss: 1.0559]\n",
            "[Epoch 14] [Batch 4000] [G loss: 1.1762]\n",
            "[Epoch 14] [Batch 4100] [G loss: 1.6039]\n",
            "[Epoch 14] [Batch 4200] [G loss: 1.2485]\n",
            "[Epoch 14] [Batch 4300] [G loss: 1.4821]\n",
            "[Epoch 14] [Batch 4400] [G loss: 1.2890]\n",
            "[Epoch 14] [Batch 4500] [G loss: 1.5412]\n",
            "[Epoch 14] [Batch 4600] [G loss: 1.7857]\n",
            "[Epoch 14] [Batch 4700] [G loss: 1.7223]\n",
            "[Epoch 14] [Batch 4800] [G loss: 1.8048]\n",
            "[Epoch 14] [Batch 4900] [G loss: 1.6181]\n",
            "[Epoch 14] [Batch 5000] [G loss: 1.4334]\n",
            "[Epoch 14] [Batch 5100] [G loss: 1.0767]\n",
            "[Epoch 14] [Batch 5200] [G loss: 0.8986]\n",
            "[Epoch 14] [Batch 5300] [G loss: 1.3568]\n",
            "[Epoch 14] [Batch 5400] [G loss: 1.4374]\n",
            "[Epoch 14] [Batch 5500] [G loss: 1.2632]\n",
            "[Epoch 14] [Batch 5600] [G loss: 1.3260]\n",
            "[Epoch 14] [Batch 5700] [G loss: 1.6005]\n",
            "[Epoch 14] [Batch 5800] [G loss: 1.4069]\n",
            "[Epoch 14] [Batch 5900] [G loss: 1.2977]\n",
            "[Epoch 14] [Batch 6000] [G loss: 1.8948]\n",
            "[Epoch 14] [Batch 6100] [G loss: 1.2625]\n",
            "[Epoch 14] [Batch 6200] [G loss: 1.3949]\n",
            "[Epoch 14] [Batch 6300] [G loss: 1.2416]\n",
            "[Epoch 14] [Batch 6400] [G loss: 1.4536]\n",
            "[Epoch 14] [Batch 6500] [G loss: 1.3087]\n",
            "[Epoch 14] [Batch 6600] [G loss: 1.1389]\n",
            "[Epoch 14] [Batch 6700] [G loss: 1.1228]\n",
            "[Epoch 14] [Batch 6800] [G loss: 1.4198]\n",
            "[Epoch 14] [Batch 6900] [G loss: 1.4382]\n",
            "[Epoch 14] [Batch 7000] [G loss: 1.5593]\n",
            "[Epoch 15] [Batch 0] [G loss: 1.8938]\n",
            "[Epoch 15] [Batch 100] [G loss: 1.4783]\n",
            "[Epoch 15] [Batch 200] [G loss: 1.3558]\n",
            "[Epoch 15] [Batch 300] [G loss: 1.1749]\n",
            "[Epoch 15] [Batch 400] [G loss: 1.7293]\n",
            "[Epoch 15] [Batch 500] [G loss: 1.3120]\n",
            "[Epoch 15] [Batch 600] [G loss: 1.5540]\n",
            "[Epoch 15] [Batch 700] [G loss: 1.5216]\n",
            "[Epoch 15] [Batch 800] [G loss: 1.4874]\n",
            "[Epoch 15] [Batch 900] [G loss: 1.3086]\n",
            "[Epoch 15] [Batch 1000] [G loss: 1.8106]\n",
            "[Epoch 15] [Batch 1100] [G loss: 1.5738]\n",
            "[Epoch 15] [Batch 1200] [G loss: 1.4373]\n",
            "[Epoch 15] [Batch 1300] [G loss: 1.5929]\n",
            "[Epoch 15] [Batch 1400] [G loss: 1.4299]\n",
            "[Epoch 15] [Batch 1500] [G loss: 1.7040]\n",
            "[Epoch 15] [Batch 1600] [G loss: 1.6845]\n",
            "[Epoch 15] [Batch 1700] [G loss: 1.5403]\n",
            "[Epoch 15] [Batch 1800] [G loss: 1.3821]\n",
            "[Epoch 15] [Batch 1900] [G loss: 1.5189]\n",
            "[Epoch 15] [Batch 2000] [G loss: 1.3890]\n",
            "[Epoch 15] [Batch 2100] [G loss: 1.2062]\n",
            "[Epoch 15] [Batch 2200] [G loss: 1.4659]\n",
            "[Epoch 15] [Batch 2300] [G loss: 1.2928]\n",
            "[Epoch 15] [Batch 2400] [G loss: 1.2384]\n",
            "[Epoch 15] [Batch 2500] [G loss: 1.5581]\n",
            "[Epoch 15] [Batch 2600] [G loss: 1.6408]\n",
            "[Epoch 15] [Batch 2700] [G loss: 1.5487]\n",
            "[Epoch 15] [Batch 2800] [G loss: 1.6680]\n",
            "[Epoch 15] [Batch 2900] [G loss: 1.3727]\n",
            "[Epoch 15] [Batch 3000] [G loss: 1.4681]\n",
            "[Epoch 15] [Batch 3100] [G loss: 1.3613]\n",
            "[Epoch 15] [Batch 3200] [G loss: 1.5120]\n",
            "[Epoch 15] [Batch 3300] [G loss: 1.0385]\n",
            "[Epoch 15] [Batch 3400] [G loss: 1.0302]\n",
            "[Epoch 15] [Batch 3500] [G loss: 1.4022]\n",
            "[Epoch 15] [Batch 3600] [G loss: 1.4280]\n",
            "[Epoch 15] [Batch 3700] [G loss: 1.3609]\n",
            "[Epoch 15] [Batch 3800] [G loss: 1.3380]\n",
            "[Epoch 15] [Batch 3900] [G loss: 2.2011]\n",
            "[Epoch 15] [Batch 4000] [G loss: 1.3611]\n",
            "[Epoch 15] [Batch 4100] [G loss: 1.2687]\n",
            "[Epoch 15] [Batch 4200] [G loss: 1.3142]\n",
            "[Epoch 15] [Batch 4300] [G loss: 1.5236]\n",
            "[Epoch 15] [Batch 4400] [G loss: 1.3186]\n",
            "[Epoch 15] [Batch 4500] [G loss: 1.3909]\n",
            "[Epoch 15] [Batch 4600] [G loss: 1.4749]\n",
            "[Epoch 15] [Batch 4700] [G loss: 1.0585]\n",
            "[Epoch 15] [Batch 4800] [G loss: 1.5407]\n",
            "[Epoch 15] [Batch 4900] [G loss: 1.1451]\n",
            "[Epoch 15] [Batch 5000] [G loss: 1.3851]\n",
            "[Epoch 15] [Batch 5100] [G loss: 1.2062]\n",
            "[Epoch 15] [Batch 5200] [G loss: 1.4014]\n",
            "[Epoch 15] [Batch 5300] [G loss: 1.6035]\n",
            "[Epoch 15] [Batch 5400] [G loss: 1.5991]\n",
            "[Epoch 15] [Batch 5500] [G loss: 1.1016]\n",
            "[Epoch 15] [Batch 5600] [G loss: 1.5870]\n",
            "[Epoch 15] [Batch 5700] [G loss: 1.4685]\n",
            "[Epoch 15] [Batch 5800] [G loss: 1.5374]\n",
            "[Epoch 15] [Batch 5900] [G loss: 0.9956]\n",
            "[Epoch 15] [Batch 6000] [G loss: 1.3723]\n",
            "[Epoch 15] [Batch 6100] [G loss: 1.3197]\n",
            "[Epoch 15] [Batch 6200] [G loss: 1.3967]\n",
            "[Epoch 15] [Batch 6300] [G loss: 0.9991]\n",
            "[Epoch 15] [Batch 6400] [G loss: 1.2246]\n",
            "[Epoch 15] [Batch 6500] [G loss: 1.4445]\n",
            "[Epoch 15] [Batch 6600] [G loss: 1.2443]\n",
            "[Epoch 15] [Batch 6700] [G loss: 1.8041]\n",
            "[Epoch 15] [Batch 6800] [G loss: 1.3629]\n",
            "[Epoch 15] [Batch 6900] [G loss: 1.1998]\n",
            "[Epoch 15] [Batch 7000] [G loss: 1.3090]\n",
            "üíæ UNet Checkpoint saved: /content/drive/MyDrive/Painter_Assignment/checkpoints_unet/epoch_15.pth\n",
            "[Epoch 16] [Batch 0] [G loss: 1.7830]\n",
            "[Epoch 16] [Batch 100] [G loss: 1.6740]\n",
            "[Epoch 16] [Batch 200] [G loss: 1.3984]\n",
            "[Epoch 16] [Batch 300] [G loss: 1.2563]\n",
            "[Epoch 16] [Batch 400] [G loss: 1.0827]\n",
            "[Epoch 16] [Batch 500] [G loss: 1.8303]\n",
            "[Epoch 16] [Batch 600] [G loss: 1.2712]\n",
            "[Epoch 16] [Batch 700] [G loss: 1.5582]\n",
            "[Epoch 16] [Batch 800] [G loss: 1.5122]\n",
            "[Epoch 16] [Batch 900] [G loss: 1.6146]\n",
            "[Epoch 16] [Batch 1000] [G loss: 1.3565]\n",
            "[Epoch 16] [Batch 1100] [G loss: 1.0248]\n",
            "[Epoch 16] [Batch 1200] [G loss: 1.1373]\n",
            "[Epoch 16] [Batch 1300] [G loss: 1.5389]\n",
            "[Epoch 16] [Batch 1400] [G loss: 1.4533]\n",
            "[Epoch 16] [Batch 1500] [G loss: 1.2471]\n",
            "[Epoch 16] [Batch 1600] [G loss: 1.5131]\n",
            "[Epoch 16] [Batch 1700] [G loss: 1.5037]\n",
            "[Epoch 16] [Batch 1800] [G loss: 1.4065]\n",
            "[Epoch 16] [Batch 1900] [G loss: 1.2992]\n",
            "[Epoch 16] [Batch 2000] [G loss: 1.3284]\n",
            "[Epoch 16] [Batch 2100] [G loss: 1.1131]\n",
            "[Epoch 16] [Batch 2200] [G loss: 1.2726]\n",
            "[Epoch 16] [Batch 2300] [G loss: 1.7081]\n",
            "[Epoch 16] [Batch 2400] [G loss: 1.2576]\n",
            "[Epoch 16] [Batch 2500] [G loss: 1.7331]\n",
            "[Epoch 16] [Batch 2600] [G loss: 1.6202]\n",
            "[Epoch 16] [Batch 2700] [G loss: 1.5450]\n",
            "[Epoch 16] [Batch 2800] [G loss: 1.3297]\n",
            "[Epoch 16] [Batch 2900] [G loss: 1.3716]\n",
            "[Epoch 16] [Batch 3000] [G loss: 1.4811]\n",
            "[Epoch 16] [Batch 3100] [G loss: 1.3018]\n",
            "[Epoch 16] [Batch 3200] [G loss: 1.4284]\n",
            "[Epoch 16] [Batch 3300] [G loss: 1.4380]\n",
            "[Epoch 16] [Batch 3400] [G loss: 1.4204]\n",
            "[Epoch 16] [Batch 3500] [G loss: 1.5439]\n",
            "[Epoch 16] [Batch 3600] [G loss: 1.1302]\n",
            "[Epoch 16] [Batch 3700] [G loss: 1.2817]\n",
            "[Epoch 16] [Batch 3800] [G loss: 1.4708]\n",
            "[Epoch 16] [Batch 3900] [G loss: 1.4378]\n",
            "[Epoch 16] [Batch 4000] [G loss: 1.3776]\n",
            "[Epoch 16] [Batch 4100] [G loss: 1.6112]\n",
            "[Epoch 16] [Batch 4200] [G loss: 1.4342]\n",
            "[Epoch 16] [Batch 4300] [G loss: 1.3617]\n",
            "[Epoch 16] [Batch 4400] [G loss: 1.4232]\n",
            "[Epoch 16] [Batch 4500] [G loss: 1.3556]\n",
            "[Epoch 16] [Batch 4600] [G loss: 1.5655]\n",
            "[Epoch 16] [Batch 4700] [G loss: 1.3800]\n",
            "[Epoch 16] [Batch 4800] [G loss: 1.5212]\n",
            "[Epoch 16] [Batch 4900] [G loss: 1.4179]\n",
            "[Epoch 16] [Batch 5000] [G loss: 1.2542]\n",
            "[Epoch 16] [Batch 5100] [G loss: 1.3330]\n",
            "[Epoch 16] [Batch 5200] [G loss: 1.0998]\n",
            "[Epoch 16] [Batch 5300] [G loss: 0.9913]\n",
            "[Epoch 16] [Batch 5400] [G loss: 1.3740]\n",
            "[Epoch 16] [Batch 5500] [G loss: 1.1739]\n",
            "[Epoch 16] [Batch 5600] [G loss: 1.7175]\n",
            "[Epoch 16] [Batch 5700] [G loss: 1.6423]\n",
            "[Epoch 16] [Batch 5800] [G loss: 1.4888]\n",
            "[Epoch 16] [Batch 5900] [G loss: 1.6295]\n",
            "[Epoch 16] [Batch 6000] [G loss: 1.5711]\n",
            "[Epoch 16] [Batch 6100] [G loss: 1.1106]\n",
            "[Epoch 16] [Batch 6200] [G loss: 1.2708]\n",
            "[Epoch 16] [Batch 6300] [G loss: 1.4220]\n",
            "[Epoch 16] [Batch 6400] [G loss: 1.3739]\n",
            "[Epoch 16] [Batch 6500] [G loss: 1.4243]\n",
            "[Epoch 16] [Batch 6600] [G loss: 1.2057]\n",
            "[Epoch 16] [Batch 6700] [G loss: 1.1975]\n",
            "[Epoch 16] [Batch 6800] [G loss: 1.1950]\n",
            "[Epoch 16] [Batch 6900] [G loss: 1.6368]\n",
            "[Epoch 16] [Batch 7000] [G loss: 1.2485]\n",
            "[Epoch 17] [Batch 0] [G loss: 0.9868]\n",
            "[Epoch 17] [Batch 100] [G loss: 1.6564]\n",
            "[Epoch 17] [Batch 200] [G loss: 1.6983]\n",
            "[Epoch 17] [Batch 300] [G loss: 1.6740]\n",
            "[Epoch 17] [Batch 400] [G loss: 1.0888]\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(ConfigUNet.START_EPOCH, ConfigUNet.EPOCHS):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        real_A = batch[\"photo\"].to(ConfigUNet.DEVICE)\n",
        "        real_B = batch[\"monet\"].to(ConfigUNet.DEVICE)\n",
        "\n",
        "        valid = torch.ones((real_A.size(0), 1, 16, 16), requires_grad=False).to(ConfigUNet.DEVICE)\n",
        "        fake = torch.zeros((real_A.size(0), 1, 16, 16), requires_grad=False).to(ConfigUNet.DEVICE)\n",
        "\n",
        "        # --- Train Generators ---\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        with autocast('cuda'):\n",
        "            # Identity\n",
        "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
        "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
        "            loss_identity = (loss_id_A + loss_id_B) / 2 * ConfigUNet.LAMBDA_ID\n",
        "\n",
        "            # GAN\n",
        "            fake_B = G_AB(real_A)\n",
        "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
        "            fake_A = G_BA(real_B)\n",
        "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
        "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
        "\n",
        "            # Cycle\n",
        "            recov_A = G_BA(fake_B)\n",
        "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
        "            recov_B = G_AB(fake_A)\n",
        "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
        "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2 * ConfigUNet.LAMBDA_CYCLE\n",
        "\n",
        "            loss_G = loss_GAN + loss_cycle + loss_identity\n",
        "\n",
        "        scaler.scale(loss_G).backward()\n",
        "        scaler.step(optimizer_G)\n",
        "        scaler.update()\n",
        "\n",
        "        # --- Train Discriminators ---\n",
        "        optimizer_D_A.zero_grad()\n",
        "        with autocast('cuda'):\n",
        "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
        "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
        "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
        "            loss_D_A = (loss_real + loss_fake) / 2\n",
        "        scaler.scale(loss_D_A).backward()\n",
        "        scaler.step(optimizer_D_A)\n",
        "        scaler.update()\n",
        "\n",
        "        optimizer_D_B.zero_grad()\n",
        "        with autocast('cuda'):\n",
        "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
        "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
        "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
        "            loss_D_B = (loss_real + loss_fake) / 2\n",
        "        scaler.scale(loss_D_B).backward()\n",
        "        scaler.step(optimizer_D_B)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            wandb.log({\"Loss/G\": loss_G.item(), \"Loss/D\": loss_D_A.item()+loss_D_B.item(), \"Epoch\": epoch})\n",
        "            print(f\"[Epoch {epoch}] [Batch {i}] [G loss: {loss_G.item():.4f}]\")\n",
        "\n",
        "\n",
        "    # --- Logging Images (Once per Epoch) ---\n",
        "    img_real_A = real_A[0].detach().cpu() * 0.5 + 0.5\n",
        "    img_fake_B = fake_B[0].detach().cpu() * 0.5 + 0.5\n",
        "    img_real_B = real_B[0].detach().cpu() * 0.5 + 0.5\n",
        "    img_fake_A = fake_A[0].detach().cpu() * 0.5 + 0.5\n",
        "\n",
        "    wandb.log({\n",
        "        \"Visual/Real Photo\": wandb.Image(img_real_A, caption=f\"Real Photo (Epoch {epoch})\"),\n",
        "        \"Visual/Generated Monet\": wandb.Image(img_fake_B, caption=f\"Generated Monet (Epoch {epoch})\"),\n",
        "        \"Visual/Real Monet\": wandb.Image(img_real_B, caption=f\"Real Monet (Epoch {epoch})\"),\n",
        "        \"Visual/Generated Photo\": wandb.Image(img_fake_A, caption=f\"Reconstructed Photo (Epoch {epoch})\")\n",
        "    })\n",
        "\n",
        "    # Save Checkpoint\n",
        "    if epoch % ConfigUNet.SAVE_EPOCH_FREQ == 0:\n",
        "        save_path = f\"{ConfigUNet.CHECKPOINT_DIR}/epoch_{epoch}.pth\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'G_AB': G_AB.state_dict(),\n",
        "            'G_BA': G_BA.state_dict(),\n",
        "            'D_A': D_A.state_dict(),\n",
        "            'D_B': D_B.state_dict(),\n",
        "            'optimizer_G': optimizer_G.state_dict(),\n",
        "        }, save_path)\n",
        "        print(f\"üíæ UNet Checkpoint saved: {save_path}\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "JPumTni16wb3",
        "outputId": "ba09ee65-3a8f-4b12-c025-89f90a5d3e8d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Loss/D</td><td>‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>Loss/G</td><td>‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>0</td></tr><tr><td>Loss/D</td><td>0.40621</td></tr><tr><td>Loss/G</td><td>1.11374</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wise-dust-1</strong> at: <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET/runs/nbfpc3pj' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET/runs/nbfpc3pj</a><br> View project at: <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_UNET</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251220_082336-nbfpc3pj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJSXFBGJc-Oo"
      },
      "source": [
        "#\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Augmentetion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Nz-eDaKfc84i"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.amp import GradScaler, autocast # Mixed Precision (·É°·Éò·É©·É•·Éê·É†·Éò·É°·Éó·Éï·Éò·É°)\n",
        "import wandb\n",
        "import itertools\n",
        "import shutil\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlXttBdceVMl",
        "outputId": "dd67b63e-51ae-4b1f-824d-151f899f7a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/Painter_Assignment\"\n",
        "SRC_PATH = os.path.join(PROJECT_PATH, \"src\")\n",
        "CHECKPOINT_PATH = os.path.join(PROJECT_PATH, \"checkpoints\")\n",
        "KAGGLE_JSON_PATH = os.path.join(PROJECT_PATH, \"kaggle.json\")\n",
        "\n",
        "os.makedirs(SRC_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6DX6rGrkD3G",
        "outputId": "307fae42-517c-4977-b948-30c4ab06b4fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset from Kaggle...\n",
            "Downloading gan-getting-started.zip to /content\n",
            " 80% 294M/367M [00:02<00:00, 117MB/s] \n",
            "100% 367M/367M [00:02<00:00, 165MB/s]\n",
            "Dataset downloaded and extracted successfully!\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(KAGGLE_JSON_PATH):\n",
        "    shutil.copy(KAGGLE_JSON_PATH, \"/content/kaggle.json\")\n",
        "    os.chmod(\"/content/kaggle.json\", 0o600)\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n",
        "else:\n",
        "    print(f\"ERROR: kaggle.json not found in {PROJECT_PATH}\")\n",
        "\n",
        "if not os.path.exists(\"/content/dataset\"):\n",
        "    print(\"Downloading dataset from Kaggle...\")\n",
        "    !kaggle competitions download -c gan-getting-started\n",
        "    !unzip -q gan-getting-started.zip -d /content/dataset\n",
        "    print(\"Dataset downloaded and extracted successfully!\")\n",
        "else:\n",
        "    print(\"Dataset already exists on local disk.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "MUn5BkUbeAxp"
      },
      "outputs": [],
      "source": [
        "sys.path.append(\"/content/drive/MyDrive/Painter_Assignment/src\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "IqPutDuveEEv"
      },
      "outputs": [],
      "source": [
        "from config_aug import ConfigAug\n",
        "from dataset_aug import AugmentedDataset\n",
        "from models import GeneratorResNet, Discriminator\n",
        "from utils import weights_init_normal, ReplayBuffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "YOHrjTlcgaEW"
      },
      "outputs": [],
      "source": [
        "os.makedirs(ConfigAug.CHECKPOINT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF011HLUifD6",
        "outputId": "901ab0be-6728-4894-b6e9-d4d9638824c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melene-gabeskiria2004\u001b[0m (\u001b[33melene-gabeskiria2004-free-univiersity-of-tbilisi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# This forces WandB to forget the previous key and ask again\n",
        "wandb.login(relogin=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "ktZzhMlkgdhT",
        "outputId": "9613f960-8c20-49c9-8d81-219f0eedd71e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">visionary-yogurt-2</strong> at: <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_AUG/runs/p6dee8ai' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_AUG/runs/p6dee8ai</a><br> View project at: <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_AUG' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_AUG</a><br>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251228_104312-p6dee8ai/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251228_104601-bh0osuto</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_AUG/runs/bh0osuto' target=\"_blank\">lyric-aardvark-2</a></strong> to <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_AUG' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_AUG' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_AUG</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_AUG/runs/bh0osuto' target=\"_blank\">https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_AUG/runs/bh0osuto</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/elene-gabeskiria2004-free-univiersity-of-tbilisi/CycleGAN_Painter_AUG/runs/bh0osuto?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7a031067cef0>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(\n",
        "    project=ConfigAug.PROJECT_NAME,\n",
        "    config={k:v for k,v in ConfigAug.__dict__.items() if not k.startswith('__')},\n",
        "    reinit=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc477VoRivbv"
      },
      "outputs": [],
      "source": [
        "G_AB = GeneratorResNet().to(ConfigAug.DEVICE) # Photo -> Monet\n",
        "G_BA = GeneratorResNet().to(ConfigAug.DEVICE) # Monet -> Photo\n",
        "D_A = Discriminator().to(ConfigAug.DEVICE)\n",
        "D_B = Discriminator().to(ConfigAug.DEVICE)\n",
        "\n",
        "G_AB.apply(weights_init_normal)\n",
        "G_BA.apply(weights_init_normal)\n",
        "D_A.apply(weights_init_normal)\n",
        "D_B.apply(weights_init_normal)\n",
        "\n",
        "criterion_GAN = nn.MSELoss()\n",
        "criterion_cycle = nn.L1Loss()\n",
        "criterion_identity = nn.L1Loss()\n",
        "\n",
        "optimizer_G = torch.optim.Adam(\n",
        "    itertools.chain(G_AB.parameters(), G_BA.parameters()),\n",
        "    lr=ConfigAug.LR, betas=(ConfigAug.B1, ConfigAug.B2)\n",
        ")\n",
        "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=ConfigAug.LR, betas=(ConfigAug.B1, ConfigAug.B2))\n",
        "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=ConfigAug.LR, betas=(ConfigAug.B1, ConfigAug.B2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR_70Btxi-Lk"
      },
      "outputs": [],
      "source": [
        "def lambda_rule(epoch):\n",
        "    decay_start = ConfigAug.DECAY_START_EPOCH\n",
        "    total_epochs = ConfigAug.EPOCHS\n",
        "    lr_l = 1.0 - max(0, epoch + 1 - decay_start) / float(total_epochs - decay_start + 1)\n",
        "    return lr_l\n",
        "\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_rule)\n",
        "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lambda_rule)\n",
        "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lambda_rule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBOw5OeRjHu5"
      },
      "outputs": [],
      "source": [
        "#Data Loader (Augmented)\n",
        "dataset = AugmentedDataset(ConfigAug.TRAIN_MONET, ConfigAug.TRAIN_PHOTO, mode='train')\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=ConfigAug.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=ConfigAug.NUM_WORKERS\n",
        ")\n",
        "\n",
        "# Training Utilities\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()\n",
        "scaler = GradScaler('cuda') # AMP Scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkWTPJWDkWHK",
        "outputId": "9e7d2e3d-5b7a-4f03-e955-cc0b135201f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 0/30] [Batch 0] [G loss: 12.0118]\n",
            "[Epoch 0/30] [Batch 200] [G loss: 4.9115]\n",
            "[Epoch 0/30] [Batch 400] [G loss: 5.9193]\n",
            "[Epoch 0/30] [Batch 600] [G loss: 5.3047]\n",
            "[Epoch 0/30] [Batch 800] [G loss: 3.2592]\n",
            "[Epoch 0/30] [Batch 1000] [G loss: 4.0322]\n",
            "[Epoch 0/30] [Batch 1200] [G loss: 4.9428]\n",
            "[Epoch 0/30] [Batch 1400] [G loss: 5.1872]\n",
            "[Epoch 0/30] [Batch 1600] [G loss: 3.7137]\n",
            "[Epoch 0/30] [Batch 1800] [G loss: 2.8046]\n",
            "[Epoch 0/30] [Batch 2000] [G loss: 4.8679]\n",
            "[Epoch 0/30] [Batch 2200] [G loss: 3.6290]\n",
            "[Epoch 0/30] [Batch 2400] [G loss: 3.4303]\n",
            "[Epoch 0/30] [Batch 2600] [G loss: 3.6428]\n",
            "[Epoch 0/30] [Batch 2800] [G loss: 2.6855]\n",
            "[Epoch 0/30] [Batch 3000] [G loss: 4.1454]\n",
            "[Epoch 0/30] [Batch 3200] [G loss: 3.2228]\n",
            "[Epoch 0/30] [Batch 3400] [G loss: 2.7892]\n",
            "[Epoch 0/30] [Batch 3600] [G loss: 5.0255]\n",
            "[Epoch 0/30] [Batch 3800] [G loss: 3.3763]\n",
            "[Epoch 0/30] [Batch 4000] [G loss: 3.4192]\n",
            "[Epoch 0/30] [Batch 4200] [G loss: 3.7980]\n",
            "[Epoch 0/30] [Batch 4400] [G loss: 4.2985]\n",
            "[Epoch 0/30] [Batch 4600] [G loss: 3.6938]\n",
            "[Epoch 0/30] [Batch 4800] [G loss: 3.2164]\n",
            "[Epoch 0/30] [Batch 5000] [G loss: 5.3137]\n",
            "[Epoch 0/30] [Batch 5200] [G loss: 3.1505]\n",
            "[Epoch 0/30] [Batch 5400] [G loss: 4.1151]\n",
            "[Epoch 0/30] [Batch 5600] [G loss: 2.8720]\n",
            "[Epoch 0/30] [Batch 5800] [G loss: 2.7952]\n",
            "[Epoch 0/30] [Batch 6000] [G loss: 4.8077]\n",
            "[Epoch 0/30] [Batch 6200] [G loss: 5.1491]\n",
            "[Epoch 0/30] [Batch 6400] [G loss: 3.3127]\n",
            "[Epoch 0/30] [Batch 6600] [G loss: 2.4957]\n",
            "[Epoch 0/30] [Batch 6800] [G loss: 2.7402]\n",
            "[Epoch 0/30] [Batch 7000] [G loss: 2.6351]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Data passed to `wandb.Image` should consist of values in the range [0, 255], image data will be normalized to this range, but behavior will be removed in a future version of wandb.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images logged for Epoch 0\n",
            "Checkpoint saved: /content/drive/MyDrive/University/Painter_Assignment/checkpoints_aug/epoch_0.pth\n",
            "[Epoch 1/30] [Batch 0] [G loss: 3.2457]\n",
            "[Epoch 1/30] [Batch 200] [G loss: 3.6045]\n",
            "[Epoch 1/30] [Batch 400] [G loss: 3.8146]\n",
            "[Epoch 1/30] [Batch 600] [G loss: 5.1679]\n",
            "[Epoch 1/30] [Batch 800] [G loss: 2.4944]\n",
            "[Epoch 1/30] [Batch 1000] [G loss: 2.8451]\n",
            "[Epoch 1/30] [Batch 1200] [G loss: 4.5756]\n",
            "[Epoch 1/30] [Batch 1400] [G loss: 2.7985]\n",
            "[Epoch 1/30] [Batch 1600] [G loss: 3.6621]\n",
            "[Epoch 1/30] [Batch 1800] [G loss: 4.1669]\n",
            "[Epoch 1/30] [Batch 2000] [G loss: 3.8529]\n",
            "[Epoch 1/30] [Batch 2200] [G loss: 2.8422]\n",
            "[Epoch 1/30] [Batch 2400] [G loss: 5.0382]\n",
            "[Epoch 1/30] [Batch 2600] [G loss: 4.0240]\n",
            "[Epoch 1/30] [Batch 2800] [G loss: 3.1674]\n",
            "[Epoch 1/30] [Batch 3000] [G loss: 3.5425]\n",
            "[Epoch 1/30] [Batch 3200] [G loss: 2.6305]\n",
            "[Epoch 1/30] [Batch 3400] [G loss: 4.9643]\n",
            "[Epoch 1/30] [Batch 3600] [G loss: 3.9882]\n",
            "[Epoch 1/30] [Batch 3800] [G loss: 2.5176]\n",
            "[Epoch 1/30] [Batch 4000] [G loss: 3.8727]\n",
            "[Epoch 1/30] [Batch 4200] [G loss: 3.2721]\n",
            "[Epoch 1/30] [Batch 4400] [G loss: 2.6732]\n",
            "[Epoch 1/30] [Batch 4600] [G loss: 3.1848]\n",
            "[Epoch 1/30] [Batch 4800] [G loss: 3.6604]\n",
            "[Epoch 1/30] [Batch 5000] [G loss: 2.4974]\n",
            "[Epoch 1/30] [Batch 5200] [G loss: 3.8474]\n",
            "[Epoch 1/30] [Batch 5400] [G loss: 2.4003]\n",
            "[Epoch 1/30] [Batch 5600] [G loss: 3.0843]\n",
            "[Epoch 1/30] [Batch 5800] [G loss: 3.3212]\n",
            "[Epoch 1/30] [Batch 6000] [G loss: 3.0950]\n",
            "[Epoch 1/30] [Batch 6200] [G loss: 3.0874]\n",
            "[Epoch 1/30] [Batch 6400] [G loss: 3.0060]\n",
            "[Epoch 1/30] [Batch 6600] [G loss: 2.9182]\n",
            "[Epoch 1/30] [Batch 6800] [G loss: 4.9693]\n",
            "[Epoch 1/30] [Batch 7000] [G loss: 3.3984]\n",
            "Images logged for Epoch 1\n",
            "[Epoch 2/30] [Batch 0] [G loss: 3.0250]\n",
            "[Epoch 2/30] [Batch 200] [G loss: 3.2733]\n",
            "[Epoch 2/30] [Batch 400] [G loss: 5.4699]\n",
            "[Epoch 2/30] [Batch 600] [G loss: 2.3446]\n",
            "[Epoch 2/30] [Batch 800] [G loss: 2.4391]\n",
            "[Epoch 2/30] [Batch 1000] [G loss: 4.0066]\n",
            "[Epoch 2/30] [Batch 1200] [G loss: 4.2379]\n",
            "[Epoch 2/30] [Batch 1400] [G loss: 3.2084]\n",
            "[Epoch 2/30] [Batch 1600] [G loss: 3.1992]\n",
            "[Epoch 2/30] [Batch 1800] [G loss: 3.8026]\n",
            "[Epoch 2/30] [Batch 2000] [G loss: 2.6059]\n",
            "[Epoch 2/30] [Batch 2200] [G loss: 3.3015]\n",
            "[Epoch 2/30] [Batch 2400] [G loss: 3.4529]\n",
            "[Epoch 2/30] [Batch 2600] [G loss: 2.7574]\n",
            "[Epoch 2/30] [Batch 2800] [G loss: 3.0072]\n",
            "[Epoch 2/30] [Batch 3000] [G loss: 2.8275]\n",
            "[Epoch 2/30] [Batch 3200] [G loss: 3.1367]\n",
            "[Epoch 2/30] [Batch 3400] [G loss: 3.0965]\n",
            "[Epoch 2/30] [Batch 3600] [G loss: 3.2832]\n",
            "[Epoch 2/30] [Batch 3800] [G loss: 3.2580]\n",
            "[Epoch 2/30] [Batch 4000] [G loss: 2.9006]\n",
            "[Epoch 2/30] [Batch 4200] [G loss: 3.0254]\n",
            "[Epoch 2/30] [Batch 4400] [G loss: 3.0963]\n",
            "[Epoch 2/30] [Batch 4600] [G loss: 2.5860]\n",
            "[Epoch 2/30] [Batch 4800] [G loss: 2.3699]\n",
            "[Epoch 2/30] [Batch 5000] [G loss: 3.4069]\n",
            "[Epoch 2/30] [Batch 5200] [G loss: 2.3126]\n",
            "[Epoch 2/30] [Batch 5400] [G loss: 2.2768]\n",
            "[Epoch 2/30] [Batch 5600] [G loss: 4.9283]\n",
            "[Epoch 2/30] [Batch 5800] [G loss: 3.4040]\n",
            "[Epoch 2/30] [Batch 6000] [G loss: 3.6492]\n",
            "[Epoch 2/30] [Batch 6200] [G loss: 2.9636]\n",
            "[Epoch 2/30] [Batch 6400] [G loss: 3.9400]\n",
            "[Epoch 2/30] [Batch 6600] [G loss: 2.4726]\n",
            "[Epoch 2/30] [Batch 6800] [G loss: 3.0739]\n",
            "[Epoch 2/30] [Batch 7000] [G loss: 3.7746]\n",
            "Images logged for Epoch 2\n",
            "[Epoch 3/30] [Batch 0] [G loss: 3.5346]\n",
            "[Epoch 3/30] [Batch 200] [G loss: 3.4024]\n",
            "[Epoch 3/30] [Batch 400] [G loss: 3.0688]\n",
            "[Epoch 3/30] [Batch 600] [G loss: 2.7720]\n",
            "[Epoch 3/30] [Batch 800] [G loss: 2.8083]\n",
            "[Epoch 3/30] [Batch 1000] [G loss: 2.5494]\n",
            "[Epoch 3/30] [Batch 1200] [G loss: 2.5720]\n",
            "[Epoch 3/30] [Batch 1400] [G loss: 4.2894]\n",
            "[Epoch 3/30] [Batch 1600] [G loss: 3.4030]\n",
            "[Epoch 3/30] [Batch 1800] [G loss: 3.1181]\n",
            "[Epoch 3/30] [Batch 2000] [G loss: 2.9481]\n",
            "[Epoch 3/30] [Batch 2200] [G loss: 2.4496]\n",
            "[Epoch 3/30] [Batch 2400] [G loss: 2.7396]\n",
            "[Epoch 3/30] [Batch 2600] [G loss: 2.5235]\n",
            "[Epoch 3/30] [Batch 2800] [G loss: 1.9586]\n",
            "[Epoch 3/30] [Batch 3000] [G loss: 2.8881]\n",
            "[Epoch 3/30] [Batch 3200] [G loss: 4.2821]\n",
            "[Epoch 3/30] [Batch 3400] [G loss: 2.8763]\n",
            "[Epoch 3/30] [Batch 3600] [G loss: 3.3275]\n",
            "[Epoch 3/30] [Batch 3800] [G loss: 2.8133]\n",
            "[Epoch 3/30] [Batch 4000] [G loss: 2.7306]\n",
            "[Epoch 3/30] [Batch 4200] [G loss: 3.6857]\n",
            "[Epoch 3/30] [Batch 4400] [G loss: 3.3593]\n",
            "[Epoch 3/30] [Batch 4600] [G loss: 3.0477]\n",
            "[Epoch 3/30] [Batch 4800] [G loss: 5.4613]\n",
            "[Epoch 3/30] [Batch 5000] [G loss: 2.7358]\n",
            "[Epoch 3/30] [Batch 5200] [G loss: 2.4369]\n",
            "[Epoch 3/30] [Batch 5400] [G loss: 2.9576]\n",
            "[Epoch 3/30] [Batch 5600] [G loss: 4.5680]\n",
            "[Epoch 3/30] [Batch 5800] [G loss: 2.8392]\n",
            "[Epoch 3/30] [Batch 6000] [G loss: 3.7335]\n",
            "[Epoch 3/30] [Batch 6200] [G loss: 3.9624]\n",
            "[Epoch 3/30] [Batch 6400] [G loss: 3.1363]\n",
            "[Epoch 3/30] [Batch 6600] [G loss: 2.4429]\n",
            "[Epoch 3/30] [Batch 6800] [G loss: 3.0280]\n",
            "[Epoch 3/30] [Batch 7000] [G loss: 2.8764]\n",
            "Images logged for Epoch 3\n",
            "[Epoch 4/30] [Batch 0] [G loss: 4.0377]\n",
            "[Epoch 4/30] [Batch 200] [G loss: 3.3065]\n",
            "[Epoch 4/30] [Batch 400] [G loss: 3.0637]\n",
            "[Epoch 4/30] [Batch 600] [G loss: 2.6647]\n",
            "[Epoch 4/30] [Batch 800] [G loss: 3.3470]\n",
            "[Epoch 4/30] [Batch 1000] [G loss: 2.3566]\n",
            "[Epoch 4/30] [Batch 1200] [G loss: 2.4024]\n",
            "[Epoch 4/30] [Batch 1400] [G loss: 2.9094]\n",
            "[Epoch 4/30] [Batch 1600] [G loss: 2.7930]\n",
            "[Epoch 4/30] [Batch 1800] [G loss: 2.7876]\n",
            "[Epoch 4/30] [Batch 2000] [G loss: 6.0262]\n",
            "[Epoch 4/30] [Batch 2200] [G loss: 2.6433]\n",
            "[Epoch 4/30] [Batch 2400] [G loss: 2.4544]\n",
            "[Epoch 4/30] [Batch 2600] [G loss: 3.7070]\n",
            "[Epoch 4/30] [Batch 2800] [G loss: 3.3837]\n",
            "[Epoch 4/30] [Batch 3000] [G loss: 2.4513]\n",
            "[Epoch 4/30] [Batch 3200] [G loss: 2.5251]\n",
            "[Epoch 4/30] [Batch 3400] [G loss: 2.6035]\n",
            "[Epoch 4/30] [Batch 3600] [G loss: 2.8128]\n",
            "[Epoch 4/30] [Batch 3800] [G loss: 3.2720]\n",
            "[Epoch 4/30] [Batch 4000] [G loss: 2.3772]\n",
            "[Epoch 4/30] [Batch 4200] [G loss: 2.5338]\n",
            "[Epoch 4/30] [Batch 4400] [G loss: 2.4115]\n",
            "[Epoch 4/30] [Batch 4600] [G loss: 2.4318]\n",
            "[Epoch 4/30] [Batch 4800] [G loss: 3.2487]\n",
            "[Epoch 4/30] [Batch 5000] [G loss: 3.1820]\n",
            "[Epoch 4/30] [Batch 5200] [G loss: 2.9176]\n",
            "[Epoch 4/30] [Batch 5400] [G loss: 3.5996]\n",
            "[Epoch 4/30] [Batch 5600] [G loss: 4.2398]\n",
            "[Epoch 4/30] [Batch 5800] [G loss: 2.8984]\n",
            "[Epoch 4/30] [Batch 6000] [G loss: 2.6350]\n",
            "[Epoch 4/30] [Batch 6200] [G loss: 3.1634]\n",
            "[Epoch 4/30] [Batch 6400] [G loss: 2.0525]\n",
            "[Epoch 4/30] [Batch 6600] [G loss: 3.5202]\n",
            "[Epoch 4/30] [Batch 6800] [G loss: 2.5261]\n",
            "[Epoch 4/30] [Batch 7000] [G loss: 2.2921]\n",
            "Images logged for Epoch 4\n",
            "[Epoch 5/30] [Batch 0] [G loss: 2.4465]\n",
            "[Epoch 5/30] [Batch 200] [G loss: 4.2471]\n",
            "[Epoch 5/30] [Batch 400] [G loss: 3.1934]\n",
            "[Epoch 5/30] [Batch 600] [G loss: 4.3585]\n",
            "[Epoch 5/30] [Batch 800] [G loss: 2.7575]\n",
            "[Epoch 5/30] [Batch 1000] [G loss: 3.0244]\n",
            "[Epoch 5/30] [Batch 1200] [G loss: 3.3742]\n",
            "[Epoch 5/30] [Batch 1400] [G loss: 2.7340]\n",
            "[Epoch 5/30] [Batch 1600] [G loss: 2.6573]\n",
            "[Epoch 5/30] [Batch 1800] [G loss: 2.3687]\n",
            "[Epoch 5/30] [Batch 2000] [G loss: 3.1936]\n",
            "[Epoch 5/30] [Batch 2200] [G loss: 2.9029]\n",
            "[Epoch 5/30] [Batch 2400] [G loss: 3.0858]\n",
            "[Epoch 5/30] [Batch 2600] [G loss: 2.6881]\n",
            "[Epoch 5/30] [Batch 2800] [G loss: 3.2467]\n",
            "[Epoch 5/30] [Batch 3000] [G loss: 2.4584]\n",
            "[Epoch 5/30] [Batch 3200] [G loss: 3.4782]\n",
            "[Epoch 5/30] [Batch 3400] [G loss: 2.8002]\n",
            "[Epoch 5/30] [Batch 3600] [G loss: 2.4080]\n",
            "[Epoch 5/30] [Batch 3800] [G loss: 3.5917]\n",
            "[Epoch 5/30] [Batch 4000] [G loss: 3.0132]\n",
            "[Epoch 5/30] [Batch 4200] [G loss: 3.5396]\n",
            "[Epoch 5/30] [Batch 4400] [G loss: 2.8026]\n",
            "[Epoch 5/30] [Batch 4600] [G loss: 2.9038]\n",
            "[Epoch 5/30] [Batch 4800] [G loss: 3.0289]\n",
            "[Epoch 5/30] [Batch 5000] [G loss: 4.1805]\n",
            "[Epoch 5/30] [Batch 5200] [G loss: 3.3702]\n",
            "[Epoch 5/30] [Batch 5400] [G loss: 2.5923]\n",
            "[Epoch 5/30] [Batch 5600] [G loss: 3.1162]\n",
            "[Epoch 5/30] [Batch 5800] [G loss: 3.2546]\n",
            "[Epoch 5/30] [Batch 6000] [G loss: 2.4422]\n",
            "[Epoch 5/30] [Batch 6200] [G loss: 2.9965]\n",
            "[Epoch 5/30] [Batch 6400] [G loss: 3.0036]\n",
            "[Epoch 5/30] [Batch 6600] [G loss: 3.4627]\n",
            "[Epoch 5/30] [Batch 6800] [G loss: 3.2335]\n",
            "[Epoch 5/30] [Batch 7000] [G loss: 3.1742]\n",
            "Images logged for Epoch 5\n",
            "Checkpoint saved: /content/drive/MyDrive/University/Painter_Assignment/checkpoints_aug/epoch_5.pth\n",
            "[Epoch 6/30] [Batch 0] [G loss: 2.1350]\n",
            "[Epoch 6/30] [Batch 200] [G loss: 3.2333]\n",
            "[Epoch 6/30] [Batch 400] [G loss: 3.3385]\n",
            "[Epoch 6/30] [Batch 600] [G loss: 2.6101]\n",
            "[Epoch 6/30] [Batch 800] [G loss: 2.2389]\n",
            "[Epoch 6/30] [Batch 1000] [G loss: 2.4204]\n",
            "[Epoch 6/30] [Batch 1200] [G loss: 2.2696]\n",
            "[Epoch 6/30] [Batch 1400] [G loss: 3.0468]\n",
            "[Epoch 6/30] [Batch 1600] [G loss: 3.3793]\n",
            "[Epoch 6/30] [Batch 1800] [G loss: 3.4537]\n",
            "[Epoch 6/30] [Batch 2000] [G loss: 3.3628]\n",
            "[Epoch 6/30] [Batch 2200] [G loss: 1.9516]\n",
            "[Epoch 6/30] [Batch 2400] [G loss: 3.0810]\n",
            "[Epoch 6/30] [Batch 2600] [G loss: 2.8376]\n",
            "[Epoch 6/30] [Batch 2800] [G loss: 2.3225]\n",
            "[Epoch 6/30] [Batch 3000] [G loss: 2.5995]\n",
            "[Epoch 6/30] [Batch 3200] [G loss: 2.5747]\n",
            "[Epoch 6/30] [Batch 3400] [G loss: 2.2886]\n",
            "[Epoch 6/30] [Batch 3600] [G loss: 2.4412]\n",
            "[Epoch 6/30] [Batch 3800] [G loss: 2.8716]\n",
            "[Epoch 6/30] [Batch 4000] [G loss: 2.4744]\n",
            "[Epoch 6/30] [Batch 4200] [G loss: 2.4573]\n",
            "[Epoch 6/30] [Batch 4400] [G loss: 2.6749]\n",
            "[Epoch 6/30] [Batch 4600] [G loss: 2.3549]\n",
            "[Epoch 6/30] [Batch 4800] [G loss: 2.7412]\n",
            "[Epoch 6/30] [Batch 5000] [G loss: 1.8651]\n",
            "[Epoch 6/30] [Batch 5200] [G loss: 2.6660]\n",
            "[Epoch 6/30] [Batch 5400] [G loss: 3.3246]\n",
            "[Epoch 6/30] [Batch 5600] [G loss: 3.0822]\n",
            "[Epoch 6/30] [Batch 5800] [G loss: 3.4428]\n",
            "[Epoch 6/30] [Batch 6000] [G loss: 1.9670]\n",
            "[Epoch 6/30] [Batch 6200] [G loss: 2.5578]\n",
            "[Epoch 6/30] [Batch 6400] [G loss: 2.8239]\n",
            "[Epoch 6/30] [Batch 6600] [G loss: 3.5945]\n",
            "[Epoch 6/30] [Batch 6800] [G loss: 2.4974]\n",
            "[Epoch 6/30] [Batch 7000] [G loss: 2.4240]\n",
            "Images logged for Epoch 6\n",
            "[Epoch 7/30] [Batch 0] [G loss: 3.2007]\n",
            "[Epoch 7/30] [Batch 200] [G loss: 3.0579]\n",
            "[Epoch 7/30] [Batch 400] [G loss: 2.8338]\n",
            "[Epoch 7/30] [Batch 600] [G loss: 2.5179]\n",
            "[Epoch 7/30] [Batch 800] [G loss: 2.8330]\n",
            "[Epoch 7/30] [Batch 1000] [G loss: 2.5789]\n",
            "[Epoch 7/30] [Batch 1200] [G loss: 2.4537]\n",
            "[Epoch 7/30] [Batch 1400] [G loss: 2.6976]\n",
            "[Epoch 7/30] [Batch 1600] [G loss: 2.9996]\n",
            "[Epoch 7/30] [Batch 1800] [G loss: 2.8478]\n",
            "[Epoch 7/30] [Batch 2000] [G loss: 2.9545]\n",
            "[Epoch 7/30] [Batch 2200] [G loss: 3.4176]\n",
            "[Epoch 7/30] [Batch 2400] [G loss: 2.8096]\n",
            "[Epoch 7/30] [Batch 2600] [G loss: 2.2793]\n",
            "[Epoch 7/30] [Batch 2800] [G loss: 2.8484]\n",
            "[Epoch 7/30] [Batch 3000] [G loss: 2.0964]\n",
            "[Epoch 7/30] [Batch 3200] [G loss: 2.3532]\n",
            "[Epoch 7/30] [Batch 3400] [G loss: 2.5008]\n",
            "[Epoch 7/30] [Batch 3600] [G loss: 3.1272]\n",
            "[Epoch 7/30] [Batch 3800] [G loss: 2.7294]\n",
            "[Epoch 7/30] [Batch 4000] [G loss: 2.5371]\n",
            "[Epoch 7/30] [Batch 4200] [G loss: 2.6439]\n",
            "[Epoch 7/30] [Batch 4400] [G loss: 2.5022]\n",
            "[Epoch 7/30] [Batch 4600] [G loss: 4.2183]\n",
            "[Epoch 7/30] [Batch 4800] [G loss: 3.0414]\n",
            "[Epoch 7/30] [Batch 5000] [G loss: 2.7236]\n",
            "[Epoch 7/30] [Batch 5200] [G loss: 3.3874]\n",
            "[Epoch 7/30] [Batch 5400] [G loss: 3.4920]\n",
            "[Epoch 7/30] [Batch 5600] [G loss: 3.3340]\n",
            "[Epoch 7/30] [Batch 5800] [G loss: 4.7937]\n",
            "[Epoch 7/30] [Batch 6000] [G loss: 3.3290]\n",
            "[Epoch 7/30] [Batch 6200] [G loss: 2.7252]\n",
            "[Epoch 7/30] [Batch 6400] [G loss: 2.8465]\n",
            "[Epoch 7/30] [Batch 6600] [G loss: 3.0275]\n",
            "[Epoch 7/30] [Batch 6800] [G loss: 2.7350]\n",
            "[Epoch 7/30] [Batch 7000] [G loss: 1.7877]\n",
            "Images logged for Epoch 7\n",
            "[Epoch 8/30] [Batch 0] [G loss: 2.1695]\n",
            "[Epoch 8/30] [Batch 200] [G loss: 3.0196]\n",
            "[Epoch 8/30] [Batch 400] [G loss: 2.4583]\n",
            "[Epoch 8/30] [Batch 600] [G loss: 1.7820]\n",
            "[Epoch 8/30] [Batch 800] [G loss: 2.6529]\n",
            "[Epoch 8/30] [Batch 1000] [G loss: 2.3636]\n",
            "[Epoch 8/30] [Batch 1200] [G loss: 2.3486]\n",
            "[Epoch 8/30] [Batch 1400] [G loss: 2.7859]\n",
            "[Epoch 8/30] [Batch 1600] [G loss: 2.6104]\n",
            "[Epoch 8/30] [Batch 1800] [G loss: 2.8944]\n",
            "[Epoch 8/30] [Batch 2000] [G loss: 2.4363]\n",
            "[Epoch 8/30] [Batch 2200] [G loss: 2.6956]\n",
            "[Epoch 8/30] [Batch 2400] [G loss: 2.5516]\n",
            "[Epoch 8/30] [Batch 2600] [G loss: 1.9469]\n",
            "[Epoch 8/30] [Batch 2800] [G loss: 2.5267]\n",
            "[Epoch 8/30] [Batch 3000] [G loss: 2.2408]\n",
            "[Epoch 8/30] [Batch 3200] [G loss: 2.6162]\n",
            "[Epoch 8/30] [Batch 3400] [G loss: 2.7479]\n",
            "[Epoch 8/30] [Batch 3600] [G loss: 2.6122]\n",
            "[Epoch 8/30] [Batch 3800] [G loss: 2.4393]\n",
            "[Epoch 8/30] [Batch 4000] [G loss: 2.3282]\n",
            "[Epoch 8/30] [Batch 4200] [G loss: 2.5809]\n",
            "[Epoch 8/30] [Batch 4400] [G loss: 2.6869]\n",
            "[Epoch 8/30] [Batch 4600] [G loss: 2.6855]\n",
            "[Epoch 8/30] [Batch 4800] [G loss: 3.0067]\n",
            "[Epoch 8/30] [Batch 5000] [G loss: 2.9717]\n",
            "[Epoch 8/30] [Batch 5200] [G loss: 2.3414]\n",
            "[Epoch 8/30] [Batch 5400] [G loss: 2.7780]\n",
            "[Epoch 8/30] [Batch 5600] [G loss: 3.4258]\n",
            "[Epoch 8/30] [Batch 5800] [G loss: 2.7102]\n",
            "[Epoch 8/30] [Batch 6000] [G loss: 2.8121]\n",
            "[Epoch 8/30] [Batch 6200] [G loss: 3.0048]\n",
            "[Epoch 8/30] [Batch 6400] [G loss: 2.7309]\n",
            "[Epoch 8/30] [Batch 6600] [G loss: 3.1245]\n",
            "[Epoch 8/30] [Batch 6800] [G loss: 2.2420]\n",
            "[Epoch 8/30] [Batch 7000] [G loss: 2.4751]\n",
            "Images logged for Epoch 8\n",
            "[Epoch 9/30] [Batch 0] [G loss: 3.0808]\n",
            "[Epoch 9/30] [Batch 200] [G loss: 3.3735]\n",
            "[Epoch 9/30] [Batch 400] [G loss: 3.0424]\n",
            "[Epoch 9/30] [Batch 600] [G loss: 2.9013]\n",
            "[Epoch 9/30] [Batch 800] [G loss: 2.7644]\n",
            "[Epoch 9/30] [Batch 1000] [G loss: 2.2677]\n",
            "[Epoch 9/30] [Batch 1200] [G loss: 2.1507]\n",
            "[Epoch 9/30] [Batch 1400] [G loss: 2.5398]\n",
            "[Epoch 9/30] [Batch 1600] [G loss: 3.4930]\n",
            "[Epoch 9/30] [Batch 1800] [G loss: 2.4249]\n",
            "[Epoch 9/30] [Batch 2000] [G loss: 2.9876]\n",
            "[Epoch 9/30] [Batch 2200] [G loss: 3.1175]\n",
            "[Epoch 9/30] [Batch 2400] [G loss: 3.6984]\n",
            "[Epoch 9/30] [Batch 2600] [G loss: 2.1127]\n",
            "[Epoch 9/30] [Batch 2800] [G loss: 2.2116]\n",
            "[Epoch 9/30] [Batch 3000] [G loss: 2.3348]\n",
            "[Epoch 9/30] [Batch 3200] [G loss: 2.3578]\n",
            "[Epoch 9/30] [Batch 3400] [G loss: 2.7543]\n",
            "[Epoch 9/30] [Batch 3600] [G loss: 2.9662]\n",
            "[Epoch 9/30] [Batch 3800] [G loss: 1.8499]\n",
            "[Epoch 9/30] [Batch 4000] [G loss: 4.1908]\n",
            "[Epoch 9/30] [Batch 4200] [G loss: 2.6205]\n",
            "[Epoch 9/30] [Batch 4400] [G loss: 2.8619]\n",
            "[Epoch 9/30] [Batch 4600] [G loss: 2.2047]\n",
            "[Epoch 9/30] [Batch 4800] [G loss: 3.0753]\n",
            "[Epoch 9/30] [Batch 5000] [G loss: 3.0263]\n",
            "[Epoch 9/30] [Batch 5200] [G loss: 2.1448]\n",
            "[Epoch 9/30] [Batch 5400] [G loss: 2.7229]\n",
            "[Epoch 9/30] [Batch 5600] [G loss: 2.7690]\n",
            "[Epoch 9/30] [Batch 5800] [G loss: 2.6335]\n",
            "[Epoch 9/30] [Batch 6000] [G loss: 3.0002]\n",
            "[Epoch 9/30] [Batch 6200] [G loss: 2.0449]\n",
            "[Epoch 9/30] [Batch 6400] [G loss: 3.7230]\n",
            "[Epoch 9/30] [Batch 6600] [G loss: 2.8299]\n",
            "[Epoch 9/30] [Batch 6800] [G loss: 1.7799]\n",
            "[Epoch 9/30] [Batch 7000] [G loss: 2.8039]\n",
            "Images logged for Epoch 9\n",
            "[Epoch 10/30] [Batch 0] [G loss: 2.5049]\n",
            "[Epoch 10/30] [Batch 200] [G loss: 4.0070]\n",
            "[Epoch 10/30] [Batch 400] [G loss: 2.0194]\n",
            "[Epoch 10/30] [Batch 600] [G loss: 2.8324]\n",
            "[Epoch 10/30] [Batch 800] [G loss: 2.4950]\n",
            "[Epoch 10/30] [Batch 1000] [G loss: 2.7996]\n",
            "[Epoch 10/30] [Batch 1200] [G loss: 3.5273]\n",
            "[Epoch 10/30] [Batch 1400] [G loss: 2.6084]\n",
            "[Epoch 10/30] [Batch 1600] [G loss: 2.3143]\n",
            "[Epoch 10/30] [Batch 1800] [G loss: 2.1346]\n",
            "[Epoch 10/30] [Batch 2000] [G loss: 3.4584]\n",
            "[Epoch 10/30] [Batch 2200] [G loss: 2.7653]\n",
            "[Epoch 10/30] [Batch 2400] [G loss: 2.5948]\n",
            "[Epoch 10/30] [Batch 2600] [G loss: 2.2468]\n",
            "[Epoch 10/30] [Batch 2800] [G loss: 2.7832]\n",
            "[Epoch 10/30] [Batch 3000] [G loss: 2.4819]\n",
            "[Epoch 10/30] [Batch 3200] [G loss: 2.8191]\n",
            "[Epoch 10/30] [Batch 3400] [G loss: 2.8483]\n",
            "[Epoch 10/30] [Batch 3600] [G loss: 2.4802]\n",
            "[Epoch 10/30] [Batch 3800] [G loss: 3.1173]\n",
            "[Epoch 10/30] [Batch 4000] [G loss: 2.1720]\n",
            "[Epoch 10/30] [Batch 4200] [G loss: 2.2585]\n",
            "[Epoch 10/30] [Batch 4400] [G loss: 2.8496]\n",
            "[Epoch 10/30] [Batch 4600] [G loss: 2.8001]\n",
            "[Epoch 10/30] [Batch 4800] [G loss: 3.1941]\n",
            "[Epoch 10/30] [Batch 5000] [G loss: 2.2961]\n",
            "[Epoch 10/30] [Batch 5200] [G loss: 2.0448]\n",
            "[Epoch 10/30] [Batch 5400] [G loss: 2.5189]\n",
            "[Epoch 10/30] [Batch 5600] [G loss: 2.4223]\n",
            "[Epoch 10/30] [Batch 5800] [G loss: 2.4695]\n",
            "[Epoch 10/30] [Batch 6000] [G loss: 2.2769]\n",
            "[Epoch 10/30] [Batch 6200] [G loss: 3.4510]\n",
            "[Epoch 10/30] [Batch 6400] [G loss: 2.0795]\n",
            "[Epoch 10/30] [Batch 6600] [G loss: 4.1955]\n",
            "[Epoch 10/30] [Batch 6800] [G loss: 3.0017]\n",
            "[Epoch 10/30] [Batch 7000] [G loss: 2.1963]\n",
            "Images logged for Epoch 10\n",
            "Checkpoint saved: /content/drive/MyDrive/University/Painter_Assignment/checkpoints_aug/epoch_10.pth\n",
            "[Epoch 11/30] [Batch 0] [G loss: 2.4543]\n",
            "[Epoch 11/30] [Batch 200] [G loss: 2.5868]\n",
            "[Epoch 11/30] [Batch 400] [G loss: 2.1902]\n",
            "[Epoch 11/30] [Batch 600] [G loss: 3.2739]\n",
            "[Epoch 11/30] [Batch 800] [G loss: 2.5421]\n",
            "[Epoch 11/30] [Batch 1000] [G loss: 2.2176]\n",
            "[Epoch 11/30] [Batch 1200] [G loss: 3.5679]\n",
            "[Epoch 11/30] [Batch 1400] [G loss: 2.6196]\n",
            "[Epoch 11/30] [Batch 1600] [G loss: 3.1473]\n",
            "[Epoch 11/30] [Batch 1800] [G loss: 2.9034]\n",
            "[Epoch 11/30] [Batch 2000] [G loss: 2.3273]\n",
            "[Epoch 11/30] [Batch 2200] [G loss: 2.7144]\n",
            "[Epoch 11/30] [Batch 2400] [G loss: 3.1940]\n",
            "[Epoch 11/30] [Batch 2600] [G loss: 2.5520]\n",
            "[Epoch 11/30] [Batch 2800] [G loss: 2.3258]\n",
            "[Epoch 11/30] [Batch 3000] [G loss: 2.9052]\n",
            "[Epoch 11/30] [Batch 3200] [G loss: 2.1271]\n",
            "[Epoch 11/30] [Batch 3400] [G loss: 2.5123]\n",
            "[Epoch 11/30] [Batch 3600] [G loss: 2.6441]\n",
            "[Epoch 11/30] [Batch 3800] [G loss: 2.6411]\n",
            "[Epoch 11/30] [Batch 4000] [G loss: 2.9881]\n",
            "[Epoch 11/30] [Batch 4200] [G loss: 2.5789]\n",
            "[Epoch 11/30] [Batch 4400] [G loss: 2.5374]\n",
            "[Epoch 11/30] [Batch 4600] [G loss: 2.5453]\n",
            "[Epoch 11/30] [Batch 4800] [G loss: 2.1383]\n",
            "[Epoch 11/30] [Batch 5000] [G loss: 3.1331]\n",
            "[Epoch 11/30] [Batch 5200] [G loss: 3.1409]\n",
            "[Epoch 11/30] [Batch 5400] [G loss: 3.1948]\n",
            "[Epoch 11/30] [Batch 5600] [G loss: 2.2537]\n",
            "[Epoch 11/30] [Batch 5800] [G loss: 2.7745]\n",
            "[Epoch 11/30] [Batch 6000] [G loss: 2.8064]\n",
            "[Epoch 11/30] [Batch 6200] [G loss: 2.7404]\n",
            "[Epoch 11/30] [Batch 6400] [G loss: 3.1827]\n",
            "[Epoch 11/30] [Batch 6600] [G loss: 2.7955]\n",
            "[Epoch 11/30] [Batch 6800] [G loss: 3.0214]\n",
            "[Epoch 11/30] [Batch 7000] [G loss: 3.6281]\n",
            "Images logged for Epoch 11\n",
            "[Epoch 12/30] [Batch 0] [G loss: 3.4645]\n",
            "[Epoch 12/30] [Batch 200] [G loss: 2.5352]\n",
            "[Epoch 12/30] [Batch 400] [G loss: 2.3216]\n",
            "[Epoch 12/30] [Batch 600] [G loss: 2.4287]\n",
            "[Epoch 12/30] [Batch 800] [G loss: 2.9363]\n",
            "[Epoch 12/30] [Batch 1000] [G loss: 2.9828]\n",
            "[Epoch 12/30] [Batch 1200] [G loss: 2.8634]\n",
            "[Epoch 12/30] [Batch 1400] [G loss: 3.6067]\n",
            "[Epoch 12/30] [Batch 1600] [G loss: 2.0697]\n",
            "[Epoch 12/30] [Batch 1800] [G loss: 2.5244]\n",
            "[Epoch 12/30] [Batch 2000] [G loss: 3.0847]\n",
            "[Epoch 12/30] [Batch 2200] [G loss: 3.2170]\n",
            "[Epoch 12/30] [Batch 2400] [G loss: 2.3656]\n",
            "[Epoch 12/30] [Batch 2600] [G loss: 2.5817]\n",
            "[Epoch 12/30] [Batch 2800] [G loss: 2.3748]\n",
            "[Epoch 12/30] [Batch 3000] [G loss: 2.3001]\n",
            "[Epoch 12/30] [Batch 3200] [G loss: 2.1922]\n",
            "[Epoch 12/30] [Batch 3400] [G loss: 2.9144]\n",
            "[Epoch 12/30] [Batch 3600] [G loss: 2.3298]\n",
            "[Epoch 12/30] [Batch 3800] [G loss: 2.0896]\n",
            "[Epoch 12/30] [Batch 4000] [G loss: 2.2903]\n",
            "[Epoch 12/30] [Batch 4200] [G loss: 2.2038]\n",
            "[Epoch 12/30] [Batch 4400] [G loss: 2.6020]\n",
            "[Epoch 12/30] [Batch 4600] [G loss: 3.2627]\n",
            "[Epoch 12/30] [Batch 4800] [G loss: 4.0196]\n",
            "[Epoch 12/30] [Batch 5000] [G loss: 2.6851]\n",
            "[Epoch 12/30] [Batch 5200] [G loss: 2.7387]\n",
            "[Epoch 12/30] [Batch 5400] [G loss: 2.4590]\n",
            "[Epoch 12/30] [Batch 5600] [G loss: 2.7299]\n",
            "[Epoch 12/30] [Batch 5800] [G loss: 1.8922]\n",
            "[Epoch 12/30] [Batch 6000] [G loss: 2.4592]\n",
            "[Epoch 12/30] [Batch 6200] [G loss: 3.4431]\n",
            "[Epoch 12/30] [Batch 6400] [G loss: 2.4390]\n",
            "[Epoch 12/30] [Batch 6600] [G loss: 3.5101]\n",
            "[Epoch 12/30] [Batch 6800] [G loss: 3.1317]\n",
            "[Epoch 12/30] [Batch 7000] [G loss: 2.4115]\n",
            "Images logged for Epoch 12\n",
            "[Epoch 13/30] [Batch 0] [G loss: 2.4442]\n",
            "[Epoch 13/30] [Batch 200] [G loss: 3.0428]\n",
            "[Epoch 13/30] [Batch 400] [G loss: 2.6971]\n",
            "[Epoch 13/30] [Batch 600] [G loss: 3.4300]\n",
            "[Epoch 13/30] [Batch 800] [G loss: 2.7578]\n",
            "[Epoch 13/30] [Batch 1000] [G loss: 3.6553]\n",
            "[Epoch 13/30] [Batch 1200] [G loss: 2.4890]\n",
            "[Epoch 13/30] [Batch 1400] [G loss: 2.5388]\n",
            "[Epoch 13/30] [Batch 1600] [G loss: 2.6311]\n",
            "[Epoch 13/30] [Batch 1800] [G loss: 2.4677]\n",
            "[Epoch 13/30] [Batch 2000] [G loss: 3.1410]\n",
            "[Epoch 13/30] [Batch 2200] [G loss: 3.4856]\n",
            "[Epoch 13/30] [Batch 2400] [G loss: 2.4313]\n",
            "[Epoch 13/30] [Batch 2600] [G loss: 2.4097]\n",
            "[Epoch 13/30] [Batch 2800] [G loss: 2.6293]\n",
            "[Epoch 13/30] [Batch 3000] [G loss: 2.4092]\n",
            "[Epoch 13/30] [Batch 3200] [G loss: 2.4566]\n",
            "[Epoch 13/30] [Batch 3400] [G loss: 2.5162]\n",
            "[Epoch 13/30] [Batch 3600] [G loss: 2.4138]\n",
            "[Epoch 13/30] [Batch 3800] [G loss: 2.8068]\n",
            "[Epoch 13/30] [Batch 4000] [G loss: 2.0723]\n",
            "[Epoch 13/30] [Batch 4200] [G loss: 2.4781]\n",
            "[Epoch 13/30] [Batch 4400] [G loss: 2.2256]\n",
            "[Epoch 13/30] [Batch 4600] [G loss: 2.9659]\n",
            "[Epoch 13/30] [Batch 4800] [G loss: 2.1481]\n",
            "[Epoch 13/30] [Batch 5000] [G loss: 2.7802]\n",
            "[Epoch 13/30] [Batch 5200] [G loss: 2.7301]\n",
            "[Epoch 13/30] [Batch 5400] [G loss: 2.1399]\n",
            "[Epoch 13/30] [Batch 5600] [G loss: 2.3368]\n",
            "[Epoch 13/30] [Batch 5800] [G loss: 2.6766]\n",
            "[Epoch 13/30] [Batch 6000] [G loss: 2.2521]\n",
            "[Epoch 13/30] [Batch 6200] [G loss: 4.3134]\n",
            "[Epoch 13/30] [Batch 6400] [G loss: 2.5607]\n",
            "[Epoch 13/30] [Batch 6600] [G loss: 2.5889]\n",
            "[Epoch 13/30] [Batch 6800] [G loss: 2.5772]\n",
            "[Epoch 13/30] [Batch 7000] [G loss: 1.9507]\n",
            "Images logged for Epoch 13\n",
            "[Epoch 14/30] [Batch 0] [G loss: 2.9745]\n",
            "[Epoch 14/30] [Batch 200] [G loss: 2.5394]\n",
            "[Epoch 14/30] [Batch 400] [G loss: 3.6361]\n",
            "[Epoch 14/30] [Batch 600] [G loss: 2.5862]\n",
            "[Epoch 14/30] [Batch 800] [G loss: 2.2457]\n",
            "[Epoch 14/30] [Batch 1000] [G loss: 3.1058]\n",
            "[Epoch 14/30] [Batch 1200] [G loss: 2.8796]\n",
            "[Epoch 14/30] [Batch 1400] [G loss: 2.2643]\n",
            "[Epoch 14/30] [Batch 1600] [G loss: 3.4350]\n",
            "[Epoch 14/30] [Batch 1800] [G loss: 2.9857]\n",
            "[Epoch 14/30] [Batch 2000] [G loss: 2.8734]\n",
            "[Epoch 14/30] [Batch 2200] [G loss: 2.1860]\n",
            "[Epoch 14/30] [Batch 2400] [G loss: 2.2058]\n",
            "[Epoch 14/30] [Batch 2600] [G loss: 3.0512]\n",
            "[Epoch 14/30] [Batch 2800] [G loss: 2.1858]\n",
            "[Epoch 14/30] [Batch 3000] [G loss: 2.1325]\n",
            "[Epoch 14/30] [Batch 3200] [G loss: 2.7967]\n",
            "[Epoch 14/30] [Batch 3400] [G loss: 1.9622]\n",
            "[Epoch 14/30] [Batch 3600] [G loss: 2.2618]\n",
            "[Epoch 14/30] [Batch 3800] [G loss: 2.4069]\n",
            "[Epoch 14/30] [Batch 4000] [G loss: 2.2824]\n",
            "[Epoch 14/30] [Batch 4200] [G loss: 2.5498]\n",
            "[Epoch 14/30] [Batch 4400] [G loss: 2.2520]\n",
            "[Epoch 14/30] [Batch 4600] [G loss: 2.7792]\n",
            "[Epoch 14/30] [Batch 4800] [G loss: 1.9323]\n",
            "[Epoch 14/30] [Batch 5000] [G loss: 2.7137]\n",
            "[Epoch 14/30] [Batch 5200] [G loss: 2.5321]\n",
            "[Epoch 14/30] [Batch 5400] [G loss: 2.9545]\n",
            "[Epoch 14/30] [Batch 5600] [G loss: 2.7689]\n",
            "[Epoch 14/30] [Batch 5800] [G loss: 1.6746]\n",
            "[Epoch 14/30] [Batch 6000] [G loss: 2.8747]\n",
            "[Epoch 14/30] [Batch 6200] [G loss: 2.0348]\n",
            "[Epoch 14/30] [Batch 6400] [G loss: 2.2113]\n",
            "[Epoch 14/30] [Batch 6600] [G loss: 1.9927]\n",
            "[Epoch 14/30] [Batch 6800] [G loss: 3.0881]\n",
            "[Epoch 14/30] [Batch 7000] [G loss: 2.7729]\n",
            "Images logged for Epoch 14\n",
            "[Epoch 15/30] [Batch 0] [G loss: 2.5527]\n",
            "[Epoch 15/30] [Batch 200] [G loss: 2.3054]\n",
            "[Epoch 15/30] [Batch 400] [G loss: 2.2456]\n",
            "[Epoch 15/30] [Batch 600] [G loss: 2.3236]\n",
            "[Epoch 15/30] [Batch 800] [G loss: 2.9699]\n",
            "[Epoch 15/30] [Batch 1000] [G loss: 2.0047]\n",
            "[Epoch 15/30] [Batch 1200] [G loss: 3.5090]\n",
            "[Epoch 15/30] [Batch 1400] [G loss: 2.1501]\n",
            "[Epoch 15/30] [Batch 1600] [G loss: 3.0995]\n",
            "[Epoch 15/30] [Batch 1800] [G loss: 2.7077]\n",
            "[Epoch 15/30] [Batch 2000] [G loss: 2.8114]\n",
            "[Epoch 15/30] [Batch 2200] [G loss: 2.3037]\n",
            "[Epoch 15/30] [Batch 2400] [G loss: 2.4057]\n",
            "[Epoch 15/30] [Batch 2600] [G loss: 2.4393]\n",
            "[Epoch 15/30] [Batch 2800] [G loss: 2.2095]\n",
            "[Epoch 15/30] [Batch 3000] [G loss: 2.4267]\n",
            "[Epoch 15/30] [Batch 3200] [G loss: 2.1389]\n",
            "[Epoch 15/30] [Batch 3400] [G loss: 2.6391]\n",
            "[Epoch 15/30] [Batch 3600] [G loss: 2.8385]\n",
            "[Epoch 15/30] [Batch 3800] [G loss: 2.8589]\n",
            "[Epoch 15/30] [Batch 4000] [G loss: 2.3740]\n",
            "[Epoch 15/30] [Batch 4200] [G loss: 2.7205]\n",
            "[Epoch 15/30] [Batch 4400] [G loss: 2.5213]\n",
            "[Epoch 15/30] [Batch 4600] [G loss: 2.3303]\n",
            "[Epoch 15/30] [Batch 4800] [G loss: 2.3635]\n",
            "[Epoch 15/30] [Batch 5000] [G loss: 2.4817]\n",
            "[Epoch 15/30] [Batch 5200] [G loss: 2.6121]\n",
            "[Epoch 15/30] [Batch 5400] [G loss: 2.7262]\n",
            "[Epoch 15/30] [Batch 5600] [G loss: 2.1315]\n",
            "[Epoch 15/30] [Batch 5800] [G loss: 2.3778]\n",
            "[Epoch 15/30] [Batch 6000] [G loss: 2.7217]\n",
            "[Epoch 15/30] [Batch 6200] [G loss: 2.0719]\n",
            "[Epoch 15/30] [Batch 6400] [G loss: 2.1839]\n",
            "[Epoch 15/30] [Batch 6600] [G loss: 2.7406]\n",
            "[Epoch 15/30] [Batch 6800] [G loss: 2.1917]\n",
            "[Epoch 15/30] [Batch 7000] [G loss: 2.0232]\n",
            "Images logged for Epoch 15\n",
            "Checkpoint saved: /content/drive/MyDrive/University/Painter_Assignment/checkpoints_aug/epoch_15.pth\n",
            "[Epoch 16/30] [Batch 0] [G loss: 2.4520]\n",
            "[Epoch 16/30] [Batch 200] [G loss: 2.1045]\n",
            "[Epoch 16/30] [Batch 400] [G loss: 2.0101]\n",
            "[Epoch 16/30] [Batch 600] [G loss: 2.7084]\n",
            "[Epoch 16/30] [Batch 800] [G loss: 3.4578]\n",
            "[Epoch 16/30] [Batch 1000] [G loss: 1.8929]\n",
            "[Epoch 16/30] [Batch 1200] [G loss: 2.5340]\n",
            "[Epoch 16/30] [Batch 1400] [G loss: 2.2505]\n",
            "[Epoch 16/30] [Batch 1600] [G loss: 2.9081]\n",
            "[Epoch 16/30] [Batch 1800] [G loss: 2.0063]\n",
            "[Epoch 16/30] [Batch 2000] [G loss: 2.2517]\n",
            "[Epoch 16/30] [Batch 2200] [G loss: 2.5805]\n",
            "[Epoch 16/30] [Batch 2400] [G loss: 2.8641]\n",
            "[Epoch 16/30] [Batch 2600] [G loss: 2.0747]\n",
            "[Epoch 16/30] [Batch 2800] [G loss: 3.0761]\n",
            "[Epoch 16/30] [Batch 3000] [G loss: 2.4998]\n",
            "[Epoch 16/30] [Batch 3200] [G loss: 2.4901]\n",
            "[Epoch 16/30] [Batch 3400] [G loss: 3.1694]\n",
            "[Epoch 16/30] [Batch 3600] [G loss: 2.3998]\n",
            "[Epoch 16/30] [Batch 3800] [G loss: 4.0311]\n",
            "[Epoch 16/30] [Batch 4000] [G loss: 2.9546]\n",
            "[Epoch 16/30] [Batch 4200] [G loss: 2.2458]\n",
            "[Epoch 16/30] [Batch 4400] [G loss: 2.2308]\n",
            "[Epoch 16/30] [Batch 4600] [G loss: 1.8648]\n",
            "[Epoch 16/30] [Batch 4800] [G loss: 2.5920]\n",
            "[Epoch 16/30] [Batch 5000] [G loss: 2.7181]\n",
            "[Epoch 16/30] [Batch 5200] [G loss: 2.2816]\n",
            "[Epoch 16/30] [Batch 5400] [G loss: 2.3675]\n",
            "[Epoch 16/30] [Batch 5600] [G loss: 2.6642]\n",
            "[Epoch 16/30] [Batch 5800] [G loss: 1.8628]\n",
            "[Epoch 16/30] [Batch 6000] [G loss: 2.2106]\n",
            "[Epoch 16/30] [Batch 6200] [G loss: 2.3948]\n",
            "[Epoch 16/30] [Batch 6400] [G loss: 2.8015]\n",
            "[Epoch 16/30] [Batch 6600] [G loss: 2.7238]\n",
            "[Epoch 16/30] [Batch 6800] [G loss: 2.6002]\n",
            "[Epoch 16/30] [Batch 7000] [G loss: 2.2345]\n",
            "Images logged for Epoch 16\n",
            "[Epoch 17/30] [Batch 0] [G loss: 2.3088]\n",
            "[Epoch 17/30] [Batch 200] [G loss: 2.5985]\n",
            "[Epoch 17/30] [Batch 400] [G loss: 2.4962]\n",
            "[Epoch 17/30] [Batch 600] [G loss: 1.9579]\n",
            "[Epoch 17/30] [Batch 800] [G loss: 2.2823]\n",
            "[Epoch 17/30] [Batch 1000] [G loss: 2.1831]\n",
            "[Epoch 17/30] [Batch 1200] [G loss: 2.5287]\n",
            "[Epoch 17/30] [Batch 1400] [G loss: 2.5376]\n",
            "[Epoch 17/30] [Batch 1600] [G loss: 2.1765]\n",
            "[Epoch 17/30] [Batch 1800] [G loss: 2.0786]\n",
            "[Epoch 17/30] [Batch 2000] [G loss: 2.2784]\n",
            "[Epoch 17/30] [Batch 2200] [G loss: 1.8415]\n",
            "[Epoch 17/30] [Batch 2400] [G loss: 1.7879]\n",
            "[Epoch 17/30] [Batch 2600] [G loss: 2.3429]\n",
            "[Epoch 17/30] [Batch 2800] [G loss: 1.8752]\n",
            "[Epoch 17/30] [Batch 3000] [G loss: 2.4489]\n",
            "[Epoch 17/30] [Batch 3200] [G loss: 2.5312]\n",
            "[Epoch 17/30] [Batch 3400] [G loss: 1.6974]\n",
            "[Epoch 17/30] [Batch 3600] [G loss: 3.7228]\n",
            "[Epoch 17/30] [Batch 3800] [G loss: 1.9697]\n",
            "[Epoch 17/30] [Batch 4000] [G loss: 2.6399]\n",
            "[Epoch 17/30] [Batch 4200] [G loss: 2.5060]\n",
            "[Epoch 17/30] [Batch 4400] [G loss: 2.2626]\n",
            "[Epoch 17/30] [Batch 4600] [G loss: 2.5543]\n",
            "[Epoch 17/30] [Batch 4800] [G loss: 2.5969]\n",
            "[Epoch 17/30] [Batch 5000] [G loss: 2.3330]\n",
            "[Epoch 17/30] [Batch 5200] [G loss: 2.4217]\n",
            "[Epoch 17/30] [Batch 5400] [G loss: 2.2035]\n",
            "[Epoch 17/30] [Batch 5600] [G loss: 2.3092]\n",
            "[Epoch 17/30] [Batch 5800] [G loss: 1.9805]\n",
            "[Epoch 17/30] [Batch 6000] [G loss: 2.3342]\n",
            "[Epoch 17/30] [Batch 6200] [G loss: 2.4032]\n",
            "[Epoch 17/30] [Batch 6400] [G loss: 2.2045]\n",
            "[Epoch 17/30] [Batch 6600] [G loss: 2.3229]\n",
            "[Epoch 17/30] [Batch 6800] [G loss: 2.1503]\n",
            "[Epoch 17/30] [Batch 7000] [G loss: 2.5847]\n",
            "Images logged for Epoch 17\n",
            "[Epoch 18/30] [Batch 0] [G loss: 2.1354]\n",
            "[Epoch 18/30] [Batch 200] [G loss: 2.6898]\n",
            "[Epoch 18/30] [Batch 400] [G loss: 2.8321]\n",
            "[Epoch 18/30] [Batch 600] [G loss: 2.0721]\n",
            "[Epoch 18/30] [Batch 800] [G loss: 2.0312]\n",
            "[Epoch 18/30] [Batch 1000] [G loss: 2.4371]\n",
            "[Epoch 18/30] [Batch 1200] [G loss: 1.9951]\n",
            "[Epoch 18/30] [Batch 1400] [G loss: 1.9106]\n",
            "[Epoch 18/30] [Batch 1600] [G loss: 2.4232]\n",
            "[Epoch 18/30] [Batch 1800] [G loss: 2.2611]\n",
            "[Epoch 18/30] [Batch 2000] [G loss: 2.2344]\n",
            "[Epoch 18/30] [Batch 2200] [G loss: 2.0002]\n",
            "[Epoch 18/30] [Batch 2400] [G loss: 1.9848]\n",
            "[Epoch 18/30] [Batch 2600] [G loss: 3.3066]\n",
            "[Epoch 18/30] [Batch 2800] [G loss: 3.3214]\n",
            "[Epoch 18/30] [Batch 3000] [G loss: 2.5691]\n",
            "[Epoch 18/30] [Batch 3200] [G loss: 3.5342]\n",
            "[Epoch 18/30] [Batch 3400] [G loss: 2.5044]\n",
            "[Epoch 18/30] [Batch 3600] [G loss: 2.4563]\n",
            "[Epoch 18/30] [Batch 3800] [G loss: 2.4579]\n",
            "[Epoch 18/30] [Batch 4000] [G loss: 2.4935]\n",
            "[Epoch 18/30] [Batch 4200] [G loss: 3.0480]\n",
            "[Epoch 18/30] [Batch 4400] [G loss: 2.5304]\n",
            "[Epoch 18/30] [Batch 4600] [G loss: 2.1262]\n",
            "[Epoch 18/30] [Batch 4800] [G loss: 2.4823]\n",
            "[Epoch 18/30] [Batch 5000] [G loss: 1.7986]\n",
            "[Epoch 18/30] [Batch 5200] [G loss: 2.8393]\n",
            "[Epoch 18/30] [Batch 5400] [G loss: 2.4298]\n",
            "[Epoch 18/30] [Batch 5600] [G loss: 1.9673]\n",
            "[Epoch 18/30] [Batch 5800] [G loss: 2.1642]\n",
            "[Epoch 18/30] [Batch 6000] [G loss: 2.2302]\n",
            "[Epoch 18/30] [Batch 6200] [G loss: 3.4463]\n",
            "[Epoch 18/30] [Batch 6400] [G loss: 2.7528]\n",
            "[Epoch 18/30] [Batch 6600] [G loss: 2.2952]\n",
            "[Epoch 18/30] [Batch 6800] [G loss: 2.4294]\n",
            "[Epoch 18/30] [Batch 7000] [G loss: 2.7143]\n",
            "Images logged for Epoch 18\n",
            "[Epoch 19/30] [Batch 0] [G loss: 1.6524]\n",
            "[Epoch 19/30] [Batch 200] [G loss: 3.1363]\n",
            "[Epoch 19/30] [Batch 400] [G loss: 2.3567]\n",
            "[Epoch 19/30] [Batch 600] [G loss: 2.1853]\n",
            "[Epoch 19/30] [Batch 800] [G loss: 2.0238]\n",
            "[Epoch 19/30] [Batch 1000] [G loss: 3.2686]\n",
            "[Epoch 19/30] [Batch 1200] [G loss: 1.8908]\n",
            "[Epoch 19/30] [Batch 1400] [G loss: 2.9655]\n",
            "[Epoch 19/30] [Batch 1600] [G loss: 1.9503]\n",
            "[Epoch 19/30] [Batch 1800] [G loss: 2.0488]\n",
            "[Epoch 19/30] [Batch 2000] [G loss: 2.1752]\n",
            "[Epoch 19/30] [Batch 2200] [G loss: 2.6957]\n",
            "[Epoch 19/30] [Batch 2400] [G loss: 2.1568]\n",
            "[Epoch 19/30] [Batch 2600] [G loss: 2.3387]\n",
            "[Epoch 19/30] [Batch 2800] [G loss: 2.3256]\n",
            "[Epoch 19/30] [Batch 3000] [G loss: 2.1995]\n",
            "[Epoch 19/30] [Batch 3200] [G loss: 2.9899]\n",
            "[Epoch 19/30] [Batch 3400] [G loss: 2.0507]\n",
            "[Epoch 19/30] [Batch 3600] [G loss: 2.4389]\n",
            "[Epoch 19/30] [Batch 3800] [G loss: 2.4992]\n",
            "[Epoch 19/30] [Batch 4000] [G loss: 2.3218]\n",
            "[Epoch 19/30] [Batch 4200] [G loss: 2.7341]\n",
            "[Epoch 19/30] [Batch 4400] [G loss: 2.1146]\n",
            "[Epoch 19/30] [Batch 4600] [G loss: 1.9324]\n",
            "[Epoch 19/30] [Batch 4800] [G loss: 2.3808]\n",
            "[Epoch 19/30] [Batch 5000] [G loss: 2.6788]\n",
            "[Epoch 19/30] [Batch 5200] [G loss: 1.7815]\n",
            "[Epoch 19/30] [Batch 5400] [G loss: 2.3109]\n",
            "[Epoch 19/30] [Batch 5600] [G loss: 1.9296]\n",
            "[Epoch 19/30] [Batch 5800] [G loss: 2.7216]\n",
            "[Epoch 19/30] [Batch 6000] [G loss: 2.3629]\n",
            "[Epoch 19/30] [Batch 6200] [G loss: 2.5068]\n",
            "[Epoch 19/30] [Batch 6400] [G loss: 2.3209]\n",
            "[Epoch 19/30] [Batch 6600] [G loss: 2.5651]\n",
            "[Epoch 19/30] [Batch 6800] [G loss: 2.7869]\n",
            "[Epoch 19/30] [Batch 7000] [G loss: 2.0540]\n",
            "Images logged for Epoch 19\n",
            "[Epoch 20/30] [Batch 0] [G loss: 2.2367]\n",
            "[Epoch 20/30] [Batch 200] [G loss: 2.5103]\n",
            "[Epoch 20/30] [Batch 400] [G loss: 2.7479]\n",
            "[Epoch 20/30] [Batch 600] [G loss: 2.3289]\n",
            "[Epoch 20/30] [Batch 800] [G loss: 3.0670]\n",
            "[Epoch 20/30] [Batch 1000] [G loss: 2.6416]\n",
            "[Epoch 20/30] [Batch 1200] [G loss: 2.3433]\n",
            "[Epoch 20/30] [Batch 1400] [G loss: 2.8015]\n",
            "[Epoch 20/30] [Batch 1600] [G loss: 2.8636]\n",
            "[Epoch 20/30] [Batch 1800] [G loss: 2.8132]\n",
            "[Epoch 20/30] [Batch 2000] [G loss: 1.9459]\n",
            "[Epoch 20/30] [Batch 2200] [G loss: 2.2124]\n",
            "[Epoch 20/30] [Batch 2400] [G loss: 2.8608]\n",
            "[Epoch 20/30] [Batch 2600] [G loss: 2.8960]\n",
            "[Epoch 20/30] [Batch 2800] [G loss: 2.4213]\n",
            "[Epoch 20/30] [Batch 3000] [G loss: 2.6659]\n",
            "[Epoch 20/30] [Batch 3200] [G loss: 2.0080]\n",
            "[Epoch 20/30] [Batch 3400] [G loss: 2.6294]\n",
            "[Epoch 20/30] [Batch 3600] [G loss: 1.8362]\n",
            "[Epoch 20/30] [Batch 3800] [G loss: 2.1224]\n",
            "[Epoch 20/30] [Batch 4000] [G loss: 2.3327]\n",
            "[Epoch 20/30] [Batch 4200] [G loss: 2.2372]\n",
            "[Epoch 20/30] [Batch 4400] [G loss: 1.9240]\n",
            "[Epoch 20/30] [Batch 4600] [G loss: 2.2845]\n",
            "[Epoch 20/30] [Batch 4800] [G loss: 2.2095]\n",
            "[Epoch 20/30] [Batch 5000] [G loss: 2.7137]\n",
            "[Epoch 20/30] [Batch 5200] [G loss: 2.7335]\n",
            "[Epoch 20/30] [Batch 5400] [G loss: 2.4137]\n",
            "[Epoch 20/30] [Batch 5600] [G loss: 1.9920]\n",
            "[Epoch 20/30] [Batch 5800] [G loss: 2.0734]\n",
            "[Epoch 20/30] [Batch 6000] [G loss: 3.0802]\n",
            "[Epoch 20/30] [Batch 6200] [G loss: 2.4978]\n",
            "[Epoch 20/30] [Batch 6400] [G loss: 2.6028]\n",
            "[Epoch 20/30] [Batch 6600] [G loss: 2.2516]\n",
            "[Epoch 20/30] [Batch 6800] [G loss: 2.0567]\n",
            "[Epoch 20/30] [Batch 7000] [G loss: 2.0993]\n",
            "Images logged for Epoch 20\n",
            "Checkpoint saved: /content/drive/MyDrive/University/Painter_Assignment/checkpoints_aug/epoch_20.pth\n",
            "[Epoch 21/30] [Batch 0] [G loss: 2.1776]\n",
            "[Epoch 21/30] [Batch 200] [G loss: 1.9876]\n",
            "[Epoch 21/30] [Batch 400] [G loss: 2.3286]\n",
            "[Epoch 21/30] [Batch 600] [G loss: 2.7608]\n",
            "[Epoch 21/30] [Batch 800] [G loss: 3.2789]\n",
            "[Epoch 21/30] [Batch 1000] [G loss: 2.4877]\n",
            "[Epoch 21/30] [Batch 1200] [G loss: 1.9258]\n",
            "[Epoch 21/30] [Batch 1400] [G loss: 2.1015]\n",
            "[Epoch 21/30] [Batch 1600] [G loss: 1.9460]\n",
            "[Epoch 21/30] [Batch 1800] [G loss: 2.1704]\n",
            "[Epoch 21/30] [Batch 2000] [G loss: 2.3107]\n",
            "[Epoch 21/30] [Batch 2200] [G loss: 2.0454]\n",
            "[Epoch 21/30] [Batch 2400] [G loss: 2.5050]\n",
            "[Epoch 21/30] [Batch 2600] [G loss: 2.2049]\n",
            "[Epoch 21/30] [Batch 2800] [G loss: 2.2461]\n",
            "[Epoch 21/30] [Batch 3000] [G loss: 2.1335]\n",
            "[Epoch 21/30] [Batch 3200] [G loss: 1.8029]\n",
            "[Epoch 21/30] [Batch 3400] [G loss: 1.9913]\n",
            "[Epoch 21/30] [Batch 3600] [G loss: 2.2792]\n",
            "[Epoch 21/30] [Batch 3800] [G loss: 3.1135]\n",
            "[Epoch 21/30] [Batch 4000] [G loss: 2.2550]\n",
            "[Epoch 21/30] [Batch 4200] [G loss: 1.9274]\n",
            "[Epoch 21/30] [Batch 4400] [G loss: 1.9778]\n",
            "[Epoch 21/30] [Batch 4600] [G loss: 2.2890]\n",
            "[Epoch 21/30] [Batch 4800] [G loss: 2.2947]\n",
            "[Epoch 21/30] [Batch 5000] [G loss: 2.7307]\n",
            "[Epoch 21/30] [Batch 5200] [G loss: 2.3308]\n",
            "[Epoch 21/30] [Batch 5400] [G loss: 2.2943]\n",
            "[Epoch 21/30] [Batch 5600] [G loss: 2.1865]\n",
            "[Epoch 21/30] [Batch 5800] [G loss: 3.0341]\n",
            "[Epoch 21/30] [Batch 6000] [G loss: 2.4466]\n",
            "[Epoch 21/30] [Batch 6200] [G loss: 2.7689]\n",
            "[Epoch 21/30] [Batch 6400] [G loss: 1.9094]\n",
            "[Epoch 21/30] [Batch 6600] [G loss: 2.6032]\n",
            "[Epoch 21/30] [Batch 6800] [G loss: 2.0207]\n",
            "[Epoch 21/30] [Batch 7000] [G loss: 2.6005]\n",
            "Images logged for Epoch 21\n",
            "[Epoch 22/30] [Batch 0] [G loss: 1.7721]\n"
          ]
        }
      ],
      "source": [
        "# --- TRAINING LOOP ---\n",
        "for epoch in range(ConfigAug.EPOCHS):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        real_A = batch[\"photo\"].to(ConfigAug.DEVICE)\n",
        "        real_B = batch[\"monet\"].to(ConfigAug.DEVICE)\n",
        "\n",
        "        valid = torch.ones((real_A.size(0), 1, 16, 16), requires_grad=False).to(ConfigAug.DEVICE)\n",
        "        fake = torch.zeros((real_A.size(0), 1, 16, 16), requires_grad=False).to(ConfigAug.DEVICE)\n",
        "\n",
        "        # ------------------\n",
        "        #  Train Generators\n",
        "        # ------------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        with autocast('cuda'): # Mixed Precision Context\n",
        "            # Identity loss\n",
        "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
        "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
        "            loss_identity = (loss_id_A + loss_id_B) / 2 * ConfigAug.LAMBDA_ID\n",
        "\n",
        "            # GAN loss\n",
        "            fake_B = G_AB(real_A)\n",
        "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
        "            fake_A = G_BA(real_B)\n",
        "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
        "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
        "\n",
        "            # Cycle loss\n",
        "            recov_A = G_BA(fake_B)\n",
        "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
        "            recov_B = G_AB(fake_A)\n",
        "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
        "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2 * ConfigAug.LAMBDA_CYCLE\n",
        "\n",
        "            loss_G = loss_GAN + loss_cycle + loss_identity\n",
        "\n",
        "        scaler.scale(loss_G).backward()\n",
        "        scaler.step(optimizer_G)\n",
        "        scaler.update()\n",
        "\n",
        "        # -----------------------\n",
        "        #  Train Discriminator A\n",
        "        # -----------------------\n",
        "        optimizer_D_A.zero_grad()\n",
        "        with autocast('cuda'):\n",
        "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
        "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
        "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
        "            loss_D_A = (loss_real + loss_fake) / 2\n",
        "        scaler.scale(loss_D_A).backward()\n",
        "        scaler.step(optimizer_D_A)\n",
        "        scaler.update()\n",
        "\n",
        "        # -----------------------\n",
        "        #  Train Discriminator B\n",
        "        # -----------------------\n",
        "        optimizer_D_B.zero_grad()\n",
        "        with autocast('cuda'):\n",
        "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
        "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
        "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
        "            loss_D_B = (loss_real + loss_fake) / 2\n",
        "        scaler.scale(loss_D_B).backward()\n",
        "        scaler.step(optimizer_D_B)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            print(f\"[Epoch {epoch}/{ConfigAug.EPOCHS}] [Batch {i}] [G loss: {loss_G.item():.4f}]\")\n",
        "            wandb.log({\n",
        "                \"Loss/G\": loss_G.item(),\n",
        "                \"Loss/D\": loss_D_A.item() + loss_D_B.item(),\n",
        "                \"Loss/Cycle\": loss_cycle.item(),\n",
        "                \"Learning_Rate\": optimizer_G.param_groups[0]['lr'] \n",
        "            })\n",
        "\n",
        "    # Update Learning Rates (Decay Step)\n",
        "    lr_scheduler_G.step()\n",
        "    lr_scheduler_D_A.step()\n",
        "    lr_scheduler_D_B.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sample = next(iter(dataloader))\n",
        "        real_photo_sample = sample['photo'].to(ConfigAug.DEVICE)\n",
        "        fake_monet_sample = G_AB(real_photo_sample)\n",
        "\n",
        "        wandb.log({\n",
        "            \"Visual/Real Photo\": wandb.Image(real_photo_sample[0], caption=f\"Real Epoch {epoch}\"),\n",
        "            \"Visual/Generated Monet\": wandb.Image(fake_monet_sample[0], caption=f\"Fake Epoch {epoch}\"),\n",
        "            \"Epoch\": epoch\n",
        "        })\n",
        "        print(f\"Images logged for Epoch {epoch}\")\n",
        "\n",
        "    if epoch % ConfigAug.SAVE_EPOCH_FREQ == 0:\n",
        "        save_path = f\"{ConfigAug.CHECKPOINT_DIR}/epoch_{epoch}.pth\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'G_AB': G_AB.state_dict(),\n",
        "            'G_BA': G_BA.state_dict(),\n",
        "            'D_A': D_A.state_dict(),\n",
        "            'D_B': D_B.state_dict(),\n",
        "            'optimizer_G': optimizer_G.state_dict(),\n",
        "        }, save_path)\n",
        "        print(f\"Checkpoint saved: {save_path}\")\n",
        "\n",
        "print(\"Experiment 3 Finished!\")\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "Snj18TlWihDd",
        "outputId": "e931547b-3f92-413f-948f-4c99f32c7a9a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deep-firebrand-3</strong> at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/CycleGAN_Painter_AUG/runs/k64u4nsq' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/CycleGAN_Painter_AUG/runs/k64u4nsq</a><br> View project at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/CycleGAN_Painter_AUG' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/CycleGAN_Painter_AUG</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251228_103635-k64u4nsq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FCHnUGMf1ImM"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
